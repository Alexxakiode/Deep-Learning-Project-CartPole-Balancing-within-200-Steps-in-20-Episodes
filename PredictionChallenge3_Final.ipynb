{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexxakiode/Deep-Learning-Project-CartPole-Balancing-within-200-Steps-in-20-Episodes/blob/main/PredictionChallenge3_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# CartPole Balancing within 200 Steps in 20 Episodes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "# !pip install keras-rl2"
      ],
      "metadata": {
        "id": "vigXep7jWbjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7abf6868-83b3-43f7-9854-87e791ab1856"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m685.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (63.4.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gym"
      ],
      "metadata": {
        "id": "e3dLlThnWg4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f99ccb-cafd-40d1-e865-454fdea15408"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "6E9nLhfGeE2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afeb8c61-4138-4fb4-ba99-bda83cabfea6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (6.0.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n",
            "Installing collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbtfhWRqeHzq",
        "outputId": "8c04191a-995e-48de-fc78-22f254fc5f0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "\n",
        "# Load other basic modules\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import randint\n",
        "\n",
        "import pickle\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyOml0McTqQX",
        "outputId": "039e4290-c3d2-4d7d-a4f1-36586dca029f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n"
      ],
      "metadata": {
        "id": "crCTjDiOUUJQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EpsGreedyQPolicy\n",
        "# setup experience replay buffer\n",
        "# here the sequential memory limit is set up the same as the nb_steps (number of steps)\n",
        "# parameter in the fit method.  This means that all the action-states will fit into the\n",
        "# memory buffer\n",
        "# keep window_length as 1. It's used in other RL methods, but keep it to 1 in DQNs\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "policy_inner = EpsGreedyQPolicy()"
      ],
      "metadata": {
        "id": "NrHkEYq3VCnJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qr9G2ChZLGt",
        "outputId": "59972e94-3195-4d16-c10c-9f76d746967b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run params\n",
        "\n",
        "value_max = 1.0\n",
        "\n",
        "value_min = 0.01\n",
        "\n",
        "value_test = 0.05\n",
        "\n",
        "nb_steps_warmup=10\n",
        "\n",
        "target_model_update=1e-2\n",
        "\n",
        "nb_steps=50000\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "nb_episodes=20"
      ],
      "metadata": {
        "id": "TnucN5MrDKrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_outer =  LinearAnnealedPolicy(inner_policy=policy_inner, \n",
        "                               attr='eps',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.01, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy_outer) \n",
        "\n",
        "dqn.compile(Adam(lr=0.01), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)\n",
        "\n",
        "plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "\n",
        "#### PLEASE NOTE THAT I HAVE HAD TO RERUN THIS CELL SINCE I COPIED IT FROM MY OTHER TRIALS FILE, HENCE THE DIFFERENCE IN THE PLOT IMAGE."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PvsP88vVDJQj",
        "outputId": "db8b8240-d396-4871-b3c0-a3306fe49783"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    20/50000: episode: 1, duration: 1.112s, episode steps:  20, steps per second:  18, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 16.365135, mae: 75.835971, mean_q: 155.119371, mean_eps: 0.998515\n",
            "    32/50000: episode: 2, duration: 0.098s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 27.074064, mae: 76.492582, mean_q: 155.907291, mean_eps: 0.997476\n",
            "    51/50000: episode: 3, duration: 0.138s, episode steps:  19, steps per second: 137, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 50.489501, mae: 75.360250, mean_q: 154.132372, mean_eps: 0.995941\n",
            "    84/50000: episode: 4, duration: 0.249s, episode steps:  33, steps per second: 133, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 38.538694, mae: 76.090674, mean_q: 154.964549, mean_eps: 0.993367\n",
            "   113/50000: episode: 5, duration: 0.208s, episode steps:  29, steps per second: 139, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 98.387301, mae: 75.025392, mean_q: 152.386646, mean_eps: 0.990298\n",
            "   129/50000: episode: 6, duration: 0.146s, episode steps:  16, steps per second: 109, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 83.086020, mae: 75.445747, mean_q: 153.264441, mean_eps: 0.988071\n",
            "   170/50000: episode: 7, duration: 0.330s, episode steps:  41, steps per second: 124, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 64.005445, mae: 75.646908, mean_q: 154.669317, mean_eps: 0.985249\n",
            "   182/50000: episode: 8, duration: 0.093s, episode steps:  12, steps per second: 129, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 59.618990, mae: 75.205485, mean_q: 151.533456, mean_eps: 0.982626\n",
            "   202/50000: episode: 9, duration: 0.159s, episode steps:  20, steps per second: 126, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 119.871904, mae: 76.000779, mean_q: 153.479858, mean_eps: 0.981042\n",
            "   214/50000: episode: 10, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 45.874593, mae: 77.118618, mean_q: 157.972645, mean_eps: 0.979458\n",
            "   234/50000: episode: 11, duration: 0.153s, episode steps:  20, steps per second: 131, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 53.857484, mae: 75.237469, mean_q: 153.072147, mean_eps: 0.977873\n",
            "   244/50000: episode: 12, duration: 0.088s, episode steps:  10, steps per second: 114, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 83.067150, mae: 77.288023, mean_q: 157.051132, mean_eps: 0.976388\n",
            "   324/50000: episode: 13, duration: 0.587s, episode steps:  80, steps per second: 136, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 51.857745, mae: 75.574282, mean_q: 153.806503, mean_eps: 0.971934\n",
            "   369/50000: episode: 14, duration: 0.338s, episode steps:  45, steps per second: 133, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 80.034020, mae: 76.724808, mean_q: 155.483656, mean_eps: 0.965746\n",
            "   383/50000: episode: 15, duration: 0.109s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 74.429098, mae: 77.364246, mean_q: 156.956701, mean_eps: 0.962826\n",
            "   441/50000: episode: 16, duration: 0.443s, episode steps:  58, steps per second: 131, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 92.294252, mae: 76.167350, mean_q: 154.048114, mean_eps: 0.959261\n",
            "   456/50000: episode: 17, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 72.968670, mae: 76.574751, mean_q: 155.847228, mean_eps: 0.955648\n",
            "   484/50000: episode: 18, duration: 0.212s, episode steps:  28, steps per second: 132, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 49.969353, mae: 76.810237, mean_q: 157.171303, mean_eps: 0.953520\n",
            "   544/50000: episode: 19, duration: 0.462s, episode steps:  60, steps per second: 130, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 53.811315, mae: 76.533599, mean_q: 155.417637, mean_eps: 0.949163\n",
            "   562/50000: episode: 20, duration: 0.132s, episode steps:  18, steps per second: 136, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 18.570733, mae: 76.497570, mean_q: 156.030257, mean_eps: 0.945302\n",
            "   585/50000: episode: 21, duration: 0.168s, episode steps:  23, steps per second: 137, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 71.023655, mae: 76.169139, mean_q: 156.005926, mean_eps: 0.943273\n",
            "   643/50000: episode: 22, duration: 0.439s, episode steps:  58, steps per second: 132, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 64.118187, mae: 76.889900, mean_q: 156.336167, mean_eps: 0.939264\n",
            "   662/50000: episode: 23, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 48.729468, mae: 74.979241, mean_q: 152.642707, mean_eps: 0.935452\n",
            "   685/50000: episode: 24, duration: 0.173s, episode steps:  23, steps per second: 133, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.304 [0.000, 1.000],  loss: 38.293998, mae: 76.441214, mean_q: 156.274656, mean_eps: 0.933373\n",
            "   703/50000: episode: 25, duration: 0.127s, episode steps:  18, steps per second: 142, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 64.484995, mae: 76.464126, mean_q: 155.990729, mean_eps: 0.931343\n",
            "   725/50000: episode: 26, duration: 0.155s, episode steps:  22, steps per second: 142, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 60.898561, mae: 76.362710, mean_q: 155.333020, mean_eps: 0.929363\n",
            "   747/50000: episode: 27, duration: 0.157s, episode steps:  22, steps per second: 140, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 67.075553, mae: 76.883159, mean_q: 156.313800, mean_eps: 0.927186\n",
            "   779/50000: episode: 28, duration: 0.245s, episode steps:  32, steps per second: 130, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 57.607745, mae: 76.866387, mean_q: 157.025355, mean_eps: 0.924513\n",
            "   789/50000: episode: 29, duration: 0.081s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 51.553714, mae: 74.237015, mean_q: 151.736560, mean_eps: 0.922434\n",
            "   807/50000: episode: 30, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 64.634747, mae: 74.201855, mean_q: 151.928375, mean_eps: 0.921048\n",
            "   821/50000: episode: 31, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 57.173581, mae: 77.901455, mean_q: 159.466329, mean_eps: 0.919463\n",
            "   838/50000: episode: 32, duration: 0.129s, episode steps:  17, steps per second: 132, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 36.884975, mae: 76.291386, mean_q: 154.911873, mean_eps: 0.917929\n",
            "   857/50000: episode: 33, duration: 0.136s, episode steps:  19, steps per second: 140, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 49.499103, mae: 77.020230, mean_q: 156.781969, mean_eps: 0.916147\n",
            "   884/50000: episode: 34, duration: 0.211s, episode steps:  27, steps per second: 128, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 38.400542, mae: 77.588419, mean_q: 158.721565, mean_eps: 0.913870\n",
            "   900/50000: episode: 35, duration: 0.172s, episode steps:  16, steps per second:  93, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 94.874294, mae: 76.360835, mean_q: 154.486774, mean_eps: 0.911741\n",
            "   911/50000: episode: 36, duration: 0.134s, episode steps:  11, steps per second:  82, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 142.834489, mae: 75.912426, mean_q: 152.990455, mean_eps: 0.910405\n",
            "   944/50000: episode: 37, duration: 0.356s, episode steps:  33, steps per second:  93, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 92.782362, mae: 76.564405, mean_q: 155.673980, mean_eps: 0.908227\n",
            "   978/50000: episode: 38, duration: 0.364s, episode steps:  34, steps per second:  93, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 62.440069, mae: 77.407175, mean_q: 156.604375, mean_eps: 0.904911\n",
            "  1000/50000: episode: 39, duration: 0.250s, episode steps:  22, steps per second:  88, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 44.520864, mae: 76.719324, mean_q: 156.079804, mean_eps: 0.902138\n",
            "  1019/50000: episode: 40, duration: 0.227s, episode steps:  19, steps per second:  84, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 23.888016, mae: 76.278232, mean_q: 154.902549, mean_eps: 0.900109\n",
            "  1082/50000: episode: 41, duration: 0.695s, episode steps:  63, steps per second:  91, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.603 [0.000, 1.000],  loss: 74.180995, mae: 77.048319, mean_q: 155.071606, mean_eps: 0.896050\n",
            "  1103/50000: episode: 42, duration: 0.232s, episode steps:  21, steps per second:  91, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 50.132948, mae: 76.328554, mean_q: 155.356386, mean_eps: 0.891892\n",
            "  1138/50000: episode: 43, duration: 0.381s, episode steps:  35, steps per second:  92, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 75.881209, mae: 76.168320, mean_q: 154.718004, mean_eps: 0.889120\n",
            "  1161/50000: episode: 44, duration: 0.190s, episode steps:  23, steps per second: 121, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 59.224389, mae: 76.597106, mean_q: 155.046507, mean_eps: 0.886249\n",
            "  1182/50000: episode: 45, duration: 0.166s, episode steps:  21, steps per second: 127, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 85.844374, mae: 76.690555, mean_q: 154.964835, mean_eps: 0.884071\n",
            "  1195/50000: episode: 46, duration: 0.103s, episode steps:  13, steps per second: 126, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 35.112736, mae: 76.991915, mean_q: 157.342972, mean_eps: 0.882388\n",
            "  1206/50000: episode: 47, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 86.642123, mae: 75.561012, mean_q: 152.683506, mean_eps: 0.881200\n",
            "  1220/50000: episode: 48, duration: 0.118s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 57.109357, mae: 75.951015, mean_q: 153.797783, mean_eps: 0.879962\n",
            "  1247/50000: episode: 49, duration: 0.207s, episode steps:  27, steps per second: 130, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 72.861870, mae: 75.601109, mean_q: 153.243050, mean_eps: 0.877933\n",
            "  1270/50000: episode: 50, duration: 0.175s, episode steps:  23, steps per second: 131, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 58.918924, mae: 76.328336, mean_q: 154.171497, mean_eps: 0.875458\n",
            "  1282/50000: episode: 51, duration: 0.094s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 80.591115, mae: 75.520771, mean_q: 151.414210, mean_eps: 0.873726\n",
            "  1336/50000: episode: 52, duration: 0.396s, episode steps:  54, steps per second: 136, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 92.647624, mae: 75.885998, mean_q: 154.140686, mean_eps: 0.870459\n",
            "  1366/50000: episode: 53, duration: 0.237s, episode steps:  30, steps per second: 127, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 55.070685, mae: 77.174858, mean_q: 156.917459, mean_eps: 0.866301\n",
            "  1399/50000: episode: 54, duration: 0.231s, episode steps:  33, steps per second: 143, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 33.417905, mae: 76.838196, mean_q: 156.857039, mean_eps: 0.863182\n",
            "  1431/50000: episode: 55, duration: 0.235s, episode steps:  32, steps per second: 136, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 54.151737, mae: 76.139370, mean_q: 154.875634, mean_eps: 0.859965\n",
            "  1456/50000: episode: 56, duration: 0.181s, episode steps:  25, steps per second: 138, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 54.269350, mae: 75.848580, mean_q: 154.830530, mean_eps: 0.857143\n",
            "  1477/50000: episode: 57, duration: 0.175s, episode steps:  21, steps per second: 120, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 78.694102, mae: 77.607506, mean_q: 157.064527, mean_eps: 0.854866\n",
            "  1509/50000: episode: 58, duration: 0.248s, episode steps:  32, steps per second: 129, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 26.454865, mae: 76.842741, mean_q: 156.665019, mean_eps: 0.852243\n",
            "  1536/50000: episode: 59, duration: 0.210s, episode steps:  27, steps per second: 129, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 43.569078, mae: 76.261556, mean_q: 155.981307, mean_eps: 0.849322\n",
            "  1588/50000: episode: 60, duration: 0.403s, episode steps:  52, steps per second: 129, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 60.086762, mae: 76.910360, mean_q: 156.054664, mean_eps: 0.845412\n",
            "  1601/50000: episode: 61, duration: 0.097s, episode steps:  13, steps per second: 135, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 21.001865, mae: 78.900531, mean_q: 160.295378, mean_eps: 0.842194\n",
            "  1724/50000: episode: 62, duration: 0.910s, episode steps: 123, steps per second: 135, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 53.319683, mae: 76.745098, mean_q: 155.321976, mean_eps: 0.835462\n",
            "  1744/50000: episode: 63, duration: 0.145s, episode steps:  20, steps per second: 137, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 56.459507, mae: 77.280659, mean_q: 156.854958, mean_eps: 0.828383\n",
            "  1768/50000: episode: 64, duration: 0.194s, episode steps:  24, steps per second: 124, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 61.768439, mae: 77.039207, mean_q: 156.794093, mean_eps: 0.826205\n",
            "  1805/50000: episode: 65, duration: 0.289s, episode steps:  37, steps per second: 128, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 52.900154, mae: 76.578009, mean_q: 156.498769, mean_eps: 0.823186\n",
            "  1820/50000: episode: 66, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 49.819881, mae: 75.246495, mean_q: 153.707279, mean_eps: 0.820612\n",
            "  1854/50000: episode: 67, duration: 0.256s, episode steps:  34, steps per second: 133, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 36.901886, mae: 76.962468, mean_q: 156.491521, mean_eps: 0.818187\n",
            "  1882/50000: episode: 68, duration: 0.232s, episode steps:  28, steps per second: 121, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 25.005611, mae: 77.464585, mean_q: 158.090231, mean_eps: 0.815118\n",
            "  1902/50000: episode: 69, duration: 0.149s, episode steps:  20, steps per second: 134, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 38.187862, mae: 77.468560, mean_q: 156.912016, mean_eps: 0.812741\n",
            "  1916/50000: episode: 70, duration: 0.116s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 30.441129, mae: 76.327644, mean_q: 155.516088, mean_eps: 0.811059\n",
            "  1943/50000: episode: 71, duration: 0.202s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 38.174907, mae: 77.019462, mean_q: 156.791086, mean_eps: 0.809029\n",
            "  1974/50000: episode: 72, duration: 0.240s, episode steps:  31, steps per second: 129, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.355 [0.000, 1.000],  loss: 41.565902, mae: 77.565852, mean_q: 157.587020, mean_eps: 0.806158\n",
            "  1992/50000: episode: 73, duration: 0.132s, episode steps:  18, steps per second: 136, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 57.865066, mae: 76.915874, mean_q: 155.574119, mean_eps: 0.803733\n",
            "  2016/50000: episode: 74, duration: 0.187s, episode steps:  24, steps per second: 128, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 94.277112, mae: 77.106915, mean_q: 155.587546, mean_eps: 0.801654\n",
            "  2058/50000: episode: 75, duration: 0.323s, episode steps:  42, steps per second: 130, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 50.349967, mae: 76.967246, mean_q: 156.021643, mean_eps: 0.798386\n",
            "  2116/50000: episode: 76, duration: 0.438s, episode steps:  58, steps per second: 132, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 74.942535, mae: 77.266028, mean_q: 156.437319, mean_eps: 0.793437\n",
            "  2139/50000: episode: 77, duration: 0.176s, episode steps:  23, steps per second: 130, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 67.224967, mae: 77.152657, mean_q: 155.390246, mean_eps: 0.789427\n",
            "  2172/50000: episode: 78, duration: 0.258s, episode steps:  33, steps per second: 128, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 54.066255, mae: 77.722265, mean_q: 156.592412, mean_eps: 0.786655\n",
            "  2227/50000: episode: 79, duration: 0.399s, episode steps:  55, steps per second: 138, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 42.200769, mae: 76.491071, mean_q: 155.180141, mean_eps: 0.782299\n",
            "  2263/50000: episode: 80, duration: 0.264s, episode steps:  36, steps per second: 136, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 62.337830, mae: 76.756842, mean_q: 154.785206, mean_eps: 0.777794\n",
            "  2283/50000: episode: 81, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 17.403522, mae: 76.448954, mean_q: 155.996135, mean_eps: 0.775023\n",
            "  2399/50000: episode: 82, duration: 0.851s, episode steps: 116, steps per second: 136, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 60.815097, mae: 76.683619, mean_q: 154.927637, mean_eps: 0.768290\n",
            "  2433/50000: episode: 83, duration: 0.258s, episode steps:  34, steps per second: 132, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 33.783198, mae: 76.382962, mean_q: 155.013132, mean_eps: 0.760866\n",
            "  2474/50000: episode: 84, duration: 0.430s, episode steps:  41, steps per second:  95, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 47.761336, mae: 76.147307, mean_q: 154.579071, mean_eps: 0.757153\n",
            "  2495/50000: episode: 85, duration: 0.239s, episode steps:  21, steps per second:  88, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 29.685267, mae: 77.279168, mean_q: 157.622920, mean_eps: 0.754084\n",
            "  2551/50000: episode: 86, duration: 0.628s, episode steps:  56, steps per second:  89, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 52.093906, mae: 76.658452, mean_q: 155.454358, mean_eps: 0.750273\n",
            "  2587/50000: episode: 87, duration: 0.385s, episode steps:  36, steps per second:  94, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 38.617247, mae: 77.803513, mean_q: 158.431729, mean_eps: 0.745718\n",
            "  2642/50000: episode: 88, duration: 0.603s, episode steps:  55, steps per second:  91, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 43.279321, mae: 75.748682, mean_q: 154.653857, mean_eps: 0.741214\n",
            "  2661/50000: episode: 89, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 46.746833, mae: 76.057353, mean_q: 154.987377, mean_eps: 0.737551\n",
            "  2678/50000: episode: 90, duration: 0.196s, episode steps:  17, steps per second:  87, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 58.930171, mae: 75.304021, mean_q: 152.488826, mean_eps: 0.735769\n",
            "  2697/50000: episode: 91, duration: 0.185s, episode steps:  19, steps per second: 103, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 56.511072, mae: 76.702214, mean_q: 155.507884, mean_eps: 0.733987\n",
            "  2712/50000: episode: 92, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 40.427680, mae: 75.915877, mean_q: 153.425576, mean_eps: 0.732304\n",
            "  2749/50000: episode: 93, duration: 0.308s, episode steps:  37, steps per second: 120, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 46.646460, mae: 76.704891, mean_q: 156.125597, mean_eps: 0.729730\n",
            "  2778/50000: episode: 94, duration: 0.215s, episode steps:  29, steps per second: 135, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 44.307328, mae: 77.077517, mean_q: 156.322623, mean_eps: 0.726463\n",
            "  2793/50000: episode: 95, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 47.719706, mae: 77.396607, mean_q: 158.129368, mean_eps: 0.724285\n",
            "  2835/50000: episode: 96, duration: 0.316s, episode steps:  42, steps per second: 133, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 77.009986, mae: 77.146624, mean_q: 155.795394, mean_eps: 0.721463\n",
            "  2859/50000: episode: 97, duration: 0.193s, episode steps:  24, steps per second: 124, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 47.431590, mae: 77.267754, mean_q: 156.574355, mean_eps: 0.718197\n",
            "  2875/50000: episode: 98, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 50.548048, mae: 75.576834, mean_q: 153.415101, mean_eps: 0.716217\n",
            "  2918/50000: episode: 99, duration: 0.304s, episode steps:  43, steps per second: 142, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 75.610147, mae: 76.644114, mean_q: 154.784340, mean_eps: 0.713296\n",
            "  3044/50000: episode: 100, duration: 0.908s, episode steps: 126, steps per second: 139, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 52.683539, mae: 76.583332, mean_q: 155.195336, mean_eps: 0.704931\n",
            "  3089/50000: episode: 101, duration: 0.320s, episode steps:  45, steps per second: 141, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 35.212885, mae: 76.140520, mean_q: 154.959232, mean_eps: 0.696466\n",
            "  3105/50000: episode: 102, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 23.891770, mae: 78.192741, mean_q: 158.393571, mean_eps: 0.693447\n",
            "  3177/50000: episode: 103, duration: 0.535s, episode steps:  72, steps per second: 135, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 47.785895, mae: 77.024625, mean_q: 156.170126, mean_eps: 0.689091\n",
            "  3199/50000: episode: 104, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 51.792063, mae: 77.282104, mean_q: 157.111426, mean_eps: 0.684438\n",
            "  3276/50000: episode: 105, duration: 0.557s, episode steps:  77, steps per second: 138, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 50.532975, mae: 76.499856, mean_q: 155.726449, mean_eps: 0.679537\n",
            "  3299/50000: episode: 106, duration: 0.175s, episode steps:  23, steps per second: 131, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 40.579627, mae: 76.598778, mean_q: 155.699669, mean_eps: 0.674587\n",
            "  3375/50000: episode: 107, duration: 0.537s, episode steps:  76, steps per second: 141, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 52.968134, mae: 77.503491, mean_q: 157.321518, mean_eps: 0.669687\n",
            "  3450/50000: episode: 108, duration: 0.561s, episode steps:  75, steps per second: 134, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 45.752041, mae: 77.227705, mean_q: 156.547227, mean_eps: 0.662212\n",
            "  3515/50000: episode: 109, duration: 0.455s, episode steps:  65, steps per second: 143, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 41.815377, mae: 77.473867, mean_q: 157.663427, mean_eps: 0.655282\n",
            "  3535/50000: episode: 110, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 42.858902, mae: 76.859938, mean_q: 155.879023, mean_eps: 0.651074\n",
            "  3722/50000: episode: 111, duration: 1.385s, episode steps: 187, steps per second: 135, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 55.843626, mae: 77.287251, mean_q: 156.739725, mean_eps: 0.640828\n",
            "  3753/50000: episode: 112, duration: 0.224s, episode steps:  31, steps per second: 138, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 33.803816, mae: 78.163549, mean_q: 159.928084, mean_eps: 0.630037\n",
            "  3775/50000: episode: 113, duration: 0.165s, episode steps:  22, steps per second: 133, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 40.387084, mae: 78.132611, mean_q: 158.541610, mean_eps: 0.627414\n",
            "  3885/50000: episode: 114, duration: 0.783s, episode steps: 110, steps per second: 141, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 34.957218, mae: 78.310866, mean_q: 158.793843, mean_eps: 0.620879\n",
            "  3960/50000: episode: 115, duration: 0.550s, episode steps:  75, steps per second: 136, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 39.298691, mae: 77.066921, mean_q: 156.881387, mean_eps: 0.611722\n",
            "  4159/50000: episode: 116, duration: 1.870s, episode steps: 199, steps per second: 106, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 46.942911, mae: 78.400848, mean_q: 158.620476, mean_eps: 0.598159\n",
            "  4174/50000: episode: 117, duration: 0.182s, episode steps:  15, steps per second:  83, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 46.081633, mae: 77.040672, mean_q: 156.188999, mean_eps: 0.587566\n",
            "  4260/50000: episode: 118, duration: 0.916s, episode steps:  86, steps per second:  94, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.047438, mae: 78.187770, mean_q: 158.500122, mean_eps: 0.582567\n",
            "  4455/50000: episode: 119, duration: 1.523s, episode steps: 195, steps per second: 128, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 51.696986, mae: 77.883621, mean_q: 157.936601, mean_eps: 0.568657\n",
            "  4490/50000: episode: 120, duration: 0.258s, episode steps:  35, steps per second: 136, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 43.130964, mae: 78.426388, mean_q: 159.674911, mean_eps: 0.557272\n",
            "  4616/50000: episode: 121, duration: 0.903s, episode steps: 126, steps per second: 139, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 43.576466, mae: 77.549058, mean_q: 157.166102, mean_eps: 0.549303\n",
            "  4671/50000: episode: 122, duration: 0.394s, episode steps:  55, steps per second: 139, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 36.894291, mae: 78.586721, mean_q: 159.973520, mean_eps: 0.540343\n",
            "  4857/50000: episode: 123, duration: 1.354s, episode steps: 186, steps per second: 137, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 34.028701, mae: 78.357151, mean_q: 159.166033, mean_eps: 0.528413\n",
            "  4995/50000: episode: 124, duration: 1.005s, episode steps: 138, steps per second: 137, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 40.304938, mae: 78.589608, mean_q: 159.329934, mean_eps: 0.512375\n",
            "  5068/50000: episode: 125, duration: 0.534s, episode steps:  73, steps per second: 137, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 38.538940, mae: 78.441277, mean_q: 159.507305, mean_eps: 0.501931\n",
            "  5099/50000: episode: 126, duration: 0.237s, episode steps:  31, steps per second: 131, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.355 [0.000, 1.000],  loss: 49.015880, mae: 78.145793, mean_q: 158.983484, mean_eps: 0.496783\n",
            "  5299/50000: episode: 127, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 40.061136, mae: 79.949825, mean_q: 161.924424, mean_eps: 0.485349\n",
            "  5491/50000: episode: 128, duration: 1.366s, episode steps: 192, steps per second: 141, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 36.591510, mae: 80.147182, mean_q: 163.102753, mean_eps: 0.465944\n",
            "  5611/50000: episode: 129, duration: 0.873s, episode steps: 120, steps per second: 137, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 34.117678, mae: 81.069044, mean_q: 165.179610, mean_eps: 0.450501\n",
            "  5786/50000: episode: 130, duration: 1.616s, episode steps: 175, steps per second: 108, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 42.692053, mae: 81.273561, mean_q: 164.987275, mean_eps: 0.435898\n",
            "  5902/50000: episode: 131, duration: 1.254s, episode steps: 116, steps per second:  93, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 36.690088, mae: 81.348502, mean_q: 165.202168, mean_eps: 0.421494\n",
            "  6092/50000: episode: 132, duration: 1.488s, episode steps: 190, steps per second: 128, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 43.848398, mae: 81.292769, mean_q: 164.941610, mean_eps: 0.406347\n",
            "  6246/50000: episode: 133, duration: 1.095s, episode steps: 154, steps per second: 141, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 50.902456, mae: 81.856192, mean_q: 165.886466, mean_eps: 0.389319\n",
            "  6284/50000: episode: 134, duration: 0.283s, episode steps:  38, steps per second: 134, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 36.487600, mae: 82.176628, mean_q: 167.051313, mean_eps: 0.379815\n",
            "  6484/50000: episode: 135, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 37.927114, mae: 81.908172, mean_q: 166.680281, mean_eps: 0.368034\n",
            "  6667/50000: episode: 136, duration: 1.358s, episode steps: 183, steps per second: 135, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 38.973213, mae: 82.521506, mean_q: 167.583256, mean_eps: 0.349075\n",
            "  6818/50000: episode: 137, duration: 1.124s, episode steps: 151, steps per second: 134, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 35.521776, mae: 83.293109, mean_q: 169.480864, mean_eps: 0.332542\n",
            "  6983/50000: episode: 138, duration: 1.227s, episode steps: 165, steps per second: 134, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 37.219630, mae: 83.760422, mean_q: 170.103355, mean_eps: 0.316900\n",
            "  7135/50000: episode: 139, duration: 1.101s, episode steps: 152, steps per second: 138, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 37.954613, mae: 83.934894, mean_q: 170.487008, mean_eps: 0.301209\n",
            "  7331/50000: episode: 140, duration: 1.487s, episode steps: 196, steps per second: 132, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 37.408269, mae: 83.952284, mean_q: 170.463149, mean_eps: 0.283983\n",
            "  7485/50000: episode: 141, duration: 1.663s, episode steps: 154, steps per second:  93, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 33.283518, mae: 85.118574, mean_q: 173.038255, mean_eps: 0.266658\n",
            "  7648/50000: episode: 142, duration: 1.413s, episode steps: 163, steps per second: 115, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 35.714956, mae: 85.090962, mean_q: 172.955027, mean_eps: 0.250966\n",
            "  7848/50000: episode: 143, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 38.747688, mae: 85.184190, mean_q: 172.938033, mean_eps: 0.232998\n",
            "  8009/50000: episode: 144, duration: 1.151s, episode steps: 161, steps per second: 140, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 37.870178, mae: 85.414664, mean_q: 173.146775, mean_eps: 0.215128\n",
            "  8209/50000: episode: 145, duration: 1.434s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 38.230190, mae: 85.328715, mean_q: 173.150393, mean_eps: 0.197259\n",
            "  8379/50000: episode: 146, duration: 1.236s, episode steps: 170, steps per second: 138, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 35.154938, mae: 85.909151, mean_q: 174.276294, mean_eps: 0.178944\n",
            "  8569/50000: episode: 147, duration: 1.378s, episode steps: 190, steps per second: 138, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 29.966153, mae: 85.691056, mean_q: 174.150591, mean_eps: 0.161124\n",
            "  8769/50000: episode: 148, duration: 1.477s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 38.731903, mae: 86.299284, mean_q: 174.654381, mean_eps: 0.141819\n",
            "  8969/50000: episode: 149, duration: 1.583s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 47.200610, mae: 85.577451, mean_q: 173.184829, mean_eps: 0.122019\n",
            "  9169/50000: episode: 150, duration: 2.119s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 35.176856, mae: 86.444095, mean_q: 175.046707, mean_eps: 0.102219\n",
            "  9369/50000: episode: 151, duration: 1.520s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 40.380364, mae: 87.293181, mean_q: 176.662238, mean_eps: 0.082419\n",
            "  9569/50000: episode: 152, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 45.231286, mae: 86.973068, mean_q: 175.207654, mean_eps: 0.062619\n",
            "  9763/50000: episode: 153, duration: 1.397s, episode steps: 194, steps per second: 139, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 41.557853, mae: 86.939076, mean_q: 175.532778, mean_eps: 0.043116\n",
            "  9954/50000: episode: 154, duration: 1.394s, episode steps: 191, steps per second: 137, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 32.683723, mae: 88.001856, mean_q: 177.751218, mean_eps: 0.024058\n",
            " 10154/50000: episode: 155, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 45.406807, mae: 87.777028, mean_q: 176.789298, mean_eps: 0.010535\n",
            " 10354/50000: episode: 156, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 40.201883, mae: 87.544809, mean_q: 176.215593, mean_eps: 0.010000\n",
            " 10554/50000: episode: 157, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 33.522036, mae: 87.584132, mean_q: 176.383694, mean_eps: 0.010000\n",
            " 10754/50000: episode: 158, duration: 2.112s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 36.851799, mae: 88.028158, mean_q: 177.437046, mean_eps: 0.010000\n",
            " 10954/50000: episode: 159, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 36.381102, mae: 88.004564, mean_q: 176.683676, mean_eps: 0.010000\n",
            " 11154/50000: episode: 160, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 31.047851, mae: 88.321960, mean_q: 178.001817, mean_eps: 0.010000\n",
            " 11345/50000: episode: 161, duration: 1.383s, episode steps: 191, steps per second: 138, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 39.921357, mae: 88.382909, mean_q: 177.846280, mean_eps: 0.010000\n",
            " 11540/50000: episode: 162, duration: 1.412s, episode steps: 195, steps per second: 138, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 32.307897, mae: 87.336209, mean_q: 175.525785, mean_eps: 0.010000\n",
            " 11737/50000: episode: 163, duration: 1.465s, episode steps: 197, steps per second: 135, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 31.190816, mae: 88.248151, mean_q: 176.886558, mean_eps: 0.010000\n",
            " 11921/50000: episode: 164, duration: 1.342s, episode steps: 184, steps per second: 137, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 23.692878, mae: 87.389067, mean_q: 175.098184, mean_eps: 0.010000\n",
            " 12097/50000: episode: 165, duration: 1.257s, episode steps: 176, steps per second: 140, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 28.742749, mae: 87.166523, mean_q: 174.411216, mean_eps: 0.010000\n",
            " 12297/50000: episode: 166, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 34.373858, mae: 86.811791, mean_q: 173.780153, mean_eps: 0.010000\n",
            " 12478/50000: episode: 167, duration: 1.847s, episode steps: 181, steps per second:  98, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 29.162689, mae: 86.108894, mean_q: 172.355394, mean_eps: 0.010000\n",
            " 12673/50000: episode: 168, duration: 1.407s, episode steps: 195, steps per second: 139, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 24.115761, mae: 86.061822, mean_q: 172.227691, mean_eps: 0.010000\n",
            " 12860/50000: episode: 169, duration: 1.336s, episode steps: 187, steps per second: 140, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 20.303686, mae: 85.669680, mean_q: 170.823504, mean_eps: 0.010000\n",
            " 13050/50000: episode: 170, duration: 1.366s, episode steps: 190, steps per second: 139, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 22.350249, mae: 83.844959, mean_q: 167.209487, mean_eps: 0.010000\n",
            " 13220/50000: episode: 171, duration: 1.222s, episode steps: 170, steps per second: 139, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 20.034973, mae: 83.940243, mean_q: 167.377058, mean_eps: 0.010000\n",
            " 13394/50000: episode: 172, duration: 1.257s, episode steps: 174, steps per second: 138, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 19.661997, mae: 82.818475, mean_q: 164.924529, mean_eps: 0.010000\n",
            " 13581/50000: episode: 173, duration: 1.367s, episode steps: 187, steps per second: 137, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 19.669654, mae: 82.382891, mean_q: 163.875307, mean_eps: 0.010000\n",
            " 13755/50000: episode: 174, duration: 1.247s, episode steps: 174, steps per second: 140, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 17.785173, mae: 81.133058, mean_q: 161.103847, mean_eps: 0.010000\n",
            " 13926/50000: episode: 175, duration: 1.523s, episode steps: 171, steps per second: 112, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 12.153394, mae: 80.275046, mean_q: 159.275065, mean_eps: 0.010000\n",
            " 14109/50000: episode: 176, duration: 1.944s, episode steps: 183, steps per second:  94, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 15.658817, mae: 79.827300, mean_q: 158.193947, mean_eps: 0.010000\n",
            " 14309/50000: episode: 177, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 10.969985, mae: 78.845161, mean_q: 156.226561, mean_eps: 0.010000\n",
            " 14509/50000: episode: 178, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 12.534219, mae: 79.498601, mean_q: 157.496188, mean_eps: 0.010000\n",
            " 14708/50000: episode: 179, duration: 1.420s, episode steps: 199, steps per second: 140, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 9.885720, mae: 78.159673, mean_q: 154.934491, mean_eps: 0.010000\n",
            " 14908/50000: episode: 180, duration: 1.443s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.059542, mae: 77.682172, mean_q: 154.194044, mean_eps: 0.010000\n",
            " 15106/50000: episode: 181, duration: 1.425s, episode steps: 198, steps per second: 139, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 7.011333, mae: 77.088709, mean_q: 153.369961, mean_eps: 0.010000\n",
            " 15306/50000: episode: 182, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8.795601, mae: 76.202669, mean_q: 151.112962, mean_eps: 0.010000\n",
            " 15506/50000: episode: 183, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.683904, mae: 75.326319, mean_q: 149.396484, mean_eps: 0.010000\n",
            " 15706/50000: episode: 184, duration: 2.133s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.351323, mae: 74.723138, mean_q: 148.371291, mean_eps: 0.010000\n",
            " 15906/50000: episode: 185, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.953177, mae: 74.458571, mean_q: 147.635975, mean_eps: 0.010000\n",
            " 16106/50000: episode: 186, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.527685, mae: 73.340115, mean_q: 145.275254, mean_eps: 0.010000\n",
            " 16306/50000: episode: 187, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.145585, mae: 72.654301, mean_q: 143.606778, mean_eps: 0.010000\n",
            " 16506/50000: episode: 188, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 7.153132, mae: 71.899593, mean_q: 142.268094, mean_eps: 0.010000\n",
            " 16706/50000: episode: 189, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.296327, mae: 71.027319, mean_q: 140.177906, mean_eps: 0.010000\n",
            " 16906/50000: episode: 190, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.260380, mae: 71.951584, mean_q: 142.141739, mean_eps: 0.010000\n",
            " 17106/50000: episode: 191, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 9.156161, mae: 71.542954, mean_q: 141.944901, mean_eps: 0.010000\n",
            " 17306/50000: episode: 192, duration: 2.002s, episode steps: 200, steps per second: 100, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 7.186475, mae: 70.519750, mean_q: 139.894804, mean_eps: 0.010000\n",
            " 17506/50000: episode: 193, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.504247, mae: 71.148015, mean_q: 141.201535, mean_eps: 0.010000\n",
            " 17609/50000: episode: 194, duration: 0.742s, episode steps: 103, steps per second: 139, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 16.301444, mae: 70.892807, mean_q: 140.808563, mean_eps: 0.010000\n",
            " 17728/50000: episode: 195, duration: 0.864s, episode steps: 119, steps per second: 138, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 16.061247, mae: 70.052761, mean_q: 138.999514, mean_eps: 0.010000\n",
            " 17866/50000: episode: 196, duration: 0.981s, episode steps: 138, steps per second: 141, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 16.217925, mae: 69.507125, mean_q: 137.506069, mean_eps: 0.010000\n",
            " 18066/50000: episode: 197, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 18.732524, mae: 69.102956, mean_q: 137.004827, mean_eps: 0.010000\n",
            " 18266/50000: episode: 198, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 21.812367, mae: 69.212734, mean_q: 137.391841, mean_eps: 0.010000\n",
            " 18466/50000: episode: 199, duration: 1.434s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 20.694896, mae: 68.646595, mean_q: 136.439905, mean_eps: 0.010000\n",
            " 18666/50000: episode: 200, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 13.879119, mae: 68.303695, mean_q: 135.561276, mean_eps: 0.010000\n",
            " 18866/50000: episode: 201, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 19.959501, mae: 67.536309, mean_q: 133.846218, mean_eps: 0.010000\n",
            " 19066/50000: episode: 202, duration: 2.058s, episode steps: 200, steps per second:  97, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16.728781, mae: 66.637056, mean_q: 132.256239, mean_eps: 0.010000\n",
            " 19266/50000: episode: 203, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.271144, mae: 66.613506, mean_q: 132.241391, mean_eps: 0.010000\n",
            " 19466/50000: episode: 204, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 44.182220, mae: 65.972455, mean_q: 130.359259, mean_eps: 0.010000\n",
            " 19666/50000: episode: 205, duration: 1.474s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.439746, mae: 65.232611, mean_q: 129.344661, mean_eps: 0.010000\n",
            " 19866/50000: episode: 206, duration: 1.436s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.323597, mae: 65.869625, mean_q: 130.979536, mean_eps: 0.010000\n",
            " 20066/50000: episode: 207, duration: 1.423s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.410070, mae: 65.433862, mean_q: 129.565095, mean_eps: 0.010000\n",
            " 20266/50000: episode: 208, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.538870, mae: 65.556396, mean_q: 130.219375, mean_eps: 0.010000\n",
            " 20466/50000: episode: 209, duration: 1.617s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 20.378493, mae: 64.740756, mean_q: 128.561686, mean_eps: 0.010000\n",
            " 20666/50000: episode: 210, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 40.897514, mae: 65.120459, mean_q: 128.667138, mean_eps: 0.010000\n",
            " 20866/50000: episode: 211, duration: 1.490s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.988217, mae: 64.857538, mean_q: 129.135088, mean_eps: 0.010000\n",
            " 21066/50000: episode: 212, duration: 1.436s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 32.471101, mae: 64.709381, mean_q: 128.610050, mean_eps: 0.010000\n",
            " 21266/50000: episode: 213, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.819844, mae: 65.330723, mean_q: 129.497401, mean_eps: 0.010000\n",
            " 21466/50000: episode: 214, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 28.390586, mae: 65.322407, mean_q: 129.792771, mean_eps: 0.010000\n",
            " 21666/50000: episode: 215, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 33.888256, mae: 66.134576, mean_q: 131.378836, mean_eps: 0.010000\n",
            " 21866/50000: episode: 216, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 46.563131, mae: 65.727187, mean_q: 129.908220, mean_eps: 0.010000\n",
            " 21934/50000: episode: 217, duration: 0.498s, episode steps:  68, steps per second: 137, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.574 [0.000, 1.000],  loss: 33.603356, mae: 65.770230, mean_q: 129.860304, mean_eps: 0.010000\n",
            " 21944/50000: episode: 218, duration: 0.084s, episode steps:  10, steps per second: 119, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.753832, mae: 68.035292, mean_q: 135.722456, mean_eps: 0.010000\n",
            " 22093/50000: episode: 219, duration: 1.212s, episode steps: 149, steps per second: 123, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 40.781630, mae: 65.988925, mean_q: 130.137270, mean_eps: 0.010000\n",
            " 22103/50000: episode: 220, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 5.287666, mae: 66.098792, mean_q: 131.551991, mean_eps: 0.010000\n",
            " 22114/50000: episode: 221, duration: 0.131s, episode steps:  11, steps per second:  84, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 42.355864, mae: 64.564399, mean_q: 127.844258, mean_eps: 0.010000\n",
            " 22314/50000: episode: 222, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 44.345932, mae: 65.608412, mean_q: 129.427086, mean_eps: 0.010000\n",
            " 22330/50000: episode: 223, duration: 0.114s, episode steps:  16, steps per second: 140, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.812 [0.000, 1.000],  loss: 43.238372, mae: 67.805020, mean_q: 133.433870, mean_eps: 0.010000\n",
            " 22530/50000: episode: 224, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 48.777560, mae: 65.383847, mean_q: 128.861017, mean_eps: 0.010000\n",
            " 22730/50000: episode: 225, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 32.549432, mae: 65.435628, mean_q: 129.691532, mean_eps: 0.010000\n",
            " 22930/50000: episode: 226, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 46.589414, mae: 66.244497, mean_q: 130.846883, mean_eps: 0.010000\n",
            " 23130/50000: episode: 227, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 29.587264, mae: 65.676548, mean_q: 129.899041, mean_eps: 0.010000\n",
            " 23330/50000: episode: 228, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 40.251736, mae: 65.890588, mean_q: 130.499978, mean_eps: 0.010000\n",
            " 23530/50000: episode: 229, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 34.769220, mae: 65.711000, mean_q: 130.088514, mean_eps: 0.010000\n",
            " 23730/50000: episode: 230, duration: 1.640s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 42.938318, mae: 66.146214, mean_q: 131.103162, mean_eps: 0.010000\n",
            " 23930/50000: episode: 231, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 35.640391, mae: 65.842228, mean_q: 130.557019, mean_eps: 0.010000\n",
            " 24130/50000: episode: 232, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 37.933392, mae: 65.917966, mean_q: 130.637736, mean_eps: 0.010000\n",
            " 24330/50000: episode: 233, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 37.318914, mae: 65.495897, mean_q: 129.952881, mean_eps: 0.010000\n",
            " 24420/50000: episode: 234, duration: 0.674s, episode steps:  90, steps per second: 134, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 52.702297, mae: 65.691725, mean_q: 129.888018, mean_eps: 0.010000\n",
            " 24620/50000: episode: 235, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 33.801156, mae: 64.883360, mean_q: 128.666912, mean_eps: 0.010000\n",
            " 24704/50000: episode: 236, duration: 0.586s, episode steps:  84, steps per second: 143, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 51.178736, mae: 65.401040, mean_q: 129.450576, mean_eps: 0.010000\n",
            " 24904/50000: episode: 237, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 43.729544, mae: 64.288383, mean_q: 127.505647, mean_eps: 0.010000\n",
            " 25104/50000: episode: 238, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 33.892008, mae: 64.583965, mean_q: 128.351865, mean_eps: 0.010000\n",
            " 25277/50000: episode: 239, duration: 1.310s, episode steps: 173, steps per second: 132, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 19.110298, mae: 64.755071, mean_q: 128.995958, mean_eps: 0.010000\n",
            " 25477/50000: episode: 240, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 40.522692, mae: 64.207197, mean_q: 127.166424, mean_eps: 0.010000\n",
            " 25677/50000: episode: 241, duration: 1.709s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 39.641468, mae: 64.028018, mean_q: 126.945837, mean_eps: 0.010000\n",
            " 25832/50000: episode: 242, duration: 1.144s, episode steps: 155, steps per second: 135, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 45.983921, mae: 63.400274, mean_q: 125.508732, mean_eps: 0.010000\n",
            " 25981/50000: episode: 243, duration: 1.096s, episode steps: 149, steps per second: 136, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 27.309396, mae: 62.971429, mean_q: 125.292621, mean_eps: 0.010000\n",
            " 26147/50000: episode: 244, duration: 1.210s, episode steps: 166, steps per second: 137, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 33.998215, mae: 62.859972, mean_q: 124.672546, mean_eps: 0.010000\n",
            " 26347/50000: episode: 245, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 21.415248, mae: 62.716205, mean_q: 124.406249, mean_eps: 0.010000\n",
            " 26507/50000: episode: 246, duration: 1.157s, episode steps: 160, steps per second: 138, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 44.687806, mae: 62.101535, mean_q: 122.469114, mean_eps: 0.010000\n",
            " 26537/50000: episode: 247, duration: 0.235s, episode steps:  30, steps per second: 128, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 82.963228, mae: 61.900936, mean_q: 121.146430, mean_eps: 0.010000\n",
            " 26639/50000: episode: 248, duration: 0.762s, episode steps: 102, steps per second: 134, episode reward: 102.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.461 [0.000, 1.000],  loss: 49.153331, mae: 61.843813, mean_q: 122.095590, mean_eps: 0.010000\n",
            " 26839/50000: episode: 249, duration: 1.487s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 34.043485, mae: 60.614398, mean_q: 120.077745, mean_eps: 0.010000\n",
            " 26990/50000: episode: 250, duration: 1.378s, episode steps: 151, steps per second: 110, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 27.216821, mae: 60.423407, mean_q: 119.797818, mean_eps: 0.010000\n",
            " 27190/50000: episode: 251, duration: 2.054s, episode steps: 200, steps per second:  97, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.764447, mae: 59.671581, mean_q: 118.602415, mean_eps: 0.010000\n",
            " 27390/50000: episode: 252, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 30.199483, mae: 59.764945, mean_q: 118.446747, mean_eps: 0.010000\n",
            " 27590/50000: episode: 253, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 25.678730, mae: 59.431259, mean_q: 118.144076, mean_eps: 0.010000\n",
            " 27790/50000: episode: 254, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 29.914484, mae: 59.051316, mean_q: 117.185639, mean_eps: 0.010000\n",
            " 27990/50000: episode: 255, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.465242, mae: 59.002482, mean_q: 117.345792, mean_eps: 0.010000\n",
            " 28190/50000: episode: 256, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 34.655229, mae: 58.601676, mean_q: 116.544375, mean_eps: 0.010000\n",
            " 28390/50000: episode: 257, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 29.420231, mae: 58.374860, mean_q: 116.050267, mean_eps: 0.010000\n",
            " 28590/50000: episode: 258, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 28.768422, mae: 57.743650, mean_q: 115.030888, mean_eps: 0.010000\n",
            " 28790/50000: episode: 259, duration: 2.083s, episode steps: 200, steps per second:  96, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.786217, mae: 57.593962, mean_q: 114.689188, mean_eps: 0.010000\n",
            " 28990/50000: episode: 260, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.965890, mae: 57.601879, mean_q: 114.713112, mean_eps: 0.010000\n",
            " 29190/50000: episode: 261, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 26.641361, mae: 57.503629, mean_q: 114.698566, mean_eps: 0.010000\n",
            " 29390/50000: episode: 262, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 26.598575, mae: 57.278976, mean_q: 114.192400, mean_eps: 0.010000\n",
            " 29590/50000: episode: 263, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 30.943677, mae: 56.769328, mean_q: 113.151349, mean_eps: 0.010000\n",
            " 29790/50000: episode: 264, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 31.915265, mae: 56.771403, mean_q: 112.999696, mean_eps: 0.010000\n",
            " 29990/50000: episode: 265, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 33.406342, mae: 56.026973, mean_q: 111.454895, mean_eps: 0.010000\n",
            " 30190/50000: episode: 266, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 24.579375, mae: 56.108960, mean_q: 111.888754, mean_eps: 0.010000\n",
            " 30390/50000: episode: 267, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 32.313048, mae: 55.875334, mean_q: 111.244772, mean_eps: 0.010000\n",
            " 30590/50000: episode: 268, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.615976, mae: 55.555599, mean_q: 110.656019, mean_eps: 0.010000\n",
            " 30790/50000: episode: 269, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 29.289925, mae: 55.424591, mean_q: 110.531788, mean_eps: 0.010000\n",
            " 30990/50000: episode: 270, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 24.280370, mae: 55.351282, mean_q: 110.380465, mean_eps: 0.010000\n",
            " 31190/50000: episode: 271, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.234161, mae: 54.811414, mean_q: 109.265464, mean_eps: 0.010000\n",
            " 31390/50000: episode: 272, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 25.870836, mae: 54.818897, mean_q: 109.625349, mean_eps: 0.010000\n",
            " 31590/50000: episode: 273, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 29.827827, mae: 54.222208, mean_q: 108.170954, mean_eps: 0.010000\n",
            " 31790/50000: episode: 274, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 28.232614, mae: 54.338022, mean_q: 108.392495, mean_eps: 0.010000\n",
            " 31990/50000: episode: 275, duration: 2.036s, episode steps: 200, steps per second:  98, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 25.665350, mae: 53.759448, mean_q: 107.417471, mean_eps: 0.010000\n",
            " 32190/50000: episode: 276, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.327430, mae: 53.574752, mean_q: 106.507529, mean_eps: 0.010000\n",
            " 32250/50000: episode: 277, duration: 0.437s, episode steps:  60, steps per second: 137, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 21.692133, mae: 53.594688, mean_q: 106.800395, mean_eps: 0.010000\n",
            " 32450/50000: episode: 278, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.030499, mae: 53.141491, mean_q: 105.999669, mean_eps: 0.010000\n",
            " 32650/50000: episode: 279, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 24.559993, mae: 52.723640, mean_q: 105.149168, mean_eps: 0.010000\n",
            " 32850/50000: episode: 280, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 25.063152, mae: 52.635598, mean_q: 104.970980, mean_eps: 0.010000\n",
            " 33050/50000: episode: 281, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.766647, mae: 52.309512, mean_q: 104.483510, mean_eps: 0.010000\n",
            " 33250/50000: episode: 282, duration: 1.445s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 20.017033, mae: 51.947516, mean_q: 103.839620, mean_eps: 0.010000\n",
            " 33450/50000: episode: 283, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.827299, mae: 52.005876, mean_q: 103.893553, mean_eps: 0.010000\n",
            " 33650/50000: episode: 284, duration: 2.132s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 25.201514, mae: 51.726778, mean_q: 103.197293, mean_eps: 0.010000\n",
            " 33850/50000: episode: 285, duration: 1.731s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.561728, mae: 51.463328, mean_q: 102.684100, mean_eps: 0.010000\n",
            " 34050/50000: episode: 286, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.539453, mae: 51.251493, mean_q: 102.360285, mean_eps: 0.010000\n",
            " 34250/50000: episode: 287, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.206463, mae: 50.877768, mean_q: 101.579532, mean_eps: 0.010000\n",
            " 34450/50000: episode: 288, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.783430, mae: 50.626881, mean_q: 101.062643, mean_eps: 0.010000\n",
            " 34650/50000: episode: 289, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22.177599, mae: 50.272174, mean_q: 100.395108, mean_eps: 0.010000\n",
            " 34850/50000: episode: 290, duration: 1.533s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.947830, mae: 50.058823, mean_q: 100.347037, mean_eps: 0.010000\n",
            " 35050/50000: episode: 291, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 19.460736, mae: 49.897738, mean_q: 99.809434, mean_eps: 0.010000\n",
            " 35250/50000: episode: 292, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.191558, mae: 49.956813, mean_q: 99.548949, mean_eps: 0.010000\n",
            " 35450/50000: episode: 293, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 29.859505, mae: 49.911742, mean_q: 99.258460, mean_eps: 0.010000\n",
            " 35650/50000: episode: 294, duration: 1.499s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27.170276, mae: 49.520596, mean_q: 98.413759, mean_eps: 0.010000\n",
            " 35850/50000: episode: 295, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.037454, mae: 49.347713, mean_q: 98.395213, mean_eps: 0.010000\n",
            " 36050/50000: episode: 296, duration: 1.581s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.738405, mae: 49.333023, mean_q: 98.459466, mean_eps: 0.010000\n",
            " 36138/50000: episode: 297, duration: 0.694s, episode steps:  88, steps per second: 127, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 10.866808, mae: 49.483838, mean_q: 99.096925, mean_eps: 0.010000\n",
            " 36338/50000: episode: 298, duration: 1.548s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 25.584798, mae: 49.326214, mean_q: 98.182240, mean_eps: 0.010000\n",
            " 36538/50000: episode: 299, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 18.569444, mae: 48.849741, mean_q: 97.307770, mean_eps: 0.010000\n",
            " 36738/50000: episode: 300, duration: 2.154s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 21.085430, mae: 48.453806, mean_q: 96.494145, mean_eps: 0.010000\n",
            " 36938/50000: episode: 301, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 22.713382, mae: 48.128696, mean_q: 95.769001, mean_eps: 0.010000\n",
            " 37035/50000: episode: 302, duration: 0.730s, episode steps:  97, steps per second: 133, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 20.571387, mae: 47.931736, mean_q: 95.788699, mean_eps: 0.010000\n",
            " 37157/50000: episode: 303, duration: 0.900s, episode steps: 122, steps per second: 136, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.706627, mae: 47.846725, mean_q: 95.531496, mean_eps: 0.010000\n",
            " 37219/50000: episode: 304, duration: 0.462s, episode steps:  62, steps per second: 134, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.234276, mae: 47.767910, mean_q: 95.388250, mean_eps: 0.010000\n",
            " 37295/50000: episode: 305, duration: 0.568s, episode steps:  76, steps per second: 134, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 14.029922, mae: 47.606407, mean_q: 95.551021, mean_eps: 0.010000\n",
            " 37378/50000: episode: 306, duration: 0.609s, episode steps:  83, steps per second: 136, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 12.870059, mae: 47.684312, mean_q: 95.608141, mean_eps: 0.010000\n",
            " 37488/50000: episode: 307, duration: 0.832s, episode steps: 110, steps per second: 132, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.516947, mae: 47.519150, mean_q: 94.942555, mean_eps: 0.010000\n",
            " 37585/50000: episode: 308, duration: 0.746s, episode steps:  97, steps per second: 130, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 18.238451, mae: 47.246213, mean_q: 94.803537, mean_eps: 0.010000\n",
            " 37696/50000: episode: 309, duration: 0.814s, episode steps: 111, steps per second: 136, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 15.304994, mae: 47.325833, mean_q: 95.112208, mean_eps: 0.010000\n",
            " 37781/50000: episode: 310, duration: 0.640s, episode steps:  85, steps per second: 133, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 16.074954, mae: 47.543250, mean_q: 95.398973, mean_eps: 0.010000\n",
            " 37981/50000: episode: 311, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 24.346736, mae: 47.208070, mean_q: 94.377184, mean_eps: 0.010000\n",
            " 38181/50000: episode: 312, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15.993751, mae: 46.748882, mean_q: 93.976092, mean_eps: 0.010000\n",
            " 38381/50000: episode: 313, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 17.240727, mae: 46.877044, mean_q: 94.174026, mean_eps: 0.010000\n",
            " 38581/50000: episode: 314, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 19.321334, mae: 46.630189, mean_q: 93.691875, mean_eps: 0.010000\n",
            " 38781/50000: episode: 315, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 19.912672, mae: 46.798191, mean_q: 93.995733, mean_eps: 0.010000\n",
            " 38981/50000: episode: 316, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 21.231348, mae: 46.552383, mean_q: 93.593964, mean_eps: 0.010000\n",
            " 39181/50000: episode: 317, duration: 1.495s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 21.012525, mae: 46.441912, mean_q: 93.281436, mean_eps: 0.010000\n",
            " 39381/50000: episode: 318, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 12.705870, mae: 46.431064, mean_q: 93.771286, mean_eps: 0.010000\n",
            " 39581/50000: episode: 319, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 17.780475, mae: 46.804430, mean_q: 94.206879, mean_eps: 0.010000\n",
            " 39781/50000: episode: 320, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 14.413126, mae: 47.241544, mean_q: 95.046324, mean_eps: 0.010000\n",
            " 39981/50000: episode: 321, duration: 2.073s, episode steps: 200, steps per second:  96, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 21.921908, mae: 47.264771, mean_q: 95.060190, mean_eps: 0.010000\n",
            " 40181/50000: episode: 322, duration: 1.711s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15.917523, mae: 47.299654, mean_q: 95.587156, mean_eps: 0.010000\n",
            " 40381/50000: episode: 323, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 22.848439, mae: 47.855173, mean_q: 96.141017, mean_eps: 0.010000\n",
            " 40581/50000: episode: 324, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.576387, mae: 47.883357, mean_q: 96.202838, mean_eps: 0.010000\n",
            " 40781/50000: episode: 325, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.937671, mae: 47.954465, mean_q: 96.506094, mean_eps: 0.010000\n",
            " 40981/50000: episode: 326, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 19.127852, mae: 48.156573, mean_q: 96.897999, mean_eps: 0.010000\n",
            " 41181/50000: episode: 327, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 23.075365, mae: 48.144502, mean_q: 96.735427, mean_eps: 0.010000\n",
            " 41381/50000: episode: 328, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 25.207808, mae: 47.964633, mean_q: 96.277869, mean_eps: 0.010000\n",
            " 41581/50000: episode: 329, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 27.934759, mae: 48.145743, mean_q: 96.385265, mean_eps: 0.010000\n",
            " 41781/50000: episode: 330, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 17.949986, mae: 47.952905, mean_q: 96.458879, mean_eps: 0.010000\n",
            " 41981/50000: episode: 331, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.763645, mae: 47.912810, mean_q: 96.511819, mean_eps: 0.010000\n",
            " 42181/50000: episode: 332, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.985419, mae: 48.153062, mean_q: 96.895062, mean_eps: 0.010000\n",
            " 42381/50000: episode: 333, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27.674072, mae: 48.207460, mean_q: 96.623270, mean_eps: 0.010000\n",
            " 42581/50000: episode: 334, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 24.902259, mae: 48.137921, mean_q: 96.486780, mean_eps: 0.010000\n",
            " 42781/50000: episode: 335, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 16.386414, mae: 48.082275, mean_q: 96.736411, mean_eps: 0.010000\n",
            " 42981/50000: episode: 336, duration: 1.472s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.696266, mae: 48.043332, mean_q: 96.918310, mean_eps: 0.010000\n",
            " 43181/50000: episode: 337, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 20.381623, mae: 48.237809, mean_q: 97.096309, mean_eps: 0.010000\n",
            " 43381/50000: episode: 338, duration: 1.871s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.447549, mae: 48.317512, mean_q: 97.278678, mean_eps: 0.010000\n",
            " 43581/50000: episode: 339, duration: 1.444s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22.318222, mae: 48.490170, mean_q: 97.351691, mean_eps: 0.010000\n",
            " 43781/50000: episode: 340, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 27.332519, mae: 48.442811, mean_q: 97.102764, mean_eps: 0.010000\n",
            " 43981/50000: episode: 341, duration: 1.480s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 22.129513, mae: 48.212959, mean_q: 96.908747, mean_eps: 0.010000\n",
            " 44181/50000: episode: 342, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 17.725437, mae: 48.040995, mean_q: 96.822073, mean_eps: 0.010000\n",
            " 44381/50000: episode: 343, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 20.328063, mae: 48.171027, mean_q: 96.691300, mean_eps: 0.010000\n",
            " 44581/50000: episode: 344, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 25.820256, mae: 48.107757, mean_q: 96.549810, mean_eps: 0.010000\n",
            " 44781/50000: episode: 345, duration: 1.851s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.922782, mae: 47.992928, mean_q: 96.678884, mean_eps: 0.010000\n",
            " 44981/50000: episode: 346, duration: 1.975s, episode steps: 200, steps per second: 101, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.204725, mae: 47.970929, mean_q: 96.329176, mean_eps: 0.010000\n",
            " 45181/50000: episode: 347, duration: 1.499s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.824806, mae: 48.232127, mean_q: 97.091471, mean_eps: 0.010000\n",
            " 45381/50000: episode: 348, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 24.323951, mae: 48.044819, mean_q: 96.459950, mean_eps: 0.010000\n",
            " 45581/50000: episode: 349, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 20.526150, mae: 48.250536, mean_q: 96.854211, mean_eps: 0.010000\n",
            " 45781/50000: episode: 350, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 18.086147, mae: 48.247213, mean_q: 97.081730, mean_eps: 0.010000\n",
            " 45981/50000: episode: 351, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 23.646422, mae: 48.446230, mean_q: 97.176855, mean_eps: 0.010000\n",
            " 46181/50000: episode: 352, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.634401, mae: 48.335433, mean_q: 96.977293, mean_eps: 0.010000\n",
            " 46381/50000: episode: 353, duration: 1.759s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 21.299249, mae: 48.542890, mean_q: 97.463615, mean_eps: 0.010000\n",
            " 46581/50000: episode: 354, duration: 2.013s, episode steps: 200, steps per second:  99, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 28.676753, mae: 48.626614, mean_q: 97.373384, mean_eps: 0.010000\n",
            " 46638/50000: episode: 355, duration: 0.431s, episode steps:  57, steps per second: 132, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.386 [0.000, 1.000],  loss: 33.875262, mae: 48.896854, mean_q: 97.683391, mean_eps: 0.010000\n",
            " 46838/50000: episode: 356, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 17.489181, mae: 48.406295, mean_q: 97.248634, mean_eps: 0.010000\n",
            " 47038/50000: episode: 357, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26.554858, mae: 48.446420, mean_q: 97.152773, mean_eps: 0.010000\n",
            " 47238/50000: episode: 358, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.536662, mae: 48.355380, mean_q: 97.014967, mean_eps: 0.010000\n",
            " 47438/50000: episode: 359, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.432806, mae: 48.493317, mean_q: 96.997480, mean_eps: 0.010000\n",
            " 47638/50000: episode: 360, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 14.810923, mae: 48.292980, mean_q: 97.100985, mean_eps: 0.010000\n",
            " 47838/50000: episode: 361, duration: 1.433s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 21.190458, mae: 48.334272, mean_q: 96.790498, mean_eps: 0.010000\n",
            " 48038/50000: episode: 362, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.444271, mae: 48.397107, mean_q: 96.825718, mean_eps: 0.010000\n",
            " 48238/50000: episode: 363, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.038410, mae: 48.295550, mean_q: 96.606320, mean_eps: 0.010000\n",
            " 48438/50000: episode: 364, duration: 1.515s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 20.237603, mae: 47.996732, mean_q: 95.980122, mean_eps: 0.010000\n",
            " 48638/50000: episode: 365, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 18.241708, mae: 47.931209, mean_q: 95.833040, mean_eps: 0.010000\n",
            " 48838/50000: episode: 366, duration: 1.441s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22.383832, mae: 47.956398, mean_q: 95.875420, mean_eps: 0.010000\n",
            " 49038/50000: episode: 367, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 17.125452, mae: 47.827229, mean_q: 95.823250, mean_eps: 0.010000\n",
            " 49238/50000: episode: 368, duration: 1.436s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 24.403078, mae: 47.781816, mean_q: 95.513171, mean_eps: 0.010000\n",
            " 49438/50000: episode: 369, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.680788, mae: 47.718756, mean_q: 95.562365, mean_eps: 0.010000\n",
            " 49638/50000: episode: 370, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 21.144660, mae: 47.753075, mean_q: 95.419440, mean_eps: 0.010000\n",
            " 49838/50000: episode: 371, duration: 2.074s, episode steps: 200, steps per second:  96, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 25.241418, mae: 47.695049, mean_q: 95.119992, mean_eps: 0.010000\n",
            "done, took 394.181 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3a0dca5130>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run params\n",
        "\n",
        "value_min = 0.05\n",
        "\n",
        "nb_steps_warmup=10\n",
        "\n",
        "target_model_update=1e-2\n",
        "\n",
        "nb_steps=10000\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "nb_episodes=20"
      ],
      "metadata": {
        "id": "Y8s7-Od_I_eH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89bc62d8-b9a1-496f-a1bf-2998a475eecd"
      },
      "source": [
        "policy_outer =  LinearAnnealedPolicy(inner_policy=policy_inner, \n",
        "                               attr='eps',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.05, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy_outer) \n",
        "\n",
        "dqn.compile(Adam(lr=0.01), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)\n",
        "\n",
        "plt.imshow(env.render(mode='rgb_array'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   16/10000: episode: 1, duration: 0.843s, episode steps:  16, steps per second:  19, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 17.407410, mae: 53.504501, mean_q: 108.467363, mean_eps: 0.998765\n",
            "   51/10000: episode: 2, duration: 0.288s, episode steps:  35, steps per second: 121, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 18.239691, mae: 54.091392, mean_q: 110.262780, mean_eps: 0.996865\n",
            "   61/10000: episode: 3, duration: 0.078s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 6.360101, mae: 53.901926, mean_q: 109.789648, mean_eps: 0.994728\n",
            "   87/10000: episode: 4, duration: 0.191s, episode steps:  26, steps per second: 136, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 8.230370, mae: 54.348962, mean_q: 110.774188, mean_eps: 0.993017\n",
            "  130/10000: episode: 5, duration: 0.312s, episode steps:  43, steps per second: 138, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.674 [0.000, 1.000],  loss: 24.556269, mae: 54.387600, mean_q: 110.552814, mean_eps: 0.989740\n",
            "  152/10000: episode: 6, duration: 0.162s, episode steps:  22, steps per second: 136, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 25.019023, mae: 55.055143, mean_q: 111.574637, mean_eps: 0.986653\n",
            "  170/10000: episode: 7, duration: 0.131s, episode steps:  18, steps per second: 138, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 17.649931, mae: 55.231638, mean_q: 111.755544, mean_eps: 0.984752\n",
            "  195/10000: episode: 8, duration: 0.175s, episode steps:  25, steps per second: 142, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 24.321596, mae: 54.331797, mean_q: 110.352592, mean_eps: 0.982710\n",
            "  232/10000: episode: 9, duration: 0.252s, episode steps:  37, steps per second: 147, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.595 [0.000, 1.000],  loss: 17.938661, mae: 54.794590, mean_q: 111.465947, mean_eps: 0.979765\n",
            "  247/10000: episode: 10, duration: 0.109s, episode steps:  15, steps per second: 137, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 28.272257, mae: 54.637204, mean_q: 111.146481, mean_eps: 0.977295\n",
            "  267/10000: episode: 11, duration: 0.149s, episode steps:  20, steps per second: 134, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 30.750463, mae: 54.828743, mean_q: 111.793353, mean_eps: 0.975633\n",
            "  314/10000: episode: 12, duration: 0.325s, episode steps:  47, steps per second: 145, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.404 [0.000, 1.000],  loss: 17.789547, mae: 55.390734, mean_q: 112.363183, mean_eps: 0.972450\n",
            "  327/10000: episode: 13, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 31.766178, mae: 54.288191, mean_q: 110.346976, mean_eps: 0.969600\n",
            "  345/10000: episode: 14, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 16.558519, mae: 55.271346, mean_q: 112.619773, mean_eps: 0.968128\n",
            "  368/10000: episode: 15, duration: 0.160s, episode steps:  23, steps per second: 144, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 23.973042, mae: 54.622727, mean_q: 110.773422, mean_eps: 0.966180\n",
            "  381/10000: episode: 16, duration: 0.097s, episode steps:  13, steps per second: 134, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 25.414367, mae: 55.920298, mean_q: 113.585657, mean_eps: 0.964470\n",
            "  394/10000: episode: 17, duration: 0.094s, episode steps:  13, steps per second: 139, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 14.354284, mae: 55.670773, mean_q: 113.153964, mean_eps: 0.963235\n",
            "  405/10000: episode: 18, duration: 0.080s, episode steps:  11, steps per second: 138, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 13.873582, mae: 55.607336, mean_q: 113.694466, mean_eps: 0.962095\n",
            "  455/10000: episode: 19, duration: 0.340s, episode steps:  50, steps per second: 147, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 22.191472, mae: 55.081715, mean_q: 111.756954, mean_eps: 0.959198\n",
            "  468/10000: episode: 20, duration: 0.094s, episode steps:  13, steps per second: 139, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 4.542776, mae: 55.534514, mean_q: 114.154836, mean_eps: 0.956205\n",
            "  486/10000: episode: 21, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 23.940796, mae: 54.323054, mean_q: 111.245872, mean_eps: 0.954733\n",
            "  503/10000: episode: 22, duration: 0.119s, episode steps:  17, steps per second: 143, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 9.139313, mae: 55.256800, mean_q: 113.357899, mean_eps: 0.953070\n",
            "  532/10000: episode: 23, duration: 0.202s, episode steps:  29, steps per second: 144, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 15.299319, mae: 55.380865, mean_q: 112.503923, mean_eps: 0.950885\n",
            "  552/10000: episode: 24, duration: 0.141s, episode steps:  20, steps per second: 142, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 18.707779, mae: 55.089151, mean_q: 113.042868, mean_eps: 0.948558\n",
            "  603/10000: episode: 25, duration: 0.349s, episode steps:  51, steps per second: 146, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.569 [0.000, 1.000],  loss: 21.477593, mae: 55.765815, mean_q: 113.601576, mean_eps: 0.945185\n",
            "  635/10000: episode: 26, duration: 0.224s, episode steps:  32, steps per second: 143, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 15.591861, mae: 56.457939, mean_q: 114.816902, mean_eps: 0.941242\n",
            "  651/10000: episode: 27, duration: 0.112s, episode steps:  16, steps per second: 143, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 23.697820, mae: 55.892479, mean_q: 113.704309, mean_eps: 0.938963\n",
            "  670/10000: episode: 28, duration: 0.137s, episode steps:  19, steps per second: 139, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 17.381679, mae: 55.346554, mean_q: 113.438096, mean_eps: 0.937300\n",
            "  687/10000: episode: 29, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 24.158215, mae: 56.667625, mean_q: 115.503457, mean_eps: 0.935590\n",
            "  702/10000: episode: 30, duration: 0.110s, episode steps:  15, steps per second: 137, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 8.975698, mae: 56.899668, mean_q: 115.779367, mean_eps: 0.934070\n",
            "  726/10000: episode: 31, duration: 0.187s, episode steps:  24, steps per second: 128, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 15.709382, mae: 55.696624, mean_q: 113.191028, mean_eps: 0.932218\n",
            "  812/10000: episode: 32, duration: 0.606s, episode steps:  86, steps per second: 142, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 10.961750, mae: 55.738082, mean_q: 114.129630, mean_eps: 0.926992\n",
            "  877/10000: episode: 33, duration: 0.465s, episode steps:  65, steps per second: 140, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 14.801719, mae: 56.848787, mean_q: 115.631340, mean_eps: 0.919820\n",
            "  886/10000: episode: 34, duration: 0.069s, episode steps:   9, steps per second: 130, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 7.381429, mae: 56.395668, mean_q: 115.010959, mean_eps: 0.916305\n",
            "  901/10000: episode: 35, duration: 0.108s, episode steps:  15, steps per second: 138, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 28.264275, mae: 57.239868, mean_q: 115.438774, mean_eps: 0.915165\n",
            "  921/10000: episode: 36, duration: 0.152s, episode steps:  20, steps per second: 131, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 25.943098, mae: 56.435228, mean_q: 115.052079, mean_eps: 0.913503\n",
            "  933/10000: episode: 37, duration: 0.085s, episode steps:  12, steps per second: 142, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 10.386767, mae: 56.878614, mean_q: 116.159302, mean_eps: 0.911983\n",
            "  968/10000: episode: 38, duration: 0.255s, episode steps:  35, steps per second: 137, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 18.902515, mae: 56.909210, mean_q: 116.118155, mean_eps: 0.909750\n",
            " 1003/10000: episode: 39, duration: 0.239s, episode steps:  35, steps per second: 146, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 25.876274, mae: 56.907722, mean_q: 115.561008, mean_eps: 0.906425\n",
            " 1029/10000: episode: 40, duration: 0.194s, episode steps:  26, steps per second: 134, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 10.510160, mae: 57.288354, mean_q: 116.422626, mean_eps: 0.903527\n",
            " 1062/10000: episode: 41, duration: 0.239s, episode steps:  33, steps per second: 138, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 20.300529, mae: 56.996578, mean_q: 116.275363, mean_eps: 0.900725\n",
            " 1096/10000: episode: 42, duration: 0.244s, episode steps:  34, steps per second: 139, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 32.913569, mae: 56.948902, mean_q: 115.149106, mean_eps: 0.897543\n",
            " 1136/10000: episode: 43, duration: 0.282s, episode steps:  40, steps per second: 142, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 20.137695, mae: 56.964904, mean_q: 115.965831, mean_eps: 0.894027\n",
            " 1148/10000: episode: 44, duration: 0.100s, episode steps:  12, steps per second: 120, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 17.216037, mae: 56.789007, mean_q: 115.623101, mean_eps: 0.891558\n",
            " 1164/10000: episode: 45, duration: 0.114s, episode steps:  16, steps per second: 140, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 24.574488, mae: 56.238425, mean_q: 114.306491, mean_eps: 0.890228\n",
            " 1177/10000: episode: 46, duration: 0.130s, episode steps:  13, steps per second: 100, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 20.706904, mae: 56.143029, mean_q: 114.788233, mean_eps: 0.888850\n",
            " 1206/10000: episode: 47, duration: 0.671s, episode steps:  29, steps per second:  43, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 17.240890, mae: 57.210038, mean_q: 115.674872, mean_eps: 0.886855\n",
            " 1229/10000: episode: 48, duration: 0.626s, episode steps:  23, steps per second:  37, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 33.639413, mae: 57.268080, mean_q: 115.656580, mean_eps: 0.884385\n",
            " 1240/10000: episode: 49, duration: 0.299s, episode steps:  11, steps per second:  37, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 11.862974, mae: 57.163534, mean_q: 115.882575, mean_eps: 0.882770\n",
            " 1271/10000: episode: 50, duration: 0.438s, episode steps:  31, steps per second:  71, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 31.332851, mae: 57.260493, mean_q: 115.962818, mean_eps: 0.880775\n",
            " 1285/10000: episode: 51, duration: 0.240s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 31.568310, mae: 57.393322, mean_q: 116.446214, mean_eps: 0.878637\n",
            " 1311/10000: episode: 52, duration: 0.463s, episode steps:  26, steps per second:  56, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 10.867533, mae: 57.377993, mean_q: 116.392908, mean_eps: 0.876738\n",
            " 1333/10000: episode: 53, duration: 0.326s, episode steps:  22, steps per second:  67, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 22.126633, mae: 57.177974, mean_q: 116.055815, mean_eps: 0.874458\n",
            " 1356/10000: episode: 54, duration: 0.392s, episode steps:  23, steps per second:  59, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 29.415035, mae: 57.476240, mean_q: 116.862542, mean_eps: 0.872320\n",
            " 1372/10000: episode: 55, duration: 0.120s, episode steps:  16, steps per second: 133, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 33.092614, mae: 56.177755, mean_q: 114.141886, mean_eps: 0.870468\n",
            " 1399/10000: episode: 56, duration: 0.271s, episode steps:  27, steps per second: 100, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 39.564091, mae: 57.764003, mean_q: 116.291124, mean_eps: 0.868425\n",
            " 1418/10000: episode: 57, duration: 0.265s, episode steps:  19, steps per second:  72, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 16.303254, mae: 56.138179, mean_q: 113.804893, mean_eps: 0.866240\n",
            " 1454/10000: episode: 58, duration: 0.556s, episode steps:  36, steps per second:  65, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 12.478169, mae: 57.283611, mean_q: 117.065267, mean_eps: 0.863628\n",
            " 1471/10000: episode: 59, duration: 0.186s, episode steps:  17, steps per second:  92, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 38.399249, mae: 58.002994, mean_q: 118.380951, mean_eps: 0.861110\n",
            " 1514/10000: episode: 60, duration: 0.384s, episode steps:  43, steps per second: 112, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 19.186173, mae: 58.095166, mean_q: 118.597238, mean_eps: 0.858260\n",
            " 1544/10000: episode: 61, duration: 0.218s, episode steps:  30, steps per second: 137, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 9.150847, mae: 57.281468, mean_q: 116.968450, mean_eps: 0.854792\n",
            " 1562/10000: episode: 62, duration: 0.137s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 31.091028, mae: 57.772004, mean_q: 117.319184, mean_eps: 0.852513\n",
            " 1575/10000: episode: 63, duration: 0.101s, episode steps:  13, steps per second: 129, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 32.138974, mae: 58.782570, mean_q: 118.592782, mean_eps: 0.851040\n",
            " 1588/10000: episode: 64, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 21.934964, mae: 57.765218, mean_q: 117.247284, mean_eps: 0.849805\n",
            " 1606/10000: episode: 65, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 40.621525, mae: 58.237349, mean_q: 118.197014, mean_eps: 0.848333\n",
            " 1623/10000: episode: 66, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 59.077480, mae: 57.664351, mean_q: 116.423974, mean_eps: 0.846670\n",
            " 1640/10000: episode: 67, duration: 0.135s, episode steps:  17, steps per second: 126, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 21.910921, mae: 58.671105, mean_q: 118.783235, mean_eps: 0.845055\n",
            " 1660/10000: episode: 68, duration: 0.373s, episode steps:  20, steps per second:  54, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.486696, mae: 58.467692, mean_q: 119.229103, mean_eps: 0.843298\n",
            " 1719/10000: episode: 69, duration: 0.816s, episode steps:  59, steps per second:  72, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 15.589752, mae: 57.748222, mean_q: 117.612603, mean_eps: 0.839545\n",
            " 1739/10000: episode: 70, duration: 0.149s, episode steps:  20, steps per second: 134, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 27.367006, mae: 58.430492, mean_q: 118.971891, mean_eps: 0.835792\n",
            " 1765/10000: episode: 71, duration: 0.192s, episode steps:  26, steps per second: 136, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.929228, mae: 56.801608, mean_q: 116.733042, mean_eps: 0.833608\n",
            " 1781/10000: episode: 72, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 43.304092, mae: 59.292329, mean_q: 120.945100, mean_eps: 0.831612\n",
            " 1799/10000: episode: 73, duration: 0.274s, episode steps:  18, steps per second:  66, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 16.658474, mae: 58.697563, mean_q: 120.281888, mean_eps: 0.829998\n",
            " 1914/10000: episode: 74, duration: 1.033s, episode steps: 115, steps per second: 111, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 17.431269, mae: 58.654146, mean_q: 119.676719, mean_eps: 0.823680\n",
            " 1931/10000: episode: 75, duration: 0.172s, episode steps:  17, steps per second:  99, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 18.015449, mae: 58.992459, mean_q: 119.968295, mean_eps: 0.817410\n",
            " 1958/10000: episode: 76, duration: 0.347s, episode steps:  27, steps per second:  78, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 23.608489, mae: 57.992378, mean_q: 117.825496, mean_eps: 0.815320\n",
            " 2052/10000: episode: 77, duration: 0.942s, episode steps:  94, steps per second: 100, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 20.011129, mae: 58.453675, mean_q: 118.840089, mean_eps: 0.809573\n",
            " 2069/10000: episode: 78, duration: 0.182s, episode steps:  17, steps per second:  93, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 28.000331, mae: 59.485185, mean_q: 120.754520, mean_eps: 0.804300\n",
            " 2086/10000: episode: 79, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 36.634631, mae: 58.485748, mean_q: 118.931444, mean_eps: 0.802685\n",
            " 2105/10000: episode: 80, duration: 0.211s, episode steps:  19, steps per second:  90, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 23.243366, mae: 58.970296, mean_q: 119.838726, mean_eps: 0.800975\n",
            " 2118/10000: episode: 81, duration: 0.179s, episode steps:  13, steps per second:  73, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 8.860539, mae: 58.997394, mean_q: 120.099848, mean_eps: 0.799455\n",
            " 2130/10000: episode: 82, duration: 0.128s, episode steps:  12, steps per second:  93, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.696496, mae: 59.928705, mean_q: 121.909667, mean_eps: 0.798268\n",
            " 2164/10000: episode: 83, duration: 0.253s, episode steps:  34, steps per second: 135, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 27.405494, mae: 59.545592, mean_q: 120.911298, mean_eps: 0.796082\n",
            " 2188/10000: episode: 84, duration: 0.181s, episode steps:  24, steps per second: 133, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 21.064294, mae: 59.681295, mean_q: 120.872911, mean_eps: 0.793328\n",
            " 2229/10000: episode: 85, duration: 0.374s, episode steps:  41, steps per second: 110, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 29.210659, mae: 58.736949, mean_q: 119.034389, mean_eps: 0.790240\n",
            " 2286/10000: episode: 86, duration: 0.752s, episode steps:  57, steps per second:  76, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 28.800451, mae: 59.085786, mean_q: 119.659638, mean_eps: 0.785585\n",
            " 2345/10000: episode: 87, duration: 0.605s, episode steps:  59, steps per second:  98, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 20.675151, mae: 59.009974, mean_q: 120.425833, mean_eps: 0.780075\n",
            " 2397/10000: episode: 88, duration: 0.503s, episode steps:  52, steps per second: 103, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 27.726470, mae: 59.070139, mean_q: 119.856755, mean_eps: 0.774803\n",
            " 2417/10000: episode: 89, duration: 0.220s, episode steps:  20, steps per second:  91, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 30.907041, mae: 58.845388, mean_q: 119.391002, mean_eps: 0.771382\n",
            " 2430/10000: episode: 90, duration: 0.134s, episode steps:  13, steps per second:  97, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 22.439149, mae: 58.673122, mean_q: 120.010207, mean_eps: 0.769815\n",
            " 2470/10000: episode: 91, duration: 0.397s, episode steps:  40, steps per second: 101, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 23.741036, mae: 59.566000, mean_q: 120.881677, mean_eps: 0.767297\n",
            " 2486/10000: episode: 92, duration: 0.160s, episode steps:  16, steps per second: 100, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 19.791427, mae: 59.224767, mean_q: 119.820220, mean_eps: 0.764637\n",
            " 2526/10000: episode: 93, duration: 0.420s, episode steps:  40, steps per second:  95, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 19.978536, mae: 59.496742, mean_q: 121.139813, mean_eps: 0.761977\n",
            " 2583/10000: episode: 94, duration: 0.510s, episode steps:  57, steps per second: 112, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 23.343392, mae: 59.591812, mean_q: 121.140295, mean_eps: 0.757370\n",
            " 2613/10000: episode: 95, duration: 0.221s, episode steps:  30, steps per second: 135, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 29.069702, mae: 59.587723, mean_q: 120.490179, mean_eps: 0.753237\n",
            " 2641/10000: episode: 96, duration: 0.189s, episode steps:  28, steps per second: 148, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.035796, mae: 59.324821, mean_q: 120.307758, mean_eps: 0.750482\n",
            " 2671/10000: episode: 97, duration: 0.222s, episode steps:  30, steps per second: 135, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 37.520112, mae: 59.419063, mean_q: 119.550712, mean_eps: 0.747728\n",
            " 2694/10000: episode: 98, duration: 0.154s, episode steps:  23, steps per second: 149, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 28.792754, mae: 58.647334, mean_q: 119.698802, mean_eps: 0.745210\n",
            " 2753/10000: episode: 99, duration: 0.402s, episode steps:  59, steps per second: 147, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 25.678175, mae: 60.221133, mean_q: 122.022419, mean_eps: 0.741315\n",
            " 2762/10000: episode: 100, duration: 0.069s, episode steps:   9, steps per second: 131, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 9.522347, mae: 59.249624, mean_q: 120.919748, mean_eps: 0.738085\n",
            " 2785/10000: episode: 101, duration: 0.165s, episode steps:  23, steps per second: 140, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 25.607278, mae: 60.022580, mean_q: 122.232185, mean_eps: 0.736565\n",
            " 2985/10000: episode: 102, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 25.963603, mae: 60.358727, mean_q: 122.394036, mean_eps: 0.725973\n",
            " 3012/10000: episode: 103, duration: 0.190s, episode steps:  27, steps per second: 142, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 17.130429, mae: 60.719741, mean_q: 123.112263, mean_eps: 0.715190\n",
            " 3068/10000: episode: 104, duration: 0.399s, episode steps:  56, steps per second: 140, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 11.572797, mae: 59.724090, mean_q: 121.600359, mean_eps: 0.711247\n",
            " 3094/10000: episode: 105, duration: 0.194s, episode steps:  26, steps per second: 134, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 35.028597, mae: 60.811323, mean_q: 123.094010, mean_eps: 0.707353\n",
            " 3164/10000: episode: 106, duration: 0.491s, episode steps:  70, steps per second: 142, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.262020, mae: 59.684519, mean_q: 121.659492, mean_eps: 0.702793\n",
            " 3185/10000: episode: 107, duration: 0.158s, episode steps:  21, steps per second: 133, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 22.414358, mae: 60.631349, mean_q: 123.044180, mean_eps: 0.698470\n",
            " 3197/10000: episode: 108, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 47.806573, mae: 60.313002, mean_q: 121.455984, mean_eps: 0.696903\n",
            " 3224/10000: episode: 109, duration: 0.194s, episode steps:  27, steps per second: 139, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 26.447004, mae: 59.328802, mean_q: 120.636795, mean_eps: 0.695050\n",
            " 3263/10000: episode: 110, duration: 0.274s, episode steps:  39, steps per second: 143, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 27.740754, mae: 59.611847, mean_q: 121.099089, mean_eps: 0.691915\n",
            " 3383/10000: episode: 111, duration: 0.825s, episode steps: 120, steps per second: 145, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27.388616, mae: 60.055872, mean_q: 122.111030, mean_eps: 0.684363\n",
            " 3416/10000: episode: 112, duration: 0.224s, episode steps:  33, steps per second: 147, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 15.359640, mae: 60.172218, mean_q: 122.021220, mean_eps: 0.677095\n",
            " 3446/10000: episode: 113, duration: 0.213s, episode steps:  30, steps per second: 141, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 24.872899, mae: 60.671915, mean_q: 123.024809, mean_eps: 0.674103\n",
            " 3456/10000: episode: 114, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 18.503160, mae: 60.247583, mean_q: 122.609944, mean_eps: 0.672203\n",
            " 3490/10000: episode: 115, duration: 0.245s, episode steps:  34, steps per second: 139, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.618 [0.000, 1.000],  loss: 34.362126, mae: 60.824705, mean_q: 122.666251, mean_eps: 0.670112\n",
            " 3517/10000: episode: 116, duration: 0.199s, episode steps:  27, steps per second: 136, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 22.226120, mae: 60.211087, mean_q: 122.322995, mean_eps: 0.667215\n",
            " 3570/10000: episode: 117, duration: 0.382s, episode steps:  53, steps per second: 139, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 34.780202, mae: 61.035616, mean_q: 123.384379, mean_eps: 0.663415\n",
            " 3592/10000: episode: 118, duration: 0.180s, episode steps:  22, steps per second: 122, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 44.009464, mae: 60.861239, mean_q: 123.158914, mean_eps: 0.659853\n",
            " 3613/10000: episode: 119, duration: 0.155s, episode steps:  21, steps per second: 136, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 13.057701, mae: 60.135577, mean_q: 122.465882, mean_eps: 0.657810\n",
            " 3634/10000: episode: 120, duration: 0.157s, episode steps:  21, steps per second: 134, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 17.976962, mae: 60.415660, mean_q: 122.481737, mean_eps: 0.655815\n",
            " 3706/10000: episode: 121, duration: 0.499s, episode steps:  72, steps per second: 144, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 34.421410, mae: 60.501748, mean_q: 122.900525, mean_eps: 0.651398\n",
            " 3780/10000: episode: 122, duration: 0.496s, episode steps:  74, steps per second: 149, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 21.066027, mae: 60.110869, mean_q: 122.400181, mean_eps: 0.644462\n",
            " 3793/10000: episode: 123, duration: 0.092s, episode steps:  13, steps per second: 141, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 30.662045, mae: 59.688339, mean_q: 121.401853, mean_eps: 0.640330\n",
            " 3854/10000: episode: 124, duration: 0.418s, episode steps:  61, steps per second: 146, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 17.681693, mae: 60.480442, mean_q: 122.695348, mean_eps: 0.636815\n",
            " 3869/10000: episode: 125, duration: 0.107s, episode steps:  15, steps per second: 141, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 24.587418, mae: 60.174097, mean_q: 123.488250, mean_eps: 0.633205\n",
            " 3947/10000: episode: 126, duration: 0.532s, episode steps:  78, steps per second: 147, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 26.705183, mae: 60.811247, mean_q: 123.596903, mean_eps: 0.628787\n",
            " 4003/10000: episode: 127, duration: 0.478s, episode steps:  56, steps per second: 117, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 23.453883, mae: 61.095282, mean_q: 124.752814, mean_eps: 0.622422\n",
            " 4026/10000: episode: 128, duration: 0.243s, episode steps:  23, steps per second:  95, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.609 [0.000, 1.000],  loss: 20.301064, mae: 60.878438, mean_q: 123.877139, mean_eps: 0.618670\n",
            " 4110/10000: episode: 129, duration: 0.829s, episode steps:  84, steps per second: 101, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 22.629963, mae: 60.526741, mean_q: 123.026723, mean_eps: 0.613588\n",
            " 4148/10000: episode: 130, duration: 0.371s, episode steps:  38, steps per second: 103, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 19.259989, mae: 60.258169, mean_q: 123.702176, mean_eps: 0.607792\n",
            " 4213/10000: episode: 131, duration: 0.660s, episode steps:  65, steps per second:  98, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 34.075214, mae: 61.299027, mean_q: 124.513980, mean_eps: 0.602900\n",
            " 4345/10000: episode: 132, duration: 0.974s, episode steps: 132, steps per second: 135, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 29.589478, mae: 61.194608, mean_q: 124.593670, mean_eps: 0.593542\n",
            " 4375/10000: episode: 133, duration: 0.204s, episode steps:  30, steps per second: 147, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 18.985286, mae: 61.878029, mean_q: 125.815428, mean_eps: 0.585847\n",
            " 4448/10000: episode: 134, duration: 0.514s, episode steps:  73, steps per second: 142, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 30.323564, mae: 62.367591, mean_q: 126.177869, mean_eps: 0.580955\n",
            " 4520/10000: episode: 135, duration: 0.504s, episode steps:  72, steps per second: 143, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 29.108995, mae: 61.877084, mean_q: 125.959647, mean_eps: 0.574068\n",
            " 4634/10000: episode: 136, duration: 0.775s, episode steps: 114, steps per second: 147, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 37.010331, mae: 61.069147, mean_q: 124.163707, mean_eps: 0.565233\n",
            " 4691/10000: episode: 137, duration: 0.384s, episode steps:  57, steps per second: 149, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 31.768416, mae: 62.127516, mean_q: 126.042241, mean_eps: 0.557110\n",
            " 4722/10000: episode: 138, duration: 0.219s, episode steps:  31, steps per second: 142, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.323 [0.000, 1.000],  loss: 21.470502, mae: 62.296822, mean_q: 126.048570, mean_eps: 0.552930\n",
            " 4913/10000: episode: 139, duration: 1.319s, episode steps: 191, steps per second: 145, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 30.989904, mae: 61.737813, mean_q: 125.335652, mean_eps: 0.542385\n",
            " 4951/10000: episode: 140, duration: 0.276s, episode steps:  38, steps per second: 138, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.549116, mae: 61.697835, mean_q: 125.311926, mean_eps: 0.531508\n",
            " 5017/10000: episode: 141, duration: 0.495s, episode steps:  66, steps per second: 133, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 25.249484, mae: 62.043790, mean_q: 125.896750, mean_eps: 0.526568\n",
            " 5134/10000: episode: 142, duration: 0.823s, episode steps: 117, steps per second: 142, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 26.092211, mae: 61.704646, mean_q: 125.524797, mean_eps: 0.517875\n",
            " 5212/10000: episode: 143, duration: 0.570s, episode steps:  78, steps per second: 137, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 24.203475, mae: 62.001518, mean_q: 126.119863, mean_eps: 0.508613\n",
            " 5253/10000: episode: 144, duration: 0.302s, episode steps:  41, steps per second: 136, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 36.756939, mae: 60.837187, mean_q: 123.083713, mean_eps: 0.502960\n",
            " 5394/10000: episode: 145, duration: 0.983s, episode steps: 141, steps per second: 143, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 22.744847, mae: 61.720979, mean_q: 125.601440, mean_eps: 0.494315\n",
            " 5508/10000: episode: 146, duration: 0.839s, episode steps: 114, steps per second: 136, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 24.893488, mae: 61.979892, mean_q: 126.243303, mean_eps: 0.482203\n",
            " 5609/10000: episode: 147, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 26.476208, mae: 61.831310, mean_q: 125.830475, mean_eps: 0.471990\n",
            " 5748/10000: episode: 148, duration: 1.211s, episode steps: 139, steps per second: 115, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 27.233741, mae: 62.063115, mean_q: 126.393755, mean_eps: 0.460590\n",
            " 5878/10000: episode: 149, duration: 1.247s, episode steps: 130, steps per second: 104, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 28.508451, mae: 62.184763, mean_q: 125.888447, mean_eps: 0.447813\n",
            " 5997/10000: episode: 150, duration: 1.063s, episode steps: 119, steps per second: 112, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.454 [0.000, 1.000],  loss: 25.930015, mae: 62.478884, mean_q: 127.383756, mean_eps: 0.435985\n",
            " 6130/10000: episode: 151, duration: 0.937s, episode steps: 133, steps per second: 142, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 21.233590, mae: 62.366111, mean_q: 127.322713, mean_eps: 0.424015\n",
            " 6163/10000: episode: 152, duration: 0.239s, episode steps:  33, steps per second: 138, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 25.747863, mae: 62.792303, mean_q: 128.188572, mean_eps: 0.416130\n",
            " 6240/10000: episode: 153, duration: 0.553s, episode steps:  77, steps per second: 139, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 36.994373, mae: 62.605665, mean_q: 127.382008, mean_eps: 0.410905\n",
            " 6382/10000: episode: 154, duration: 0.986s, episode steps: 142, steps per second: 144, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 34.317473, mae: 62.697164, mean_q: 127.269863, mean_eps: 0.400503\n",
            " 6550/10000: episode: 155, duration: 1.220s, episode steps: 168, steps per second: 138, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 32.606643, mae: 62.245846, mean_q: 126.523515, mean_eps: 0.385777\n",
            " 6667/10000: episode: 156, duration: 0.801s, episode steps: 117, steps per second: 146, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 30.044097, mae: 61.734807, mean_q: 125.697262, mean_eps: 0.372240\n",
            " 6772/10000: episode: 157, duration: 0.712s, episode steps: 105, steps per second: 147, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 31.038242, mae: 62.818210, mean_q: 127.822998, mean_eps: 0.361695\n",
            " 6920/10000: episode: 158, duration: 1.016s, episode steps: 148, steps per second: 146, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 32.931871, mae: 63.385421, mean_q: 128.629578, mean_eps: 0.349678\n",
            " 6993/10000: episode: 159, duration: 0.522s, episode steps:  73, steps per second: 140, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 26.038776, mae: 63.464272, mean_q: 128.983760, mean_eps: 0.339180\n",
            " 7193/10000: episode: 160, duration: 1.369s, episode steps: 200, steps per second: 146, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 32.571025, mae: 63.229633, mean_q: 127.959584, mean_eps: 0.326213\n",
            " 7393/10000: episode: 161, duration: 1.444s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 32.319164, mae: 63.585783, mean_q: 129.020396, mean_eps: 0.307213\n",
            " 7593/10000: episode: 162, duration: 1.946s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 25.653368, mae: 64.413923, mean_q: 131.130491, mean_eps: 0.288213\n",
            " 7793/10000: episode: 163, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 28.723150, mae: 64.120864, mean_q: 130.732466, mean_eps: 0.269213\n",
            " 7986/10000: episode: 164, duration: 1.268s, episode steps: 193, steps per second: 152, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 36.506035, mae: 64.544897, mean_q: 130.961153, mean_eps: 0.250545\n",
            " 8186/10000: episode: 165, duration: 1.388s, episode steps: 200, steps per second: 144, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 32.103861, mae: 65.624799, mean_q: 133.556964, mean_eps: 0.231878\n",
            " 8386/10000: episode: 166, duration: 1.358s, episode steps: 200, steps per second: 147, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 33.003143, mae: 65.730377, mean_q: 133.174331, mean_eps: 0.212878\n",
            " 8586/10000: episode: 167, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 34.912475, mae: 66.162819, mean_q: 134.350287, mean_eps: 0.193878\n",
            " 8786/10000: episode: 168, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 28.683867, mae: 66.925334, mean_q: 136.160700, mean_eps: 0.174878\n",
            " 8986/10000: episode: 169, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 33.583513, mae: 67.006104, mean_q: 135.724271, mean_eps: 0.155878\n",
            " 9186/10000: episode: 170, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 36.454527, mae: 67.556782, mean_q: 137.216793, mean_eps: 0.136878\n",
            " 9386/10000: episode: 171, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 33.580549, mae: 68.330668, mean_q: 138.678636, mean_eps: 0.117878\n",
            " 9586/10000: episode: 172, duration: 1.373s, episode steps: 200, steps per second: 146, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 40.697668, mae: 67.848080, mean_q: 137.514870, mean_eps: 0.098878\n",
            " 9786/10000: episode: 173, duration: 1.332s, episode steps: 200, steps per second: 150, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 36.486171, mae: 68.822067, mean_q: 139.440751, mean_eps: 0.079878\n",
            " 9986/10000: episode: 174, duration: 1.348s, episode steps: 200, steps per second: 148, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 41.508631, mae: 68.408869, mean_q: 138.863081, mean_eps: 0.060878\n",
            "done, took 80.367 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f78719286d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.test(env, nb_episodes=20, visualize=True)"
      ],
      "metadata": {
        "id": "dVgg_-CEaAW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}