{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexxakiode/Deep-Learning-Project-CartPole-Balancing-within-200-Steps-in-20-Episodes/blob/main/CartPole_balancing_BoltzmannQPolicy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# CartPole Balancing within 200 Steps in 20 Episodes using BoltzmannQPolicy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vigXep7jWbjI",
        "outputId": "784041c1-eb0c-4de6-b94b-62f694c176bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (63.4.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.40.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.27.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3dLlThnWg4W",
        "outputId": "a48f9981-285c-4cfa-a96c-cc7ed73f68ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "hUE4px66ePUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b43fb2e-7f4c-42af-b416-cd90de991d4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pygame"
      ],
      "metadata": {
        "id": "7hJrdI-xeN6m",
        "outputId": "812309b5-4605-4958-b127-28ca81cb4655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.9/dist-packages (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install randint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7O_ebBkUNav",
        "outputId": "f7586ad9-0704-450f-ec0c-6f2979ff2737"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement randint (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for randint\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "\n",
        "# Load other basic modules\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import randint\n",
        "\n",
        "import pickle\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyOml0McTqQX",
        "outputId": "c917355d-c625-4bdb-9f02-950246581ca5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n"
      ],
      "metadata": {
        "id": "crCTjDiOUUJQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EpsGreedyQPolicy\n",
        "# setup experience replay buffer\n",
        "# here the sequential memory limit is set up the same as the nb_steps (number of steps)\n",
        "# parameter in the fit method.  This means that all the action-states will fit into the\n",
        "# memory buffer\n",
        "# keep window_length as 1. It's used in other RL methods, but keep it to 1 in DQNs\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "policy_inner = BoltzmannQPolicy()"
      ],
      "metadata": {
        "id": "NrHkEYq3VCnJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "# add extra layers here\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qr9G2ChZLGt",
        "outputId": "39955f7d-ddff-487c-d77c-18f03b1703a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_outer =  LinearAnnealedPolicy(inner_policy=policy_inner, \n",
        "                               attr='tau',            \n",
        "                               value_max=0.1,\n",
        "                               value_min=0.001, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=50000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy_outer) \n",
        "\n",
        "dqn.compile(Adam(lr=0.01), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)\n",
        "\n",
        "plt.imshow(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p9zwVJPbumLe",
        "outputId": "c70412c4-53a0-4c00-dcb4-7a175c4e2ebc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    16/50000: episode: 1, duration: 1.130s, episode steps:  16, steps per second:  14, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 110.841405, mae: 80.797768, mean_q: 169.847717, mean_tau: 0.099974\n",
            "    32/50000: episode: 2, duration: 0.203s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 112.774213, mae: 81.135821, mean_q: 169.419436, mean_tau: 0.099953\n",
            "    44/50000: episode: 3, duration: 0.156s, episode steps:  12, steps per second:  77, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 119.070819, mae: 79.883658, mean_q: 168.307246, mean_tau: 0.099926\n",
            "    64/50000: episode: 4, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 91.309505, mae: 82.917023, mean_q: 175.321876, mean_tau: 0.099894\n",
            "    81/50000: episode: 5, duration: 0.213s, episode steps:  17, steps per second:  80, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 99.242155, mae: 83.421148, mean_q: 174.121943, mean_tau: 0.099857\n",
            "    95/50000: episode: 6, duration: 0.164s, episode steps:  14, steps per second:  85, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 98.966401, mae: 81.267619, mean_q: 170.080525, mean_tau: 0.099827\n",
            "   115/50000: episode: 7, duration: 0.248s, episode steps:  20, steps per second:  81, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 130.385930, mae: 83.943092, mean_q: 176.100612, mean_tau: 0.099793\n",
            "   145/50000: episode: 8, duration: 0.355s, episode steps:  30, steps per second:  85, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 106.696558, mae: 83.787122, mean_q: 175.847985, mean_tau: 0.099744\n",
            "   159/50000: episode: 9, duration: 0.165s, episode steps:  14, steps per second:  85, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 158.679813, mae: 80.941572, mean_q: 171.586428, mean_tau: 0.099700\n",
            "   174/50000: episode: 10, duration: 0.186s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 73.201257, mae: 80.349741, mean_q: 172.034567, mean_tau: 0.099671\n",
            "   192/50000: episode: 11, duration: 0.214s, episode steps:  18, steps per second:  84, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 103.370587, mae: 84.639267, mean_q: 178.455186, mean_tau: 0.099639\n",
            "   213/50000: episode: 12, duration: 0.264s, episode steps:  21, steps per second:  79, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 115.912766, mae: 86.910024, mean_q: 183.002546, mean_tau: 0.099600\n",
            "   226/50000: episode: 13, duration: 0.167s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 107.465839, mae: 83.427307, mean_q: 176.499750, mean_tau: 0.099566\n",
            "   240/50000: episode: 14, duration: 0.182s, episode steps:  14, steps per second:  77, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 117.018238, mae: 83.460848, mean_q: 177.882028, mean_tau: 0.099540\n",
            "   273/50000: episode: 15, duration: 0.265s, episode steps:  33, steps per second: 125, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 107.611824, mae: 85.229551, mean_q: 180.822238, mean_tau: 0.099493\n",
            "   294/50000: episode: 16, duration: 0.163s, episode steps:  21, steps per second: 129, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 134.308601, mae: 87.145626, mean_q: 182.267473, mean_tau: 0.099440\n",
            "   311/50000: episode: 17, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 113.971148, mae: 85.948123, mean_q: 180.415725, mean_tau: 0.099402\n",
            "   344/50000: episode: 18, duration: 0.271s, episode steps:  33, steps per second: 122, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 103.929253, mae: 87.312784, mean_q: 182.336184, mean_tau: 0.099353\n",
            "   387/50000: episode: 19, duration: 0.332s, episode steps:  43, steps per second: 129, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 147.275223, mae: 85.372438, mean_q: 179.989481, mean_tau: 0.099277\n",
            "   431/50000: episode: 20, duration: 0.329s, episode steps:  44, steps per second: 134, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 104.660022, mae: 85.306337, mean_q: 180.553559, mean_tau: 0.099191\n",
            "   454/50000: episode: 21, duration: 0.193s, episode steps:  23, steps per second: 119, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 129.498658, mae: 84.863170, mean_q: 181.501302, mean_tau: 0.099125\n",
            "   474/50000: episode: 22, duration: 0.168s, episode steps:  20, steps per second: 119, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 128.412276, mae: 87.335726, mean_q: 184.028828, mean_tau: 0.099082\n",
            "   489/50000: episode: 23, duration: 0.121s, episode steps:  15, steps per second: 124, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 171.446233, mae: 86.765519, mean_q: 180.103664, mean_tau: 0.099048\n",
            "   511/50000: episode: 24, duration: 0.180s, episode steps:  22, steps per second: 122, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 106.976555, mae: 87.843816, mean_q: 184.323198, mean_tau: 0.099011\n",
            "   526/50000: episode: 25, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 152.110329, mae: 89.638895, mean_q: 189.349557, mean_tau: 0.098974\n",
            "   548/50000: episode: 26, duration: 0.177s, episode steps:  22, steps per second: 124, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 119.699146, mae: 87.243378, mean_q: 184.235573, mean_tau: 0.098938\n",
            "   568/50000: episode: 27, duration: 0.166s, episode steps:  20, steps per second: 120, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 99.991814, mae: 90.834908, mean_q: 190.361907, mean_tau: 0.098896\n",
            "   587/50000: episode: 28, duration: 0.154s, episode steps:  19, steps per second: 123, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 115.144144, mae: 86.546009, mean_q: 184.963780, mean_tau: 0.098858\n",
            "   627/50000: episode: 29, duration: 0.316s, episode steps:  40, steps per second: 127, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 92.806303, mae: 88.864625, mean_q: 189.375483, mean_tau: 0.098799\n",
            "   657/50000: episode: 30, duration: 0.241s, episode steps:  30, steps per second: 124, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 88.076518, mae: 89.900392, mean_q: 191.322595, mean_tau: 0.098730\n",
            "   686/50000: episode: 31, duration: 0.225s, episode steps:  29, steps per second: 129, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 130.990952, mae: 90.392376, mean_q: 192.252580, mean_tau: 0.098671\n",
            "   706/50000: episode: 32, duration: 0.169s, episode steps:  20, steps per second: 118, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 149.191710, mae: 88.778376, mean_q: 187.428548, mean_tau: 0.098623\n",
            "   745/50000: episode: 33, duration: 0.297s, episode steps:  39, steps per second: 131, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 128.061614, mae: 91.408204, mean_q: 193.078508, mean_tau: 0.098565\n",
            "   769/50000: episode: 34, duration: 0.185s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 112.159074, mae: 91.261480, mean_q: 192.791379, mean_tau: 0.098502\n",
            "   782/50000: episode: 35, duration: 0.119s, episode steps:  13, steps per second: 110, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 121.272340, mae: 88.529295, mean_q: 189.422729, mean_tau: 0.098465\n",
            "   820/50000: episode: 36, duration: 0.318s, episode steps:  38, steps per second: 120, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 141.979586, mae: 90.347716, mean_q: 191.355103, mean_tau: 0.098415\n",
            "   843/50000: episode: 37, duration: 0.194s, episode steps:  23, steps per second: 119, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 161.007962, mae: 90.719359, mean_q: 191.077104, mean_tau: 0.098355\n",
            "   875/50000: episode: 38, duration: 0.253s, episode steps:  32, steps per second: 127, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 115.583320, mae: 91.122622, mean_q: 192.281842, mean_tau: 0.098300\n",
            "   887/50000: episode: 39, duration: 0.097s, episode steps:  12, steps per second: 124, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 117.152279, mae: 91.104181, mean_q: 194.647283, mean_tau: 0.098257\n",
            "   907/50000: episode: 40, duration: 0.163s, episode steps:  20, steps per second: 123, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 136.292614, mae: 93.386732, mean_q: 196.696239, mean_tau: 0.098225\n",
            "   926/50000: episode: 41, duration: 0.149s, episode steps:  19, steps per second: 127, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 135.409233, mae: 91.960608, mean_q: 192.611345, mean_tau: 0.098186\n",
            "   948/50000: episode: 42, duration: 0.187s, episode steps:  22, steps per second: 117, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 135.698208, mae: 91.938638, mean_q: 195.938090, mean_tau: 0.098146\n",
            "   984/50000: episode: 43, duration: 0.304s, episode steps:  36, steps per second: 118, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 128.329978, mae: 92.993679, mean_q: 196.562742, mean_tau: 0.098088\n",
            "  1000/50000: episode: 44, duration: 0.138s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 115.920259, mae: 92.487206, mean_q: 197.868340, mean_tau: 0.098037\n",
            "  1016/50000: episode: 45, duration: 0.142s, episode steps:  16, steps per second: 113, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 146.788647, mae: 91.331156, mean_q: 193.753202, mean_tau: 0.098005\n",
            "  1031/50000: episode: 46, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 171.484069, mae: 92.482742, mean_q: 195.589982, mean_tau: 0.097974\n",
            "  1049/50000: episode: 47, duration: 0.157s, episode steps:  18, steps per second: 115, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 106.330851, mae: 93.960481, mean_q: 202.145672, mean_tau: 0.097942\n",
            "  1070/50000: episode: 48, duration: 0.187s, episode steps:  21, steps per second: 112, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 114.182628, mae: 95.013013, mean_q: 202.407908, mean_tau: 0.097903\n",
            "  1082/50000: episode: 49, duration: 0.104s, episode steps:  12, steps per second: 115, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 158.870213, mae: 94.766612, mean_q: 201.224719, mean_tau: 0.097871\n",
            "  1100/50000: episode: 50, duration: 0.153s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 157.577107, mae: 98.406551, mean_q: 206.013852, mean_tau: 0.097841\n",
            "  1140/50000: episode: 51, duration: 0.319s, episode steps:  40, steps per second: 125, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 151.053404, mae: 93.882030, mean_q: 198.414672, mean_tau: 0.097783\n",
            "  1155/50000: episode: 52, duration: 0.123s, episode steps:  15, steps per second: 122, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 240.969208, mae: 94.702580, mean_q: 199.683896, mean_tau: 0.097729\n",
            "  1166/50000: episode: 53, duration: 0.097s, episode steps:  11, steps per second: 113, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 75.767202, mae: 98.389197, mean_q: 206.813865, mean_tau: 0.097703\n",
            "  1183/50000: episode: 54, duration: 0.149s, episode steps:  17, steps per second: 114, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 136.402306, mae: 96.584042, mean_q: 203.133069, mean_tau: 0.097675\n",
            "  1197/50000: episode: 55, duration: 0.124s, episode steps:  14, steps per second: 113, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 102.364868, mae: 99.589056, mean_q: 210.259145, mean_tau: 0.097645\n",
            "  1235/50000: episode: 56, duration: 0.326s, episode steps:  38, steps per second: 116, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 142.759937, mae: 95.531002, mean_q: 202.593281, mean_tau: 0.097593\n",
            "  1249/50000: episode: 57, duration: 0.129s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 148.219931, mae: 98.449462, mean_q: 208.937467, mean_tau: 0.097542\n",
            "  1262/50000: episode: 58, duration: 0.111s, episode steps:  13, steps per second: 117, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 179.830143, mae: 93.268679, mean_q: 198.666240, mean_tau: 0.097515\n",
            "  1287/50000: episode: 59, duration: 0.215s, episode steps:  25, steps per second: 116, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 148.989427, mae: 97.710357, mean_q: 209.494285, mean_tau: 0.097477\n",
            "  1304/50000: episode: 60, duration: 0.156s, episode steps:  17, steps per second: 109, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 111.871891, mae: 100.552950, mean_q: 212.117357, mean_tau: 0.097436\n",
            "  1319/50000: episode: 61, duration: 0.138s, episode steps:  15, steps per second: 109, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 129.904636, mae: 97.566714, mean_q: 206.977226, mean_tau: 0.097404\n",
            "  1338/50000: episode: 62, duration: 0.148s, episode steps:  19, steps per second: 128, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 167.006281, mae: 100.735056, mean_q: 210.415982, mean_tau: 0.097371\n",
            "  1351/50000: episode: 63, duration: 0.113s, episode steps:  13, steps per second: 115, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 112.961766, mae: 97.999887, mean_q: 208.580137, mean_tau: 0.097339\n",
            "  1377/50000: episode: 64, duration: 0.204s, episode steps:  26, steps per second: 127, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 139.631910, mae: 97.483967, mean_q: 206.335052, mean_tau: 0.097300\n",
            "  1422/50000: episode: 65, duration: 0.352s, episode steps:  45, steps per second: 128, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 151.239540, mae: 98.900535, mean_q: 209.591662, mean_tau: 0.097230\n",
            "  1453/50000: episode: 66, duration: 0.293s, episode steps:  31, steps per second: 106, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 133.869058, mae: 98.540133, mean_q: 208.278927, mean_tau: 0.097155\n",
            "  1469/50000: episode: 67, duration: 0.197s, episode steps:  16, steps per second:  81, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 92.928627, mae: 98.740131, mean_q: 211.179636, mean_tau: 0.097108\n",
            "  1501/50000: episode: 68, duration: 0.378s, episode steps:  32, steps per second:  85, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 117.599397, mae: 99.926089, mean_q: 213.466626, mean_tau: 0.097061\n",
            "  1519/50000: episode: 69, duration: 0.231s, episode steps:  18, steps per second:  78, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 154.387598, mae: 98.296152, mean_q: 208.654791, mean_tau: 0.097011\n",
            "  1542/50000: episode: 70, duration: 0.281s, episode steps:  23, steps per second:  82, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 177.161523, mae: 100.422121, mean_q: 211.400369, mean_tau: 0.096971\n",
            "  1554/50000: episode: 71, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 128.290234, mae: 100.619535, mean_q: 211.143867, mean_tau: 0.096936\n",
            "  1584/50000: episode: 72, duration: 0.349s, episode steps:  30, steps per second:  86, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 116.467756, mae: 100.520120, mean_q: 213.585729, mean_tau: 0.096894\n",
            "  1596/50000: episode: 73, duration: 0.147s, episode steps:  12, steps per second:  82, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 134.016478, mae: 106.369892, mean_q: 224.340048, mean_tau: 0.096853\n",
            "  1625/50000: episode: 74, duration: 0.368s, episode steps:  29, steps per second:  79, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 167.025302, mae: 104.601474, mean_q: 221.061881, mean_tau: 0.096812\n",
            "  1647/50000: episode: 75, duration: 0.288s, episode steps:  22, steps per second:  76, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 219.271263, mae: 101.086969, mean_q: 213.098541, mean_tau: 0.096762\n",
            "  1660/50000: episode: 76, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 133.825112, mae: 111.877114, mean_q: 235.331876, mean_tau: 0.096727\n",
            "  1709/50000: episode: 77, duration: 0.451s, episode steps:  49, steps per second: 109, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 176.377250, mae: 103.887746, mean_q: 220.525558, mean_tau: 0.096666\n",
            "  1743/50000: episode: 78, duration: 0.267s, episode steps:  34, steps per second: 127, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 152.467420, mae: 104.002968, mean_q: 219.879767, mean_tau: 0.096584\n",
            "  1779/50000: episode: 79, duration: 0.283s, episode steps:  36, steps per second: 127, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 184.655329, mae: 104.197453, mean_q: 221.112487, mean_tau: 0.096514\n",
            "  1811/50000: episode: 80, duration: 0.271s, episode steps:  32, steps per second: 118, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 213.668826, mae: 103.294468, mean_q: 217.878586, mean_tau: 0.096447\n",
            "  1870/50000: episode: 81, duration: 0.479s, episode steps:  59, steps per second: 123, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 140.737894, mae: 104.760149, mean_q: 220.425469, mean_tau: 0.096357\n",
            "  1898/50000: episode: 82, duration: 0.242s, episode steps:  28, steps per second: 116, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 164.215263, mae: 108.400116, mean_q: 227.437107, mean_tau: 0.096271\n",
            "  1919/50000: episode: 83, duration: 0.160s, episode steps:  21, steps per second: 131, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 169.264856, mae: 105.315369, mean_q: 224.553141, mean_tau: 0.096222\n",
            "  1949/50000: episode: 84, duration: 0.248s, episode steps:  30, steps per second: 121, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 192.227635, mae: 105.844245, mean_q: 224.683129, mean_tau: 0.096172\n",
            "  1972/50000: episode: 85, duration: 0.176s, episode steps:  23, steps per second: 131, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 165.066440, mae: 107.769473, mean_q: 226.954135, mean_tau: 0.096119\n",
            "  2005/50000: episode: 86, duration: 0.247s, episode steps:  33, steps per second: 134, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 181.108329, mae: 106.271788, mean_q: 223.747607, mean_tau: 0.096064\n",
            "  2032/50000: episode: 87, duration: 0.203s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 186.723862, mae: 109.554358, mean_q: 230.414126, mean_tau: 0.096004\n",
            "  2054/50000: episode: 88, duration: 0.172s, episode steps:  22, steps per second: 128, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 189.259451, mae: 107.803123, mean_q: 226.745672, mean_tau: 0.095956\n",
            "  2072/50000: episode: 89, duration: 0.153s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 184.906264, mae: 105.828568, mean_q: 223.647917, mean_tau: 0.095916\n",
            "  2094/50000: episode: 90, duration: 0.168s, episode steps:  22, steps per second: 131, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 172.154856, mae: 109.314622, mean_q: 230.340243, mean_tau: 0.095877\n",
            "  2175/50000: episode: 91, duration: 0.669s, episode steps:  81, steps per second: 121, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 170.863894, mae: 109.222196, mean_q: 230.855778, mean_tau: 0.095775\n",
            "  2209/50000: episode: 92, duration: 0.305s, episode steps:  34, steps per second: 112, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 146.666025, mae: 112.207686, mean_q: 238.133693, mean_tau: 0.095661\n",
            "  2224/50000: episode: 93, duration: 0.147s, episode steps:  15, steps per second: 102, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 191.753188, mae: 112.753682, mean_q: 237.152079, mean_tau: 0.095612\n",
            "  2241/50000: episode: 94, duration: 0.139s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 160.299059, mae: 108.713144, mean_q: 231.662327, mean_tau: 0.095581\n",
            "  2251/50000: episode: 95, duration: 0.084s, episode steps:  10, steps per second: 119, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 239.794017, mae: 111.821340, mean_q: 232.239285, mean_tau: 0.095554\n",
            "  2267/50000: episode: 96, duration: 0.140s, episode steps:  16, steps per second: 114, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 133.674970, mae: 113.417819, mean_q: 240.487263, mean_tau: 0.095528\n",
            "  2287/50000: episode: 97, duration: 0.157s, episode steps:  20, steps per second: 127, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 165.870820, mae: 111.447618, mean_q: 234.831664, mean_tau: 0.095493\n",
            "  2305/50000: episode: 98, duration: 0.152s, episode steps:  18, steps per second: 119, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 138.443026, mae: 113.134063, mean_q: 240.932738, mean_tau: 0.095455\n",
            "  2337/50000: episode: 99, duration: 0.260s, episode steps:  32, steps per second: 123, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 162.604990, mae: 111.278826, mean_q: 237.370586, mean_tau: 0.095405\n",
            "  2373/50000: episode: 100, duration: 0.286s, episode steps:  36, steps per second: 126, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 171.181001, mae: 113.668811, mean_q: 240.108977, mean_tau: 0.095338\n",
            "  2414/50000: episode: 101, duration: 0.349s, episode steps:  41, steps per second: 118, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 175.903625, mae: 112.392629, mean_q: 237.924891, mean_tau: 0.095262\n",
            "  2435/50000: episode: 102, duration: 0.189s, episode steps:  21, steps per second: 111, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 132.329106, mae: 112.167196, mean_q: 237.896784, mean_tau: 0.095200\n",
            "  2463/50000: episode: 103, duration: 0.247s, episode steps:  28, steps per second: 113, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 162.014553, mae: 112.508638, mean_q: 237.970118, mean_tau: 0.095152\n",
            "  2481/50000: episode: 104, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 184.474836, mae: 114.273142, mean_q: 241.153879, mean_tau: 0.095106\n",
            "  2506/50000: episode: 105, duration: 0.197s, episode steps:  25, steps per second: 127, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 185.492849, mae: 115.053835, mean_q: 243.203688, mean_tau: 0.095064\n",
            "  2518/50000: episode: 106, duration: 0.095s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 229.031942, mae: 114.997578, mean_q: 244.107952, mean_tau: 0.095027\n",
            "  2539/50000: episode: 107, duration: 0.167s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 201.588072, mae: 116.410350, mean_q: 244.685120, mean_tau: 0.094995\n",
            "  2585/50000: episode: 108, duration: 0.380s, episode steps:  46, steps per second: 121, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.587 [0.000, 1.000],  loss: 154.801471, mae: 114.429923, mean_q: 242.993962, mean_tau: 0.094928\n",
            "  2605/50000: episode: 109, duration: 0.168s, episode steps:  20, steps per second: 119, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 214.652954, mae: 116.588172, mean_q: 244.429625, mean_tau: 0.094863\n",
            "  2617/50000: episode: 110, duration: 0.103s, episode steps:  12, steps per second: 116, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 197.944421, mae: 124.849311, mean_q: 260.292894, mean_tau: 0.094831\n",
            "  2633/50000: episode: 111, duration: 0.136s, episode steps:  16, steps per second: 117, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 133.468808, mae: 117.086444, mean_q: 246.218769, mean_tau: 0.094803\n",
            "  2653/50000: episode: 112, duration: 0.163s, episode steps:  20, steps per second: 123, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 207.678052, mae: 116.902653, mean_q: 247.521175, mean_tau: 0.094768\n",
            "  2684/50000: episode: 113, duration: 0.261s, episode steps:  31, steps per second: 119, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 176.394656, mae: 114.158849, mean_q: 244.065779, mean_tau: 0.094717\n",
            "  2731/50000: episode: 114, duration: 0.389s, episode steps:  47, steps per second: 121, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 165.805078, mae: 116.628624, mean_q: 248.233178, mean_tau: 0.094640\n",
            "  2746/50000: episode: 115, duration: 0.132s, episode steps:  15, steps per second: 114, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 198.666645, mae: 118.329935, mean_q: 250.644368, mean_tau: 0.094579\n",
            "  2757/50000: episode: 116, duration: 0.105s, episode steps:  11, steps per second: 105, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 183.479472, mae: 118.407443, mean_q: 252.493014, mean_tau: 0.094553\n",
            "  2770/50000: episode: 117, duration: 0.106s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 175.674636, mae: 117.951886, mean_q: 252.385273, mean_tau: 0.094529\n",
            "  2812/50000: episode: 118, duration: 0.332s, episode steps:  42, steps per second: 127, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 177.706505, mae: 118.299372, mean_q: 251.633962, mean_tau: 0.094475\n",
            "  2834/50000: episode: 119, duration: 0.173s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 250.871638, mae: 118.573187, mean_q: 250.280776, mean_tau: 0.094411\n",
            "  2852/50000: episode: 120, duration: 0.147s, episode steps:  18, steps per second: 123, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 248.607248, mae: 116.128917, mean_q: 246.935304, mean_tau: 0.094372\n",
            "  2887/50000: episode: 121, duration: 0.337s, episode steps:  35, steps per second: 104, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 192.590984, mae: 121.849637, mean_q: 258.704187, mean_tau: 0.094319\n",
            "  2908/50000: episode: 122, duration: 0.271s, episode steps:  21, steps per second:  78, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 234.991727, mae: 122.125475, mean_q: 258.945337, mean_tau: 0.094264\n",
            "  2933/50000: episode: 123, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 185.617166, mae: 126.165385, mean_q: 265.778644, mean_tau: 0.094218\n",
            "  2952/50000: episode: 124, duration: 0.225s, episode steps:  19, steps per second:  85, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 161.214073, mae: 127.424381, mean_q: 267.796208, mean_tau: 0.094175\n",
            "  2979/50000: episode: 125, duration: 0.309s, episode steps:  27, steps per second:  87, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 162.283892, mae: 122.705401, mean_q: 261.368319, mean_tau: 0.094129\n",
            "  2989/50000: episode: 126, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 114.423260, mae: 119.919102, mean_q: 257.822389, mean_tau: 0.094093\n",
            "  3022/50000: episode: 127, duration: 0.377s, episode steps:  33, steps per second:  87, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 211.444366, mae: 124.992082, mean_q: 265.978940, mean_tau: 0.094050\n",
            "  3046/50000: episode: 128, duration: 0.313s, episode steps:  24, steps per second:  77, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 293.618369, mae: 122.323762, mean_q: 259.374173, mean_tau: 0.093994\n",
            "  3067/50000: episode: 129, duration: 0.262s, episode steps:  21, steps per second:  80, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 263.899766, mae: 125.365650, mean_q: 262.342962, mean_tau: 0.093949\n",
            "  3123/50000: episode: 130, duration: 0.611s, episode steps:  56, steps per second:  92, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 211.659058, mae: 125.991911, mean_q: 266.208052, mean_tau: 0.093873\n",
            "  3138/50000: episode: 131, duration: 0.122s, episode steps:  15, steps per second: 123, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 184.472150, mae: 122.636617, mean_q: 264.726084, mean_tau: 0.093803\n",
            "  3161/50000: episode: 132, duration: 0.181s, episode steps:  23, steps per second: 127, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 229.034528, mae: 127.224863, mean_q: 270.124502, mean_tau: 0.093765\n",
            "  3194/50000: episode: 133, duration: 0.250s, episode steps:  33, steps per second: 132, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 227.042779, mae: 128.962745, mean_q: 274.309767, mean_tau: 0.093710\n",
            "  3223/50000: episode: 134, duration: 0.223s, episode steps:  29, steps per second: 130, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 213.001773, mae: 131.584828, mean_q: 275.624018, mean_tau: 0.093648\n",
            "  3237/50000: episode: 135, duration: 0.110s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 214.077347, mae: 130.549623, mean_q: 276.867680, mean_tau: 0.093606\n",
            "  3249/50000: episode: 136, duration: 0.134s, episode steps:  12, steps per second:  90, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 149.063059, mae: 131.977752, mean_q: 277.508280, mean_tau: 0.093580\n",
            "  3282/50000: episode: 137, duration: 0.266s, episode steps:  33, steps per second: 124, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 204.519881, mae: 128.827086, mean_q: 274.613446, mean_tau: 0.093535\n",
            "  3304/50000: episode: 138, duration: 0.173s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 233.222120, mae: 129.230584, mean_q: 273.064844, mean_tau: 0.093481\n",
            "  3333/50000: episode: 139, duration: 0.238s, episode steps:  29, steps per second: 122, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 225.707924, mae: 133.457442, mean_q: 278.516498, mean_tau: 0.093430\n",
            "  3361/50000: episode: 140, duration: 0.221s, episode steps:  28, steps per second: 127, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 204.640046, mae: 132.523306, mean_q: 278.290121, mean_tau: 0.093374\n",
            "  3403/50000: episode: 141, duration: 0.324s, episode steps:  42, steps per second: 130, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 203.775228, mae: 130.931438, mean_q: 277.808926, mean_tau: 0.093305\n",
            "  3437/50000: episode: 142, duration: 0.264s, episode steps:  34, steps per second: 129, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 209.964615, mae: 129.374319, mean_q: 275.382614, mean_tau: 0.093229\n",
            "  3463/50000: episode: 143, duration: 0.208s, episode steps:  26, steps per second: 125, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 226.635575, mae: 135.678507, mean_q: 284.286839, mean_tau: 0.093170\n",
            "  3481/50000: episode: 144, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 254.805063, mae: 134.829790, mean_q: 278.375971, mean_tau: 0.093126\n",
            "  3508/50000: episode: 145, duration: 0.234s, episode steps:  27, steps per second: 115, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 221.335126, mae: 132.252753, mean_q: 278.443637, mean_tau: 0.093082\n",
            "  3543/50000: episode: 146, duration: 0.293s, episode steps:  35, steps per second: 119, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 224.033926, mae: 134.349532, mean_q: 280.137508, mean_tau: 0.093020\n",
            "  3559/50000: episode: 147, duration: 0.127s, episode steps:  16, steps per second: 126, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 213.284183, mae: 135.729055, mean_q: 280.686845, mean_tau: 0.092970\n",
            "  3582/50000: episode: 148, duration: 0.201s, episode steps:  23, steps per second: 114, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 252.158947, mae: 133.381305, mean_q: 285.103573, mean_tau: 0.092931\n",
            "  3603/50000: episode: 149, duration: 0.165s, episode steps:  21, steps per second: 127, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 183.292660, mae: 130.776695, mean_q: 274.617477, mean_tau: 0.092888\n",
            "  3655/50000: episode: 150, duration: 0.411s, episode steps:  52, steps per second: 127, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 236.435804, mae: 133.923085, mean_q: 283.125349, mean_tau: 0.092816\n",
            "  3676/50000: episode: 151, duration: 0.161s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 262.876271, mae: 133.395100, mean_q: 283.371182, mean_tau: 0.092743\n",
            "  3687/50000: episode: 152, duration: 0.095s, episode steps:  11, steps per second: 116, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 165.267309, mae: 133.477859, mean_q: 282.604123, mean_tau: 0.092712\n",
            "  3709/50000: episode: 153, duration: 0.173s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 268.037384, mae: 138.614290, mean_q: 288.658704, mean_tau: 0.092679\n",
            "  3735/50000: episode: 154, duration: 0.203s, episode steps:  26, steps per second: 128, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 211.934444, mae: 139.299859, mean_q: 291.730229, mean_tau: 0.092631\n",
            "  3754/50000: episode: 155, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 176.920947, mae: 138.483789, mean_q: 290.726807, mean_tau: 0.092587\n",
            "  3767/50000: episode: 156, duration: 0.109s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 311.072645, mae: 142.562890, mean_q: 294.385006, mean_tau: 0.092555\n",
            "  3795/50000: episode: 157, duration: 0.223s, episode steps:  28, steps per second: 126, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 222.306022, mae: 136.040107, mean_q: 285.207077, mean_tau: 0.092515\n",
            "  3841/50000: episode: 158, duration: 0.359s, episode steps:  46, steps per second: 128, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 203.429988, mae: 137.931102, mean_q: 290.288236, mean_tau: 0.092441\n",
            "  3882/50000: episode: 159, duration: 0.318s, episode steps:  41, steps per second: 129, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 206.348354, mae: 136.858071, mean_q: 287.784839, mean_tau: 0.092355\n",
            "  3898/50000: episode: 160, duration: 0.132s, episode steps:  16, steps per second: 121, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 221.040452, mae: 137.841597, mean_q: 291.402657, mean_tau: 0.092299\n",
            "  3913/50000: episode: 161, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 197.032244, mae: 139.250321, mean_q: 296.115771, mean_tau: 0.092268\n",
            "  3936/50000: episode: 162, duration: 0.169s, episode steps:  23, steps per second: 136, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 187.142774, mae: 138.153030, mean_q: 292.956891, mean_tau: 0.092230\n",
            "  3955/50000: episode: 163, duration: 0.150s, episode steps:  19, steps per second: 127, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 230.820321, mae: 137.317374, mean_q: 291.972605, mean_tau: 0.092189\n",
            "  3981/50000: episode: 164, duration: 0.201s, episode steps:  26, steps per second: 129, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 202.455023, mae: 141.901004, mean_q: 297.564934, mean_tau: 0.092144\n",
            "  4000/50000: episode: 165, duration: 0.147s, episode steps:  19, steps per second: 129, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 217.739206, mae: 139.970943, mean_q: 294.152687, mean_tau: 0.092100\n",
            "  4019/50000: episode: 166, duration: 0.155s, episode steps:  19, steps per second: 123, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 241.777222, mae: 140.251542, mean_q: 295.651502, mean_tau: 0.092062\n",
            "  4031/50000: episode: 167, duration: 0.097s, episode steps:  12, steps per second: 124, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 222.936200, mae: 143.253110, mean_q: 300.761810, mean_tau: 0.092031\n",
            "  4058/50000: episode: 168, duration: 0.211s, episode steps:  27, steps per second: 128, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 203.541699, mae: 145.103684, mean_q: 302.852825, mean_tau: 0.091993\n",
            "  4081/50000: episode: 169, duration: 0.190s, episode steps:  23, steps per second: 121, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 258.256209, mae: 140.049822, mean_q: 294.659243, mean_tau: 0.091943\n",
            "  4108/50000: episode: 170, duration: 0.224s, episode steps:  27, steps per second: 121, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 253.570359, mae: 138.556230, mean_q: 291.541088, mean_tau: 0.091894\n",
            "  4118/50000: episode: 171, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 202.094460, mae: 143.296423, mean_q: 302.048669, mean_tau: 0.091857\n",
            "  4142/50000: episode: 172, duration: 0.206s, episode steps:  24, steps per second: 116, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 212.516661, mae: 143.467105, mean_q: 304.162832, mean_tau: 0.091824\n",
            "  4158/50000: episode: 173, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 282.862046, mae: 135.766352, mean_q: 287.003266, mean_tau: 0.091784\n",
            "  4181/50000: episode: 174, duration: 0.178s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 177.748158, mae: 145.598576, mean_q: 308.450085, mean_tau: 0.091745\n",
            "  4192/50000: episode: 175, duration: 0.087s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 270.937641, mae: 144.722501, mean_q: 301.503374, mean_tau: 0.091712\n",
            "  4205/50000: episode: 176, duration: 0.104s, episode steps:  13, steps per second: 125, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 333.134347, mae: 142.766269, mean_q: 297.490676, mean_tau: 0.091688\n",
            "  4219/50000: episode: 177, duration: 0.123s, episode steps:  14, steps per second: 114, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 239.346516, mae: 138.910143, mean_q: 294.258525, mean_tau: 0.091661\n",
            "  4234/50000: episode: 178, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 261.479291, mae: 138.641520, mean_q: 288.571499, mean_tau: 0.091633\n",
            "  4252/50000: episode: 179, duration: 0.149s, episode steps:  18, steps per second: 121, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 213.161697, mae: 140.363450, mean_q: 296.236825, mean_tau: 0.091600\n",
            "  4264/50000: episode: 180, duration: 0.099s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 202.679582, mae: 147.671479, mean_q: 311.205022, mean_tau: 0.091570\n",
            "  4294/50000: episode: 181, duration: 0.232s, episode steps:  30, steps per second: 129, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 259.718344, mae: 146.556957, mean_q: 305.739991, mean_tau: 0.091529\n",
            "  4312/50000: episode: 182, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 197.351846, mae: 143.620019, mean_q: 299.297830, mean_tau: 0.091481\n",
            "  4323/50000: episode: 183, duration: 0.089s, episode steps:  11, steps per second: 123, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 298.865809, mae: 143.068948, mean_q: 296.940080, mean_tau: 0.091452\n",
            "  4365/50000: episode: 184, duration: 0.418s, episode steps:  42, steps per second: 101, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 206.387436, mae: 144.645186, mean_q: 304.875897, mean_tau: 0.091400\n",
            "  4413/50000: episode: 185, duration: 0.542s, episode steps:  48, steps per second:  89, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 242.228230, mae: 144.236624, mean_q: 303.666252, mean_tau: 0.091311\n",
            "  4429/50000: episode: 186, duration: 0.189s, episode steps:  16, steps per second:  85, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 212.994806, mae: 147.959166, mean_q: 308.283983, mean_tau: 0.091247\n",
            "  4443/50000: episode: 187, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 168.844484, mae: 148.118063, mean_q: 312.238565, mean_tau: 0.091218\n",
            "  4457/50000: episode: 188, duration: 0.173s, episode steps:  14, steps per second:  81, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 319.292806, mae: 155.567107, mean_q: 322.352419, mean_tau: 0.091190\n",
            "  4467/50000: episode: 189, duration: 0.123s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 119.534304, mae: 141.468058, mean_q: 297.407034, mean_tau: 0.091166\n",
            "  4497/50000: episode: 190, duration: 0.346s, episode steps:  30, steps per second:  87, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 231.893050, mae: 146.846674, mean_q: 307.844090, mean_tau: 0.091127\n",
            "  4510/50000: episode: 191, duration: 0.146s, episode steps:  13, steps per second:  89, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 300.083481, mae: 145.903738, mean_q: 305.579322, mean_tau: 0.091084\n",
            "  4572/50000: episode: 192, duration: 0.693s, episode steps:  62, steps per second:  89, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 220.635624, mae: 151.052128, mean_q: 315.588310, mean_tau: 0.091010\n",
            "  4587/50000: episode: 193, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 179.111542, mae: 146.143667, mean_q: 309.206649, mean_tau: 0.090934\n",
            "  4607/50000: episode: 194, duration: 0.177s, episode steps:  20, steps per second: 113, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 268.990883, mae: 149.334293, mean_q: 314.091078, mean_tau: 0.090899\n",
            "  4621/50000: episode: 195, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 268.864749, mae: 151.888323, mean_q: 315.894485, mean_tau: 0.090865\n",
            "  4650/50000: episode: 196, duration: 0.231s, episode steps:  29, steps per second: 126, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 219.428236, mae: 151.684687, mean_q: 316.738672, mean_tau: 0.090823\n",
            "  4692/50000: episode: 197, duration: 0.326s, episode steps:  42, steps per second: 129, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 214.924953, mae: 146.798501, mean_q: 310.046139, mean_tau: 0.090752\n",
            "  4706/50000: episode: 198, duration: 0.116s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 326.855714, mae: 148.547991, mean_q: 312.028863, mean_tau: 0.090697\n",
            "  4731/50000: episode: 199, duration: 0.201s, episode steps:  25, steps per second: 124, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 236.833777, mae: 152.461705, mean_q: 318.198844, mean_tau: 0.090658\n",
            "  4770/50000: episode: 200, duration: 0.326s, episode steps:  39, steps per second: 120, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 240.801501, mae: 152.303758, mean_q: 318.824085, mean_tau: 0.090595\n",
            "  4787/50000: episode: 201, duration: 0.141s, episode steps:  17, steps per second: 121, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 331.209243, mae: 151.898206, mean_q: 313.996403, mean_tau: 0.090540\n",
            "  4831/50000: episode: 202, duration: 0.361s, episode steps:  44, steps per second: 122, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 223.520411, mae: 153.760454, mean_q: 321.368278, mean_tau: 0.090479\n",
            "  4850/50000: episode: 203, duration: 0.151s, episode steps:  19, steps per second: 126, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 189.392170, mae: 151.894866, mean_q: 317.539885, mean_tau: 0.090417\n",
            "  4881/50000: episode: 204, duration: 0.263s, episode steps:  31, steps per second: 118, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 240.093491, mae: 152.543139, mean_q: 321.364530, mean_tau: 0.090367\n",
            "  4917/50000: episode: 205, duration: 0.293s, episode steps:  36, steps per second: 123, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 305.589589, mae: 149.621780, mean_q: 309.815654, mean_tau: 0.090301\n",
            "  4944/50000: episode: 206, duration: 0.217s, episode steps:  27, steps per second: 125, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 250.263244, mae: 155.224795, mean_q: 328.812606, mean_tau: 0.090239\n",
            "  4963/50000: episode: 207, duration: 0.169s, episode steps:  19, steps per second: 113, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 295.258132, mae: 154.057148, mean_q: 321.203562, mean_tau: 0.090193\n",
            "  4982/50000: episode: 208, duration: 0.150s, episode steps:  19, steps per second: 127, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 286.101091, mae: 154.476903, mean_q: 324.097165, mean_tau: 0.090155\n",
            "  5003/50000: episode: 209, duration: 0.188s, episode steps:  21, steps per second: 112, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 204.934672, mae: 148.512071, mean_q: 316.465078, mean_tau: 0.090116\n",
            "  5026/50000: episode: 210, duration: 0.196s, episode steps:  23, steps per second: 117, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 299.113684, mae: 157.307995, mean_q: 329.171278, mean_tau: 0.090072\n",
            "  5045/50000: episode: 211, duration: 0.165s, episode steps:  19, steps per second: 115, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 199.334174, mae: 156.564199, mean_q: 323.256690, mean_tau: 0.090031\n",
            "  5067/50000: episode: 212, duration: 0.182s, episode steps:  22, steps per second: 121, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 204.714175, mae: 152.295688, mean_q: 319.344283, mean_tau: 0.089990\n",
            "  5087/50000: episode: 213, duration: 0.162s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 180.481212, mae: 153.795911, mean_q: 326.854756, mean_tau: 0.089949\n",
            "  5121/50000: episode: 214, duration: 0.283s, episode steps:  34, steps per second: 120, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 211.816267, mae: 156.043653, mean_q: 328.635036, mean_tau: 0.089895\n",
            "  5144/50000: episode: 215, duration: 0.183s, episode steps:  23, steps per second: 125, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 236.308118, mae: 159.115217, mean_q: 331.803830, mean_tau: 0.089839\n",
            "  5162/50000: episode: 216, duration: 0.140s, episode steps:  18, steps per second: 128, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 200.423097, mae: 161.005001, mean_q: 339.901730, mean_tau: 0.089798\n",
            "  5180/50000: episode: 217, duration: 0.138s, episode steps:  18, steps per second: 130, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 212.587735, mae: 154.220100, mean_q: 324.913089, mean_tau: 0.089762\n",
            "  5193/50000: episode: 218, duration: 0.107s, episode steps:  13, steps per second: 121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 238.255449, mae: 159.420370, mean_q: 333.456172, mean_tau: 0.089732\n",
            "  5219/50000: episode: 219, duration: 0.213s, episode steps:  26, steps per second: 122, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 195.977253, mae: 154.945730, mean_q: 328.939524, mean_tau: 0.089693\n",
            "  5233/50000: episode: 220, duration: 0.126s, episode steps:  14, steps per second: 111, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 249.311900, mae: 156.843282, mean_q: 325.447817, mean_tau: 0.089654\n",
            "  5247/50000: episode: 221, duration: 0.134s, episode steps:  14, steps per second: 105, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 228.018889, mae: 159.179837, mean_q: 334.612619, mean_tau: 0.089626\n",
            "  5260/50000: episode: 222, duration: 0.112s, episode steps:  13, steps per second: 116, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 152.719075, mae: 159.563090, mean_q: 333.963797, mean_tau: 0.089599\n",
            "  5278/50000: episode: 223, duration: 0.139s, episode steps:  18, steps per second: 129, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 215.423921, mae: 161.102790, mean_q: 332.793644, mean_tau: 0.089568\n",
            "  5307/50000: episode: 224, duration: 0.235s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 290.816985, mae: 160.575852, mean_q: 336.082739, mean_tau: 0.089522\n",
            "  5324/50000: episode: 225, duration: 0.139s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 258.355390, mae: 161.036144, mean_q: 338.878416, mean_tau: 0.089476\n",
            "  5352/50000: episode: 226, duration: 0.274s, episode steps:  28, steps per second: 102, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 254.940729, mae: 159.481725, mean_q: 332.590183, mean_tau: 0.089432\n",
            "  5388/50000: episode: 227, duration: 0.298s, episode steps:  36, steps per second: 121, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 259.314528, mae: 160.772988, mean_q: 334.990595, mean_tau: 0.089368\n",
            "  5410/50000: episode: 228, duration: 0.169s, episode steps:  22, steps per second: 130, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 259.481415, mae: 161.370263, mean_q: 339.256966, mean_tau: 0.089311\n",
            "  5443/50000: episode: 229, duration: 0.256s, episode steps:  33, steps per second: 129, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 319.489437, mae: 162.199955, mean_q: 335.602385, mean_tau: 0.089257\n",
            "  5481/50000: episode: 230, duration: 0.302s, episode steps:  38, steps per second: 126, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 228.429964, mae: 160.673330, mean_q: 334.428882, mean_tau: 0.089186\n",
            "  5512/50000: episode: 231, duration: 0.263s, episode steps:  31, steps per second: 118, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 180.323124, mae: 160.455689, mean_q: 338.768812, mean_tau: 0.089118\n",
            "  5532/50000: episode: 232, duration: 0.161s, episode steps:  20, steps per second: 125, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 168.767653, mae: 167.431830, mean_q: 350.393645, mean_tau: 0.089067\n",
            "  5562/50000: episode: 233, duration: 0.245s, episode steps:  30, steps per second: 122, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 220.811427, mae: 161.290269, mean_q: 338.794715, mean_tau: 0.089018\n",
            "  5582/50000: episode: 234, duration: 0.158s, episode steps:  20, steps per second: 126, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 198.278691, mae: 161.984485, mean_q: 339.903215, mean_tau: 0.088968\n",
            "  5607/50000: episode: 235, duration: 0.200s, episode steps:  25, steps per second: 125, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 221.971029, mae: 166.143298, mean_q: 348.302510, mean_tau: 0.088924\n",
            "  5638/50000: episode: 236, duration: 0.252s, episode steps:  31, steps per second: 123, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 204.555903, mae: 166.459319, mean_q: 346.015587, mean_tau: 0.088868\n",
            "  5659/50000: episode: 237, duration: 0.164s, episode steps:  21, steps per second: 128, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 265.146377, mae: 166.291290, mean_q: 348.548155, mean_tau: 0.088817\n",
            "  5679/50000: episode: 238, duration: 0.157s, episode steps:  20, steps per second: 128, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 379.619299, mae: 167.485758, mean_q: 346.463388, mean_tau: 0.088776\n",
            "  5699/50000: episode: 239, duration: 0.151s, episode steps:  20, steps per second: 132, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 214.102678, mae: 167.941191, mean_q: 353.720683, mean_tau: 0.088737\n",
            "  5737/50000: episode: 240, duration: 0.334s, episode steps:  38, steps per second: 114, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 202.369015, mae: 162.994604, mean_q: 343.504788, mean_tau: 0.088679\n",
            "  5758/50000: episode: 241, duration: 0.179s, episode steps:  21, steps per second: 117, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 306.441686, mae: 166.315497, mean_q: 345.918932, mean_tau: 0.088621\n",
            "  5787/50000: episode: 242, duration: 0.234s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 315.537178, mae: 171.438936, mean_q: 356.123745, mean_tau: 0.088571\n",
            "  5805/50000: episode: 243, duration: 0.143s, episode steps:  18, steps per second: 126, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 210.446567, mae: 167.671914, mean_q: 351.624700, mean_tau: 0.088525\n",
            "  5823/50000: episode: 244, duration: 0.213s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 251.414670, mae: 164.738165, mean_q: 345.535514, mean_tau: 0.088489\n",
            "  5868/50000: episode: 245, duration: 0.514s, episode steps:  45, steps per second:  88, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 289.039860, mae: 167.595839, mean_q: 352.227410, mean_tau: 0.088427\n",
            "  5885/50000: episode: 246, duration: 0.195s, episode steps:  17, steps per second:  87, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 266.595001, mae: 173.227560, mean_q: 364.684875, mean_tau: 0.088366\n",
            "  5910/50000: episode: 247, duration: 0.299s, episode steps:  25, steps per second:  84, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 356.975875, mae: 173.092569, mean_q: 360.315228, mean_tau: 0.088324\n",
            "  5944/50000: episode: 248, duration: 0.412s, episode steps:  34, steps per second:  83, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 339.943510, mae: 169.963491, mean_q: 353.442613, mean_tau: 0.088266\n",
            "  5965/50000: episode: 249, duration: 0.237s, episode steps:  21, steps per second:  88, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 325.901054, mae: 172.119402, mean_q: 354.643502, mean_tau: 0.088211\n",
            "  5978/50000: episode: 250, duration: 0.150s, episode steps:  13, steps per second:  87, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 196.544810, mae: 175.351356, mean_q: 366.523210, mean_tau: 0.088177\n",
            "  6002/50000: episode: 251, duration: 0.287s, episode steps:  24, steps per second:  84, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 361.452911, mae: 174.508337, mean_q: 364.332448, mean_tau: 0.088141\n",
            "  6026/50000: episode: 252, duration: 0.305s, episode steps:  24, steps per second:  79, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 260.448140, mae: 172.466707, mean_q: 361.523909, mean_tau: 0.088093\n",
            "  6061/50000: episode: 253, duration: 0.328s, episode steps:  35, steps per second: 107, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 318.919780, mae: 170.706847, mean_q: 359.824318, mean_tau: 0.088035\n",
            "  6088/50000: episode: 254, duration: 0.193s, episode steps:  27, steps per second: 140, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 256.333521, mae: 170.339809, mean_q: 356.644818, mean_tau: 0.087973\n",
            "  6111/50000: episode: 255, duration: 0.179s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 181.376067, mae: 173.657146, mean_q: 364.828887, mean_tau: 0.087924\n",
            "  6139/50000: episode: 256, duration: 0.226s, episode steps:  28, steps per second: 124, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 253.584119, mae: 177.397539, mean_q: 372.962446, mean_tau: 0.087873\n",
            "  6157/50000: episode: 257, duration: 0.154s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 398.369692, mae: 171.452352, mean_q: 360.112693, mean_tau: 0.087828\n",
            "  6172/50000: episode: 258, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 270.173031, mae: 174.150159, mean_q: 368.849945, mean_tau: 0.087795\n",
            "  6211/50000: episode: 259, duration: 0.295s, episode steps:  39, steps per second: 132, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 354.622188, mae: 172.437587, mean_q: 360.089257, mean_tau: 0.087742\n",
            "  6244/50000: episode: 260, duration: 0.277s, episode steps:  33, steps per second: 119, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 307.114555, mae: 171.770005, mean_q: 358.711432, mean_tau: 0.087671\n",
            "  6258/50000: episode: 261, duration: 0.109s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 274.472878, mae: 178.670004, mean_q: 373.723958, mean_tau: 0.087624\n",
            "  6289/50000: episode: 262, duration: 0.241s, episode steps:  31, steps per second: 129, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 324.358077, mae: 175.392202, mean_q: 366.100899, mean_tau: 0.087579\n",
            "  6308/50000: episode: 263, duration: 0.147s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 266.581946, mae: 175.040680, mean_q: 367.883633, mean_tau: 0.087530\n",
            "  6353/50000: episode: 264, duration: 0.333s, episode steps:  45, steps per second: 135, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 323.660048, mae: 177.428765, mean_q: 370.563467, mean_tau: 0.087467\n",
            "  6370/50000: episode: 265, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 248.566385, mae: 178.984249, mean_q: 376.228584, mean_tau: 0.087405\n",
            "  6390/50000: episode: 266, duration: 0.171s, episode steps:  20, steps per second: 117, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 295.009443, mae: 178.186745, mean_q: 373.257600, mean_tau: 0.087369\n",
            "  6403/50000: episode: 267, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 397.218682, mae: 176.795368, mean_q: 367.533187, mean_tau: 0.087336\n",
            "  6420/50000: episode: 268, duration: 0.148s, episode steps:  17, steps per second: 115, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 311.149125, mae: 179.040962, mean_q: 376.903855, mean_tau: 0.087306\n",
            "  6486/50000: episode: 269, duration: 0.491s, episode steps:  66, steps per second: 134, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 303.573062, mae: 180.933914, mean_q: 376.210767, mean_tau: 0.087224\n",
            "  6512/50000: episode: 270, duration: 0.198s, episode steps:  26, steps per second: 131, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 269.941300, mae: 177.337254, mean_q: 371.589886, mean_tau: 0.087133\n",
            "  6531/50000: episode: 271, duration: 0.152s, episode steps:  19, steps per second: 125, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 378.723538, mae: 175.730124, mean_q: 365.990604, mean_tau: 0.087088\n",
            "  6543/50000: episode: 272, duration: 0.108s, episode steps:  12, steps per second: 111, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 418.919247, mae: 175.917675, mean_q: 367.027265, mean_tau: 0.087058\n",
            "  6562/50000: episode: 273, duration: 0.158s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 233.934869, mae: 178.341598, mean_q: 374.313714, mean_tau: 0.087027\n",
            "  6581/50000: episode: 274, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 256.120424, mae: 179.649630, mean_q: 376.494544, mean_tau: 0.086989\n",
            "  6595/50000: episode: 275, duration: 0.112s, episode steps:  14, steps per second: 125, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 411.436666, mae: 176.829615, mean_q: 373.977467, mean_tau: 0.086957\n",
            "  6607/50000: episode: 276, duration: 0.099s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 393.834743, mae: 181.016989, mean_q: 376.822461, mean_tau: 0.086931\n",
            "  6632/50000: episode: 277, duration: 0.205s, episode steps:  25, steps per second: 122, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 265.321113, mae: 181.845305, mean_q: 378.359069, mean_tau: 0.086894\n",
            "  6647/50000: episode: 278, duration: 0.134s, episode steps:  15, steps per second: 112, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 277.599173, mae: 184.388472, mean_q: 381.934534, mean_tau: 0.086855\n",
            "  6692/50000: episode: 279, duration: 0.364s, episode steps:  45, steps per second: 124, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 335.466337, mae: 181.584614, mean_q: 376.080983, mean_tau: 0.086795\n",
            "  6705/50000: episode: 280, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 349.911760, mae: 182.339391, mean_q: 375.823998, mean_tau: 0.086738\n",
            "  6738/50000: episode: 281, duration: 0.250s, episode steps:  33, steps per second: 132, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 391.106024, mae: 183.468047, mean_q: 379.627000, mean_tau: 0.086692\n",
            "  6758/50000: episode: 282, duration: 0.155s, episode steps:  20, steps per second: 129, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 357.349335, mae: 188.267228, mean_q: 390.063535, mean_tau: 0.086640\n",
            "  6773/50000: episode: 283, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 248.259141, mae: 185.386524, mean_q: 384.481913, mean_tau: 0.086605\n",
            "  6789/50000: episode: 284, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 190.356016, mae: 185.040215, mean_q: 384.592340, mean_tau: 0.086575\n",
            "  6806/50000: episode: 285, duration: 0.143s, episode steps:  17, steps per second: 119, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 246.936978, mae: 184.535326, mean_q: 382.926131, mean_tau: 0.086542\n",
            "  6822/50000: episode: 286, duration: 0.122s, episode steps:  16, steps per second: 131, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 486.709666, mae: 184.350633, mean_q: 384.072071, mean_tau: 0.086509\n",
            "  6873/50000: episode: 287, duration: 0.382s, episode steps:  51, steps per second: 133, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 381.595024, mae: 184.749189, mean_q: 384.661381, mean_tau: 0.086443\n",
            "  6897/50000: episode: 288, duration: 0.208s, episode steps:  24, steps per second: 115, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 379.980095, mae: 182.416828, mean_q: 379.202835, mean_tau: 0.086369\n",
            "  6911/50000: episode: 289, duration: 0.125s, episode steps:  14, steps per second: 112, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 320.350848, mae: 182.664219, mean_q: 378.960242, mean_tau: 0.086331\n",
            "  6923/50000: episode: 290, duration: 0.106s, episode steps:  12, steps per second: 113, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 455.263522, mae: 184.246159, mean_q: 384.152702, mean_tau: 0.086305\n",
            "  6954/50000: episode: 291, duration: 0.246s, episode steps:  31, steps per second: 126, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 405.680423, mae: 185.053646, mean_q: 378.358165, mean_tau: 0.086263\n",
            "  6975/50000: episode: 292, duration: 0.179s, episode steps:  21, steps per second: 117, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 408.744393, mae: 184.625521, mean_q: 383.685758, mean_tau: 0.086211\n",
            "  6996/50000: episode: 293, duration: 0.175s, episode steps:  21, steps per second: 120, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 343.793161, mae: 184.483610, mean_q: 384.777424, mean_tau: 0.086170\n",
            "  7020/50000: episode: 294, duration: 0.181s, episode steps:  24, steps per second: 133, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 380.257296, mae: 186.646720, mean_q: 389.963023, mean_tau: 0.086125\n",
            "  7036/50000: episode: 295, duration: 0.135s, episode steps:  16, steps per second: 119, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 291.429808, mae: 189.300387, mean_q: 396.166576, mean_tau: 0.086086\n",
            "  7066/50000: episode: 296, duration: 0.235s, episode steps:  30, steps per second: 128, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 424.681906, mae: 188.843777, mean_q: 394.940357, mean_tau: 0.086040\n",
            "  7109/50000: episode: 297, duration: 0.326s, episode steps:  43, steps per second: 132, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 350.885027, mae: 188.715204, mean_q: 390.964380, mean_tau: 0.085968\n",
            "  7123/50000: episode: 298, duration: 0.111s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 327.353246, mae: 186.733563, mean_q: 392.332770, mean_tau: 0.085911\n",
            "  7135/50000: episode: 299, duration: 0.105s, episode steps:  12, steps per second: 115, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 339.399403, mae: 190.654048, mean_q: 397.790644, mean_tau: 0.085886\n",
            "  7204/50000: episode: 300, duration: 0.543s, episode steps:  69, steps per second: 127, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 337.401775, mae: 187.192640, mean_q: 389.469749, mean_tau: 0.085805\n",
            "  7219/50000: episode: 301, duration: 0.117s, episode steps:  15, steps per second: 128, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 398.740312, mae: 189.289529, mean_q: 392.658921, mean_tau: 0.085722\n",
            "  7237/50000: episode: 302, duration: 0.157s, episode steps:  18, steps per second: 115, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 249.113889, mae: 191.227190, mean_q: 403.718219, mean_tau: 0.085690\n",
            "  7256/50000: episode: 303, duration: 0.159s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 357.819415, mae: 187.541012, mean_q: 392.309739, mean_tau: 0.085653\n",
            "  7285/50000: episode: 304, duration: 0.230s, episode steps:  29, steps per second: 126, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 331.085299, mae: 187.690213, mean_q: 393.760078, mean_tau: 0.085605\n",
            "  7299/50000: episode: 305, duration: 0.143s, episode steps:  14, steps per second:  98, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 425.065941, mae: 187.164302, mean_q: 394.925278, mean_tau: 0.085563\n",
            "  7311/50000: episode: 306, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 225.301410, mae: 194.207877, mean_q: 409.301895, mean_tau: 0.085537\n",
            "  7349/50000: episode: 307, duration: 0.458s, episode steps:  38, steps per second:  83, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 342.457107, mae: 193.034903, mean_q: 401.792088, mean_tau: 0.085488\n",
            "  7367/50000: episode: 308, duration: 0.225s, episode steps:  18, steps per second:  80, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 328.998322, mae: 190.587391, mean_q: 396.828725, mean_tau: 0.085432\n",
            "  7387/50000: episode: 309, duration: 0.247s, episode steps:  20, steps per second:  81, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 398.395088, mae: 192.425193, mean_q: 401.043121, mean_tau: 0.085395\n",
            "  7403/50000: episode: 310, duration: 0.184s, episode steps:  16, steps per second:  87, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 285.933698, mae: 190.997233, mean_q: 399.291262, mean_tau: 0.085359\n",
            "  7422/50000: episode: 311, duration: 0.226s, episode steps:  19, steps per second:  84, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 414.673423, mae: 188.005447, mean_q: 391.803886, mean_tau: 0.085324\n",
            "  7451/50000: episode: 312, duration: 0.339s, episode steps:  29, steps per second:  86, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 381.058491, mae: 197.141147, mean_q: 407.892784, mean_tau: 0.085277\n",
            "  7471/50000: episode: 313, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 320.061062, mae: 193.853038, mean_q: 401.499344, mean_tau: 0.085228\n",
            "  7486/50000: episode: 314, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 589.393949, mae: 191.755441, mean_q: 391.110459, mean_tau: 0.085194\n",
            "  7522/50000: episode: 315, duration: 0.437s, episode steps:  36, steps per second:  82, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 483.760626, mae: 191.747194, mean_q: 404.593465, mean_tau: 0.085143\n",
            "  7541/50000: episode: 316, duration: 0.159s, episode steps:  19, steps per second: 119, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 397.326867, mae: 195.698763, mean_q: 407.843646, mean_tau: 0.085089\n",
            "  7604/50000: episode: 317, duration: 0.492s, episode steps:  63, steps per second: 128, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 387.564739, mae: 194.643921, mean_q: 407.057235, mean_tau: 0.085007\n",
            "  7629/50000: episode: 318, duration: 0.201s, episode steps:  25, steps per second: 125, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 486.476243, mae: 195.436945, mean_q: 407.098131, mean_tau: 0.084920\n",
            "  7666/50000: episode: 319, duration: 0.299s, episode steps:  37, steps per second: 124, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 346.194358, mae: 195.524716, mean_q: 409.839261, mean_tau: 0.084859\n",
            "  7686/50000: episode: 320, duration: 0.168s, episode steps:  20, steps per second: 119, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 494.137058, mae: 195.141887, mean_q: 407.519582, mean_tau: 0.084803\n",
            "  7698/50000: episode: 321, duration: 0.112s, episode steps:  12, steps per second: 107, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 560.030852, mae: 200.015929, mean_q: 413.009277, mean_tau: 0.084771\n",
            "  7716/50000: episode: 322, duration: 0.143s, episode steps:  18, steps per second: 126, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 429.642814, mae: 199.345078, mean_q: 415.101013, mean_tau: 0.084741\n",
            "  7732/50000: episode: 323, duration: 0.138s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 522.881254, mae: 194.447698, mean_q: 414.016918, mean_tau: 0.084707\n",
            "  7743/50000: episode: 324, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 605.322583, mae: 189.328269, mean_q: 391.416046, mean_tau: 0.084681\n",
            "  7800/50000: episode: 325, duration: 0.447s, episode steps:  57, steps per second: 128, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 404.332435, mae: 198.546735, mean_q: 415.474326, mean_tau: 0.084613\n",
            "  7839/50000: episode: 326, duration: 0.318s, episode steps:  39, steps per second: 123, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 402.782079, mae: 202.284632, mean_q: 418.689761, mean_tau: 0.084518\n",
            "  7865/50000: episode: 327, duration: 0.206s, episode steps:  26, steps per second: 126, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 468.455602, mae: 204.540367, mean_q: 427.462623, mean_tau: 0.084454\n",
            "  7877/50000: episode: 328, duration: 0.104s, episode steps:  12, steps per second: 115, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 364.239507, mae: 191.789043, mean_q: 396.810465, mean_tau: 0.084416\n",
            "  7920/50000: episode: 329, duration: 0.351s, episode steps:  43, steps per second: 122, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 540.277495, mae: 203.094011, mean_q: 423.797795, mean_tau: 0.084362\n",
            "  7934/50000: episode: 330, duration: 0.115s, episode steps:  14, steps per second: 122, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 416.432252, mae: 203.775410, mean_q: 425.627104, mean_tau: 0.084306\n",
            "  7947/50000: episode: 331, duration: 0.117s, episode steps:  13, steps per second: 111, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 367.779422, mae: 198.257064, mean_q: 422.490680, mean_tau: 0.084279\n",
            "  7975/50000: episode: 332, duration: 0.218s, episode steps:  28, steps per second: 128, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 372.620381, mae: 199.490491, mean_q: 423.297096, mean_tau: 0.084238\n",
            "  8002/50000: episode: 333, duration: 0.219s, episode steps:  27, steps per second: 124, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 604.437249, mae: 202.045255, mean_q: 416.627420, mean_tau: 0.084184\n",
            "  8017/50000: episode: 334, duration: 0.123s, episode steps:  15, steps per second: 122, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 375.269337, mae: 209.319288, mean_q: 434.373476, mean_tau: 0.084142\n",
            "  8037/50000: episode: 335, duration: 0.158s, episode steps:  20, steps per second: 127, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 551.991505, mae: 202.061619, mean_q: 424.881059, mean_tau: 0.084108\n",
            "  8060/50000: episode: 336, duration: 0.210s, episode steps:  23, steps per second: 109, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 545.366239, mae: 208.747754, mean_q: 435.340900, mean_tau: 0.084065\n",
            "  8081/50000: episode: 337, duration: 0.177s, episode steps:  21, steps per second: 119, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 505.542226, mae: 201.012462, mean_q: 425.941110, mean_tau: 0.084021\n",
            "  8103/50000: episode: 338, duration: 0.182s, episode steps:  22, steps per second: 121, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 445.844562, mae: 207.491477, mean_q: 436.806323, mean_tau: 0.083979\n",
            "  8137/50000: episode: 339, duration: 0.281s, episode steps:  34, steps per second: 121, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 518.344020, mae: 207.175646, mean_q: 434.464619, mean_tau: 0.083923\n",
            "  8166/50000: episode: 340, duration: 0.224s, episode steps:  29, steps per second: 129, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 515.687711, mae: 200.640062, mean_q: 421.993461, mean_tau: 0.083861\n",
            "  8250/50000: episode: 341, duration: 0.633s, episode steps:  84, steps per second: 133, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 461.123401, mae: 205.942122, mean_q: 431.063033, mean_tau: 0.083749\n",
            "  8269/50000: episode: 342, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 586.383737, mae: 201.784374, mean_q: 422.546306, mean_tau: 0.083647\n",
            "  8296/50000: episode: 343, duration: 0.201s, episode steps:  27, steps per second: 134, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 455.216625, mae: 207.978527, mean_q: 441.689433, mean_tau: 0.083602\n",
            "  8320/50000: episode: 344, duration: 0.181s, episode steps:  24, steps per second: 133, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 401.653371, mae: 213.447290, mean_q: 448.130037, mean_tau: 0.083551\n",
            "  8344/50000: episode: 345, duration: 0.206s, episode steps:  24, steps per second: 116, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 714.706317, mae: 210.092452, mean_q: 438.424071, mean_tau: 0.083504\n",
            "  8370/50000: episode: 346, duration: 0.203s, episode steps:  26, steps per second: 128, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 637.438156, mae: 209.095695, mean_q: 435.349321, mean_tau: 0.083454\n",
            "  8389/50000: episode: 347, duration: 0.158s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 639.755239, mae: 208.313935, mean_q: 433.910304, mean_tau: 0.083410\n",
            "  8410/50000: episode: 348, duration: 0.170s, episode steps:  21, steps per second: 124, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 461.037083, mae: 216.300059, mean_q: 447.861786, mean_tau: 0.083370\n",
            "  8431/50000: episode: 349, duration: 0.155s, episode steps:  21, steps per second: 135, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 700.717828, mae: 207.330154, mean_q: 436.609051, mean_tau: 0.083328\n",
            "  8444/50000: episode: 350, duration: 0.113s, episode steps:  13, steps per second: 115, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 295.331831, mae: 206.645392, mean_q: 437.394034, mean_tau: 0.083295\n",
            "  8460/50000: episode: 351, duration: 0.132s, episode steps:  16, steps per second: 122, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 385.918706, mae: 210.189964, mean_q: 440.654987, mean_tau: 0.083266\n",
            "  8471/50000: episode: 352, duration: 0.092s, episode steps:  11, steps per second: 119, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 764.090211, mae: 211.129383, mean_q: 445.955980, mean_tau: 0.083239\n",
            "  8501/50000: episode: 353, duration: 0.234s, episode steps:  30, steps per second: 128, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 681.938923, mae: 210.863687, mean_q: 440.215035, mean_tau: 0.083199\n",
            "  8543/50000: episode: 354, duration: 0.326s, episode steps:  42, steps per second: 129, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 503.964798, mae: 215.825101, mean_q: 451.024658, mean_tau: 0.083127\n",
            "  8566/50000: episode: 355, duration: 0.174s, episode steps:  23, steps per second: 132, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 491.535122, mae: 212.808021, mean_q: 449.144473, mean_tau: 0.083063\n",
            "  8586/50000: episode: 356, duration: 0.187s, episode steps:  20, steps per second: 107, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 426.010241, mae: 207.979694, mean_q: 437.648247, mean_tau: 0.083021\n",
            "  8600/50000: episode: 357, duration: 0.113s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 432.833115, mae: 217.654564, mean_q: 459.082801, mean_tau: 0.082987\n",
            "  8619/50000: episode: 358, duration: 0.164s, episode steps:  19, steps per second: 116, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 413.749088, mae: 212.588905, mean_q: 450.604165, mean_tau: 0.082954\n",
            "  8638/50000: episode: 359, duration: 0.174s, episode steps:  19, steps per second: 109, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 501.469392, mae: 215.645118, mean_q: 449.911321, mean_tau: 0.082917\n",
            "  8660/50000: episode: 360, duration: 0.181s, episode steps:  22, steps per second: 122, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 322.944618, mae: 214.572470, mean_q: 455.084176, mean_tau: 0.082876\n",
            "  8678/50000: episode: 361, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 645.715860, mae: 214.532242, mean_q: 453.245378, mean_tau: 0.082836\n",
            "  8697/50000: episode: 362, duration: 0.151s, episode steps:  19, steps per second: 126, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 609.091203, mae: 211.955357, mean_q: 447.081455, mean_tau: 0.082800\n",
            "  8712/50000: episode: 363, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 558.327243, mae: 223.274682, mean_q: 467.907100, mean_tau: 0.082766\n",
            "  8726/50000: episode: 364, duration: 0.125s, episode steps:  14, steps per second: 112, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 589.162468, mae: 209.349012, mean_q: 448.054770, mean_tau: 0.082737\n",
            "  8736/50000: episode: 365, duration: 0.086s, episode steps:  10, steps per second: 116, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 413.284846, mae: 222.172574, mean_q: 468.343552, mean_tau: 0.082714\n",
            "  8747/50000: episode: 366, duration: 0.095s, episode steps:  11, steps per second: 116, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 665.182235, mae: 218.795692, mean_q: 462.846277, mean_tau: 0.082693\n",
            "  8764/50000: episode: 367, duration: 0.190s, episode steps:  17, steps per second:  90, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 958.910447, mae: 221.421803, mean_q: 456.653419, mean_tau: 0.082665\n",
            "  8788/50000: episode: 368, duration: 0.285s, episode steps:  24, steps per second:  84, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 421.009653, mae: 216.958359, mean_q: 459.091712, mean_tau: 0.082625\n",
            "  8799/50000: episode: 369, duration: 0.130s, episode steps:  11, steps per second:  85, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 659.152245, mae: 214.496699, mean_q: 450.210388, mean_tau: 0.082590\n",
            "  8830/50000: episode: 370, duration: 0.356s, episode steps:  31, steps per second:  87, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.387 [0.000, 1.000],  loss: 726.734899, mae: 219.776584, mean_q: 458.554028, mean_tau: 0.082548\n",
            "  8852/50000: episode: 371, duration: 0.266s, episode steps:  22, steps per second:  83, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 528.316032, mae: 221.869291, mean_q: 465.726225, mean_tau: 0.082496\n",
            "  8867/50000: episode: 372, duration: 0.172s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 635.474867, mae: 223.775459, mean_q: 467.527775, mean_tau: 0.082459\n",
            "  8882/50000: episode: 373, duration: 0.183s, episode steps:  15, steps per second:  82, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 666.780115, mae: 215.561487, mean_q: 454.544745, mean_tau: 0.082429\n",
            "  8898/50000: episode: 374, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 523.400692, mae: 221.905077, mean_q: 470.612738, mean_tau: 0.082399\n",
            "  8940/50000: episode: 375, duration: 0.481s, episode steps:  42, steps per second:  87, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 524.678759, mae: 220.335548, mean_q: 462.852952, mean_tau: 0.082341\n",
            "  8958/50000: episode: 376, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 691.689322, mae: 216.288555, mean_q: 459.473095, mean_tau: 0.082282\n",
            "  8986/50000: episode: 377, duration: 0.334s, episode steps:  28, steps per second:  84, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 568.477158, mae: 221.003359, mean_q: 466.665422, mean_tau: 0.082236\n",
            "  9028/50000: episode: 378, duration: 0.346s, episode steps:  42, steps per second: 121, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 531.171903, mae: 222.476699, mean_q: 467.592467, mean_tau: 0.082167\n",
            "  9049/50000: episode: 379, duration: 0.186s, episode steps:  21, steps per second: 113, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 562.621949, mae: 227.377432, mean_q: 478.149129, mean_tau: 0.082105\n",
            "  9086/50000: episode: 380, duration: 0.291s, episode steps:  37, steps per second: 127, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 704.735618, mae: 218.551074, mean_q: 462.141287, mean_tau: 0.082047\n",
            "  9101/50000: episode: 381, duration: 0.120s, episode steps:  15, steps per second: 125, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 795.359163, mae: 221.211130, mean_q: 468.950863, mean_tau: 0.081996\n",
            "  9145/50000: episode: 382, duration: 0.354s, episode steps:  44, steps per second: 124, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 553.098014, mae: 224.301482, mean_q: 471.522742, mean_tau: 0.081937\n",
            "  9159/50000: episode: 383, duration: 0.111s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 933.756852, mae: 226.439083, mean_q: 472.765084, mean_tau: 0.081880\n",
            "  9189/50000: episode: 384, duration: 0.230s, episode steps:  30, steps per second: 131, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 573.243436, mae: 229.208464, mean_q: 476.408125, mean_tau: 0.081836\n",
            "  9210/50000: episode: 385, duration: 0.181s, episode steps:  21, steps per second: 116, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 718.252398, mae: 223.603077, mean_q: 470.400383, mean_tau: 0.081786\n",
            "  9239/50000: episode: 386, duration: 0.235s, episode steps:  29, steps per second: 123, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 646.572234, mae: 226.534708, mean_q: 468.845142, mean_tau: 0.081736\n",
            "  9250/50000: episode: 387, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 452.258004, mae: 230.175565, mean_q: 483.271518, mean_tau: 0.081697\n",
            "  9268/50000: episode: 388, duration: 0.147s, episode steps:  18, steps per second: 122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 637.334474, mae: 220.935513, mean_q: 468.623830, mean_tau: 0.081668\n",
            "  9295/50000: episode: 389, duration: 0.216s, episode steps:  27, steps per second: 125, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 669.075091, mae: 227.360435, mean_q: 478.878656, mean_tau: 0.081624\n",
            "  9311/50000: episode: 390, duration: 0.129s, episode steps:  16, steps per second: 124, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 593.303904, mae: 225.512862, mean_q: 469.474247, mean_tau: 0.081581\n",
            "  9330/50000: episode: 391, duration: 0.160s, episode steps:  19, steps per second: 119, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 549.599569, mae: 231.468883, mean_q: 486.410209, mean_tau: 0.081546\n",
            "  9346/50000: episode: 392, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 421.018637, mae: 228.075750, mean_q: 482.375393, mean_tau: 0.081512\n",
            "  9362/50000: episode: 393, duration: 0.152s, episode steps:  16, steps per second: 105, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 535.603624, mae: 228.010633, mean_q: 478.586777, mean_tau: 0.081480\n",
            "  9383/50000: episode: 394, duration: 0.162s, episode steps:  21, steps per second: 129, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 498.637723, mae: 235.094395, mean_q: 495.488374, mean_tau: 0.081443\n",
            "  9408/50000: episode: 395, duration: 0.209s, episode steps:  25, steps per second: 119, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 419.056068, mae: 231.242992, mean_q: 487.050710, mean_tau: 0.081398\n",
            "  9427/50000: episode: 396, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 599.761578, mae: 223.112599, mean_q: 470.755402, mean_tau: 0.081354\n",
            "  9439/50000: episode: 397, duration: 0.100s, episode steps:  12, steps per second: 121, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 848.104669, mae: 231.644231, mean_q: 490.919614, mean_tau: 0.081324\n",
            "  9456/50000: episode: 398, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 544.525041, mae: 227.941055, mean_q: 482.884518, mean_tau: 0.081295\n",
            "  9498/50000: episode: 399, duration: 0.363s, episode steps:  42, steps per second: 116, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 524.632987, mae: 226.382517, mean_q: 478.171461, mean_tau: 0.081237\n",
            "  9514/50000: episode: 400, duration: 0.129s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 693.243983, mae: 234.293302, mean_q: 489.346024, mean_tau: 0.081179\n",
            "  9529/50000: episode: 401, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 580.982665, mae: 233.552214, mean_q: 482.821885, mean_tau: 0.081148\n",
            "  9552/50000: episode: 402, duration: 0.183s, episode steps:  23, steps per second: 126, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 712.765280, mae: 229.426557, mean_q: 479.959820, mean_tau: 0.081111\n",
            "  9565/50000: episode: 403, duration: 0.116s, episode steps:  13, steps per second: 112, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 825.898076, mae: 227.178379, mean_q: 480.356438, mean_tau: 0.081075\n",
            "  9576/50000: episode: 404, duration: 0.089s, episode steps:  11, steps per second: 124, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 456.584041, mae: 231.824047, mean_q: 486.970107, mean_tau: 0.081051\n",
            "  9598/50000: episode: 405, duration: 0.184s, episode steps:  22, steps per second: 120, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 694.420953, mae: 225.420319, mean_q: 474.184383, mean_tau: 0.081019\n",
            "  9670/50000: episode: 406, duration: 0.545s, episode steps:  72, steps per second: 132, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 670.339850, mae: 231.212217, mean_q: 483.040176, mean_tau: 0.080926\n",
            "  9688/50000: episode: 407, duration: 0.146s, episode steps:  18, steps per second: 123, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 450.326200, mae: 235.916189, mean_q: 496.765876, mean_tau: 0.080837\n",
            "  9715/50000: episode: 408, duration: 0.223s, episode steps:  27, steps per second: 121, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 556.327875, mae: 232.478961, mean_q: 481.294074, mean_tau: 0.080792\n",
            "  9725/50000: episode: 409, duration: 0.085s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 616.867697, mae: 235.851346, mean_q: 492.469583, mean_tau: 0.080755\n",
            "  9764/50000: episode: 410, duration: 0.338s, episode steps:  39, steps per second: 115, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 648.812611, mae: 236.139307, mean_q: 491.166079, mean_tau: 0.080707\n",
            "  9802/50000: episode: 411, duration: 0.283s, episode steps:  38, steps per second: 134, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 625.046044, mae: 239.540633, mean_q: 498.381598, mean_tau: 0.080631\n",
            "  9826/50000: episode: 412, duration: 0.187s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 588.408260, mae: 236.768595, mean_q: 494.572123, mean_tau: 0.080569\n",
            "  9839/50000: episode: 413, duration: 0.111s, episode steps:  13, steps per second: 117, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 741.015339, mae: 230.166462, mean_q: 487.637228, mean_tau: 0.080533\n",
            "  9853/50000: episode: 414, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 512.863013, mae: 234.763404, mean_q: 491.245810, mean_tau: 0.080506\n",
            "  9869/50000: episode: 415, duration: 0.135s, episode steps:  16, steps per second: 118, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 785.458283, mae: 230.743581, mean_q: 482.868341, mean_tau: 0.080476\n",
            "  9888/50000: episode: 416, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 609.707114, mae: 243.860148, mean_q: 509.645580, mean_tau: 0.080442\n",
            "  9902/50000: episode: 417, duration: 0.116s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 592.531344, mae: 225.950597, mean_q: 481.199188, mean_tau: 0.080409\n",
            "  9914/50000: episode: 418, duration: 0.090s, episode steps:  12, steps per second: 134, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 723.865706, mae: 239.366669, mean_q: 492.438179, mean_tau: 0.080383\n",
            "  9928/50000: episode: 419, duration: 0.109s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 916.678465, mae: 233.775240, mean_q: 493.069693, mean_tau: 0.080357\n",
            "  9947/50000: episode: 420, duration: 0.155s, episode steps:  19, steps per second: 123, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 811.822287, mae: 238.632550, mean_q: 497.994504, mean_tau: 0.080325\n",
            "  9962/50000: episode: 421, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 385.698249, mae: 246.568442, mean_q: 509.748635, mean_tau: 0.080291\n",
            "  9981/50000: episode: 422, duration: 0.173s, episode steps:  19, steps per second: 110, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 496.882311, mae: 240.841110, mean_q: 505.550823, mean_tau: 0.080257\n",
            " 10000/50000: episode: 423, duration: 0.163s, episode steps:  19, steps per second: 117, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 567.413498, mae: 245.903867, mean_q: 510.546033, mean_tau: 0.080220\n",
            " 10036/50000: episode: 424, duration: 0.275s, episode steps:  36, steps per second: 131, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 616.630788, mae: 239.477371, mean_q: 503.909317, mean_tau: 0.080165\n",
            " 10052/50000: episode: 425, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 909.713712, mae: 240.018947, mean_q: 492.240786, mean_tau: 0.080114\n",
            " 10065/50000: episode: 426, duration: 0.108s, episode steps:  13, steps per second: 120, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 712.139073, mae: 243.193881, mean_q: 500.938305, mean_tau: 0.080085\n",
            " 10079/50000: episode: 427, duration: 0.111s, episode steps:  14, steps per second: 126, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 688.526689, mae: 233.466440, mean_q: 493.252792, mean_tau: 0.080058\n",
            " 10097/50000: episode: 428, duration: 0.152s, episode steps:  18, steps per second: 118, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 696.680289, mae: 235.736835, mean_q: 490.419540, mean_tau: 0.080027\n",
            " 10139/50000: episode: 429, duration: 0.319s, episode steps:  42, steps per second: 132, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 652.535890, mae: 240.776595, mean_q: 502.749969, mean_tau: 0.079967\n",
            " 10152/50000: episode: 430, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 834.201445, mae: 238.012533, mean_q: 499.834299, mean_tau: 0.079913\n",
            " 10181/50000: episode: 431, duration: 0.208s, episode steps:  29, steps per second: 140, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 624.370299, mae: 245.315987, mean_q: 513.600097, mean_tau: 0.079871\n",
            " 10201/50000: episode: 432, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 505.668108, mae: 240.809473, mean_q: 501.226846, mean_tau: 0.079823\n",
            " 10219/50000: episode: 433, duration: 0.150s, episode steps:  18, steps per second: 120, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 854.860090, mae: 240.795297, mean_q: 500.390766, mean_tau: 0.079785\n",
            " 10234/50000: episode: 434, duration: 0.151s, episode steps:  15, steps per second:  99, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 653.656421, mae: 240.523883, mean_q: 501.594706, mean_tau: 0.079753\n",
            " 10254/50000: episode: 435, duration: 0.258s, episode steps:  20, steps per second:  77, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 558.114627, mae: 244.599715, mean_q: 515.124020, mean_tau: 0.079718\n",
            " 10303/50000: episode: 436, duration: 0.543s, episode steps:  49, steps per second:  90, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 668.729795, mae: 247.847792, mean_q: 522.301971, mean_tau: 0.079650\n",
            " 10335/50000: episode: 437, duration: 0.366s, episode steps:  32, steps per second:  87, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 596.574799, mae: 250.947416, mean_q: 528.042005, mean_tau: 0.079569\n",
            " 10359/50000: episode: 438, duration: 0.278s, episode steps:  24, steps per second:  86, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 640.831157, mae: 247.709292, mean_q: 525.820087, mean_tau: 0.079514\n",
            " 10381/50000: episode: 439, duration: 0.275s, episode steps:  22, steps per second:  80, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 734.670969, mae: 244.249520, mean_q: 510.646716, mean_tau: 0.079468\n",
            " 10399/50000: episode: 440, duration: 0.226s, episode steps:  18, steps per second:  80, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 634.486740, mae: 242.981749, mean_q: 512.114705, mean_tau: 0.079429\n",
            " 10413/50000: episode: 441, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 591.608826, mae: 247.511384, mean_q: 521.999714, mean_tau: 0.079397\n",
            " 10457/50000: episode: 442, duration: 0.519s, episode steps:  44, steps per second:  85, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 792.198459, mae: 247.402113, mean_q: 514.477687, mean_tau: 0.079340\n",
            " 10499/50000: episode: 443, duration: 0.353s, episode steps:  42, steps per second: 119, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 766.302298, mae: 254.468829, mean_q: 531.300520, mean_tau: 0.079255\n",
            " 10524/50000: episode: 444, duration: 0.188s, episode steps:  25, steps per second: 133, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 703.864086, mae: 248.724980, mean_q: 521.454537, mean_tau: 0.079188\n",
            " 10539/50000: episode: 445, duration: 0.134s, episode steps:  15, steps per second: 112, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 845.767476, mae: 258.657957, mean_q: 535.804183, mean_tau: 0.079149\n",
            " 10556/50000: episode: 446, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 730.257741, mae: 262.236205, mean_q: 545.781485, mean_tau: 0.079117\n",
            " 10580/50000: episode: 447, duration: 0.204s, episode steps:  24, steps per second: 118, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 654.643056, mae: 250.062095, mean_q: 523.288918, mean_tau: 0.079076\n",
            " 10593/50000: episode: 448, duration: 0.106s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 649.859445, mae: 257.382165, mean_q: 544.561007, mean_tau: 0.079040\n",
            " 10608/50000: episode: 449, duration: 0.133s, episode steps:  15, steps per second: 112, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 952.660162, mae: 251.509823, mean_q: 529.004830, mean_tau: 0.079012\n",
            " 10627/50000: episode: 450, duration: 0.163s, episode steps:  19, steps per second: 117, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 875.392776, mae: 258.383277, mean_q: 539.148534, mean_tau: 0.078978\n",
            " 10659/50000: episode: 451, duration: 0.270s, episode steps:  32, steps per second: 119, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 897.043811, mae: 258.801825, mean_q: 543.071572, mean_tau: 0.078928\n",
            " 10673/50000: episode: 452, duration: 0.116s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1248.423094, mae: 255.173344, mean_q: 526.437363, mean_tau: 0.078882\n",
            " 10717/50000: episode: 453, duration: 0.348s, episode steps:  44, steps per second: 127, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 914.145188, mae: 257.892123, mean_q: 541.224760, mean_tau: 0.078825\n",
            " 10734/50000: episode: 454, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 694.469147, mae: 266.074217, mean_q: 557.358472, mean_tau: 0.078765\n",
            " 10758/50000: episode: 455, duration: 0.187s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 537.537988, mae: 262.006057, mean_q: 547.774368, mean_tau: 0.078724\n",
            " 10774/50000: episode: 456, duration: 0.118s, episode steps:  16, steps per second: 135, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 807.110520, mae: 262.595423, mean_q: 550.972969, mean_tau: 0.078684\n",
            " 10790/50000: episode: 457, duration: 0.145s, episode steps:  16, steps per second: 110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 843.843548, mae: 258.016525, mean_q: 530.679213, mean_tau: 0.078653\n",
            " 10806/50000: episode: 458, duration: 0.130s, episode steps:  16, steps per second: 123, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 664.695247, mae: 260.933599, mean_q: 550.722530, mean_tau: 0.078621\n",
            " 10822/50000: episode: 459, duration: 0.126s, episode steps:  16, steps per second: 126, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 873.509699, mae: 254.155878, mean_q: 530.951267, mean_tau: 0.078589\n",
            " 10846/50000: episode: 460, duration: 0.179s, episode steps:  24, steps per second: 134, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 874.820334, mae: 259.610458, mean_q: 534.536313, mean_tau: 0.078550\n",
            " 10856/50000: episode: 461, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 572.269749, mae: 268.778827, mean_q: 553.578986, mean_tau: 0.078516\n",
            " 10875/50000: episode: 462, duration: 0.150s, episode steps:  19, steps per second: 127, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1009.263531, mae: 256.073876, mean_q: 540.225779, mean_tau: 0.078487\n",
            " 10888/50000: episode: 463, duration: 0.108s, episode steps:  13, steps per second: 121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1239.467736, mae: 255.306881, mean_q: 535.789276, mean_tau: 0.078456\n",
            " 10909/50000: episode: 464, duration: 0.179s, episode steps:  21, steps per second: 117, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 957.357445, mae: 274.115930, mean_q: 570.759184, mean_tau: 0.078422\n",
            " 10927/50000: episode: 465, duration: 0.139s, episode steps:  18, steps per second: 129, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 764.458167, mae: 256.653542, mean_q: 533.565738, mean_tau: 0.078383\n",
            " 10962/50000: episode: 466, duration: 0.291s, episode steps:  35, steps per second: 120, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 831.063628, mae: 259.694104, mean_q: 546.066560, mean_tau: 0.078331\n",
            " 10997/50000: episode: 467, duration: 0.273s, episode steps:  35, steps per second: 128, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 846.390173, mae: 262.899452, mean_q: 551.862142, mean_tau: 0.078262\n",
            " 11024/50000: episode: 468, duration: 0.217s, episode steps:  27, steps per second: 124, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 778.759422, mae: 256.855780, mean_q: 541.899640, mean_tau: 0.078200\n",
            " 11044/50000: episode: 469, duration: 0.168s, episode steps:  20, steps per second: 119, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 999.372029, mae: 270.814001, mean_q: 561.950664, mean_tau: 0.078154\n",
            " 11068/50000: episode: 470, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 854.035242, mae: 272.411391, mean_q: 563.088036, mean_tau: 0.078110\n",
            " 11104/50000: episode: 471, duration: 0.263s, episode steps:  36, steps per second: 137, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 892.881386, mae: 260.677277, mean_q: 548.146292, mean_tau: 0.078051\n",
            " 11117/50000: episode: 472, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 859.106598, mae: 265.482570, mean_q: 552.323120, mean_tau: 0.078002\n",
            " 11134/50000: episode: 473, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 798.280954, mae: 266.545696, mean_q: 558.079611, mean_tau: 0.077973\n",
            " 11154/50000: episode: 474, duration: 0.146s, episode steps:  20, steps per second: 137, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1201.065968, mae: 264.031303, mean_q: 545.135616, mean_tau: 0.077936\n",
            " 11221/50000: episode: 475, duration: 0.512s, episode steps:  67, steps per second: 131, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 981.618980, mae: 260.826272, mean_q: 547.775879, mean_tau: 0.077850\n",
            " 11239/50000: episode: 476, duration: 0.134s, episode steps:  18, steps per second: 134, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 733.201486, mae: 267.131886, mean_q: 560.793484, mean_tau: 0.077766\n",
            " 11267/50000: episode: 477, duration: 0.213s, episode steps:  28, steps per second: 132, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 832.110096, mae: 272.078676, mean_q: 571.007795, mean_tau: 0.077720\n",
            " 11279/50000: episode: 478, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 747.772508, mae: 268.900462, mean_q: 572.818298, mean_tau: 0.077680\n",
            " 11297/50000: episode: 479, duration: 0.147s, episode steps:  18, steps per second: 122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 746.455201, mae: 265.100357, mean_q: 556.512037, mean_tau: 0.077651\n",
            " 11308/50000: episode: 480, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 832.812592, mae: 264.444525, mean_q: 549.246355, mean_tau: 0.077622\n",
            " 11345/50000: episode: 481, duration: 0.281s, episode steps:  37, steps per second: 132, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1183.564609, mae: 264.795366, mean_q: 550.505234, mean_tau: 0.077575\n",
            " 11365/50000: episode: 482, duration: 0.148s, episode steps:  20, steps per second: 135, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 733.041220, mae: 271.803594, mean_q: 571.259006, mean_tau: 0.077518\n",
            " 11381/50000: episode: 483, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 630.750166, mae: 267.441134, mean_q: 557.187998, mean_tau: 0.077482\n",
            " 11400/50000: episode: 484, duration: 0.150s, episode steps:  19, steps per second: 127, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1218.843348, mae: 268.788492, mean_q: 562.714249, mean_tau: 0.077448\n",
            " 11412/50000: episode: 485, duration: 0.108s, episode steps:  12, steps per second: 112, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1023.471886, mae: 257.785333, mean_q: 537.173106, mean_tau: 0.077417\n",
            " 11427/50000: episode: 486, duration: 0.129s, episode steps:  15, steps per second: 116, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1045.032320, mae: 267.187128, mean_q: 563.243787, mean_tau: 0.077390\n",
            " 11450/50000: episode: 487, duration: 0.188s, episode steps:  23, steps per second: 122, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1065.746310, mae: 268.000925, mean_q: 563.495522, mean_tau: 0.077353\n",
            " 11463/50000: episode: 488, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 646.427469, mae: 283.972537, mean_q: 596.142034, mean_tau: 0.077317\n",
            " 11480/50000: episode: 489, duration: 0.141s, episode steps:  17, steps per second: 121, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1168.687059, mae: 264.621674, mean_q: 559.095403, mean_tau: 0.077287\n",
            " 11548/50000: episode: 490, duration: 0.527s, episode steps:  68, steps per second: 129, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 872.490002, mae: 272.119169, mean_q: 569.610847, mean_tau: 0.077203\n",
            " 11565/50000: episode: 491, duration: 0.141s, episode steps:  17, steps per second: 120, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 982.827976, mae: 277.435452, mean_q: 574.411761, mean_tau: 0.077119\n",
            " 11584/50000: episode: 492, duration: 0.156s, episode steps:  19, steps per second: 121, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1038.044358, mae: 272.451647, mean_q: 574.708493, mean_tau: 0.077083\n",
            " 11603/50000: episode: 493, duration: 0.155s, episode steps:  19, steps per second: 123, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 958.308305, mae: 279.713562, mean_q: 578.290476, mean_tau: 0.077046\n",
            " 11618/50000: episode: 494, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 807.102948, mae: 283.566990, mean_q: 600.382422, mean_tau: 0.077012\n",
            " 11632/50000: episode: 495, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 959.057173, mae: 271.189929, mean_q: 570.063771, mean_tau: 0.076983\n",
            " 11656/50000: episode: 496, duration: 0.181s, episode steps:  24, steps per second: 132, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 725.615347, mae: 275.838511, mean_q: 584.636412, mean_tau: 0.076946\n",
            " 11676/50000: episode: 497, duration: 0.153s, episode steps:  20, steps per second: 131, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 978.402936, mae: 282.217863, mean_q: 588.828616, mean_tau: 0.076902\n",
            " 11696/50000: episode: 498, duration: 0.183s, episode steps:  20, steps per second: 109, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1245.269284, mae: 285.471306, mean_q: 592.728378, mean_tau: 0.076863\n",
            " 11713/50000: episode: 499, duration: 0.138s, episode steps:  17, steps per second: 123, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1078.501115, mae: 277.378382, mean_q: 586.852891, mean_tau: 0.076826\n",
            " 11728/50000: episode: 500, duration: 0.174s, episode steps:  15, steps per second:  86, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 899.017149, mae: 281.755855, mean_q: 587.840625, mean_tau: 0.076794\n",
            " 11749/50000: episode: 501, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1438.967057, mae: 279.647868, mean_q: 575.460461, mean_tau: 0.076759\n",
            " 11777/50000: episode: 502, duration: 0.318s, episode steps:  28, steps per second:  88, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 1014.408648, mae: 278.370736, mean_q: 578.803899, mean_tau: 0.076710\n",
            " 11800/50000: episode: 503, duration: 0.268s, episode steps:  23, steps per second:  86, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1132.507826, mae: 279.383218, mean_q: 585.305433, mean_tau: 0.076660\n",
            " 11860/50000: episode: 504, duration: 0.679s, episode steps:  60, steps per second:  88, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 979.401950, mae: 281.422528, mean_q: 585.715314, mean_tau: 0.076578\n",
            " 11884/50000: episode: 505, duration: 0.279s, episode steps:  24, steps per second:  86, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1093.720066, mae: 285.630591, mean_q: 592.815048, mean_tau: 0.076494\n",
            " 11909/50000: episode: 506, duration: 0.289s, episode steps:  25, steps per second:  87, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1006.691013, mae: 283.597253, mean_q: 592.053224, mean_tau: 0.076446\n",
            " 11946/50000: episode: 507, duration: 0.438s, episode steps:  37, steps per second:  85, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1400.260451, mae: 278.109135, mean_q: 578.872822, mean_tau: 0.076385\n",
            " 11982/50000: episode: 508, duration: 0.326s, episode steps:  36, steps per second: 110, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1025.960673, mae: 282.197955, mean_q: 585.039834, mean_tau: 0.076312\n",
            " 11996/50000: episode: 509, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 662.577218, mae: 286.492778, mean_q: 606.701795, mean_tau: 0.076263\n",
            " 12022/50000: episode: 510, duration: 0.189s, episode steps:  26, steps per second: 138, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 865.994481, mae: 290.201552, mean_q: 605.688612, mean_tau: 0.076223\n",
            " 12043/50000: episode: 511, duration: 0.154s, episode steps:  21, steps per second: 137, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1261.287718, mae: 285.354726, mean_q: 597.103986, mean_tau: 0.076177\n",
            " 12053/50000: episode: 512, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1161.462061, mae: 282.910321, mean_q: 591.612854, mean_tau: 0.076146\n",
            " 12068/50000: episode: 513, duration: 0.107s, episode steps:  15, steps per second: 140, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1173.692123, mae: 289.255725, mean_q: 605.575427, mean_tau: 0.076121\n",
            " 12090/50000: episode: 514, duration: 0.171s, episode steps:  22, steps per second: 129, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 768.231374, mae: 291.622192, mean_q: 611.188937, mean_tau: 0.076085\n",
            " 12103/50000: episode: 515, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1044.941544, mae: 284.487208, mean_q: 601.774771, mean_tau: 0.076050\n",
            " 12117/50000: episode: 516, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 839.885130, mae: 290.035784, mean_q: 608.283548, mean_tau: 0.076023\n",
            " 12133/50000: episode: 517, duration: 0.128s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1184.525296, mae: 286.820393, mean_q: 600.432880, mean_tau: 0.075993\n",
            " 12156/50000: episode: 518, duration: 0.185s, episode steps:  23, steps per second: 125, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1054.442510, mae: 276.392456, mean_q: 579.805200, mean_tau: 0.075955\n",
            " 12172/50000: episode: 519, duration: 0.115s, episode steps:  16, steps per second: 140, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2228.912590, mae: 284.309303, mean_q: 582.464375, mean_tau: 0.075916\n",
            " 12192/50000: episode: 520, duration: 0.153s, episode steps:  20, steps per second: 131, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 1467.472050, mae: 287.517276, mean_q: 602.893307, mean_tau: 0.075881\n",
            " 12213/50000: episode: 521, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1135.345537, mae: 283.750414, mean_q: 597.955097, mean_tau: 0.075840\n",
            " 12239/50000: episode: 522, duration: 0.215s, episode steps:  26, steps per second: 121, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1151.414019, mae: 283.218003, mean_q: 590.569803, mean_tau: 0.075794\n",
            " 12282/50000: episode: 523, duration: 0.324s, episode steps:  43, steps per second: 133, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1099.469095, mae: 291.529962, mean_q: 606.476432, mean_tau: 0.075725\n",
            " 12329/50000: episode: 524, duration: 0.356s, episode steps:  47, steps per second: 132, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 898.314580, mae: 291.609058, mean_q: 608.409582, mean_tau: 0.075636\n",
            " 12347/50000: episode: 525, duration: 0.132s, episode steps:  18, steps per second: 136, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 652.990413, mae: 296.533325, mean_q: 624.302205, mean_tau: 0.075572\n",
            " 12382/50000: episode: 526, duration: 0.283s, episode steps:  35, steps per second: 124, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1093.764910, mae: 291.084940, mean_q: 611.581932, mean_tau: 0.075519\n",
            " 12400/50000: episode: 527, duration: 0.147s, episode steps:  18, steps per second: 123, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1367.449053, mae: 291.785917, mean_q: 605.611528, mean_tau: 0.075467\n",
            " 12411/50000: episode: 528, duration: 0.090s, episode steps:  11, steps per second: 122, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1187.592773, mae: 293.857169, mean_q: 613.868996, mean_tau: 0.075438\n",
            " 12441/50000: episode: 529, duration: 0.238s, episode steps:  30, steps per second: 126, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 988.010099, mae: 288.592356, mean_q: 601.502055, mean_tau: 0.075398\n",
            " 12452/50000: episode: 530, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1664.469274, mae: 296.049248, mean_q: 612.943021, mean_tau: 0.075357\n",
            " 12479/50000: episode: 531, duration: 0.207s, episode steps:  27, steps per second: 130, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1409.267327, mae: 286.811032, mean_q: 601.649846, mean_tau: 0.075319\n",
            " 12497/50000: episode: 532, duration: 0.144s, episode steps:  18, steps per second: 125, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 969.582333, mae: 290.694401, mean_q: 613.492642, mean_tau: 0.075275\n",
            " 12514/50000: episode: 533, duration: 0.140s, episode steps:  17, steps per second: 121, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1058.209904, mae: 300.012171, mean_q: 625.405273, mean_tau: 0.075240\n",
            " 12530/50000: episode: 534, duration: 0.122s, episode steps:  16, steps per second: 131, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1002.170847, mae: 298.555214, mean_q: 626.883636, mean_tau: 0.075207\n",
            " 12575/50000: episode: 535, duration: 0.334s, episode steps:  45, steps per second: 135, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1177.745497, mae: 299.891765, mean_q: 619.005386, mean_tau: 0.075147\n",
            " 12626/50000: episode: 536, duration: 0.383s, episode steps:  51, steps per second: 133, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1058.120060, mae: 298.542592, mean_q: 624.775531, mean_tau: 0.075052\n",
            " 12647/50000: episode: 537, duration: 0.165s, episode steps:  21, steps per second: 127, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1195.565509, mae: 300.865253, mean_q: 634.223092, mean_tau: 0.074981\n",
            " 12673/50000: episode: 538, duration: 0.221s, episode steps:  26, steps per second: 118, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 949.662341, mae: 295.703032, mean_q: 625.738779, mean_tau: 0.074934\n",
            " 12700/50000: episode: 539, duration: 0.217s, episode steps:  27, steps per second: 125, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1684.357087, mae: 307.182266, mean_q: 638.761990, mean_tau: 0.074882\n",
            " 12713/50000: episode: 540, duration: 0.098s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 953.511226, mae: 287.185399, mean_q: 601.785170, mean_tau: 0.074842\n",
            " 12749/50000: episode: 541, duration: 0.309s, episode steps:  36, steps per second: 117, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1251.564455, mae: 299.703364, mean_q: 631.714862, mean_tau: 0.074794\n",
            " 12769/50000: episode: 542, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1005.853716, mae: 300.893605, mean_q: 631.072781, mean_tau: 0.074738\n",
            " 12790/50000: episode: 543, duration: 0.163s, episode steps:  21, steps per second: 129, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1342.675924, mae: 307.900459, mean_q: 643.002505, mean_tau: 0.074698\n",
            " 12802/50000: episode: 544, duration: 0.093s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 879.262952, mae: 305.545560, mean_q: 637.669398, mean_tau: 0.074665\n",
            " 12835/50000: episode: 545, duration: 0.242s, episode steps:  33, steps per second: 136, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 1246.915372, mae: 310.508691, mean_q: 643.483931, mean_tau: 0.074620\n",
            " 12847/50000: episode: 546, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1590.323535, mae: 303.080901, mean_q: 641.272207, mean_tau: 0.074576\n",
            " 12860/50000: episode: 547, duration: 0.104s, episode steps:  13, steps per second: 126, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1113.343682, mae: 307.926197, mean_q: 646.338294, mean_tau: 0.074551\n",
            " 12877/50000: episode: 548, duration: 0.152s, episode steps:  17, steps per second: 112, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1467.432495, mae: 313.721495, mean_q: 651.476124, mean_tau: 0.074521\n",
            " 12893/50000: episode: 549, duration: 0.123s, episode steps:  16, steps per second: 130, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1178.205030, mae: 304.810183, mean_q: 637.963402, mean_tau: 0.074489\n",
            " 12910/50000: episode: 550, duration: 0.145s, episode steps:  17, steps per second: 117, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1376.069501, mae: 307.810046, mean_q: 640.145310, mean_tau: 0.074456\n",
            " 12922/50000: episode: 551, duration: 0.093s, episode steps:  12, steps per second: 129, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1642.469666, mae: 298.399684, mean_q: 630.752406, mean_tau: 0.074427\n",
            " 12943/50000: episode: 552, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1621.847124, mae: 304.558133, mean_q: 634.333240, mean_tau: 0.074395\n",
            " 12960/50000: episode: 553, duration: 0.124s, episode steps:  17, steps per second: 137, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1908.309490, mae: 301.841295, mean_q: 631.562091, mean_tau: 0.074357\n",
            " 12992/50000: episode: 554, duration: 0.249s, episode steps:  32, steps per second: 129, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1408.282014, mae: 304.604240, mean_q: 631.462521, mean_tau: 0.074309\n",
            " 13012/50000: episode: 555, duration: 0.174s, episode steps:  20, steps per second: 115, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1148.798775, mae: 301.984148, mean_q: 631.608606, mean_tau: 0.074257\n",
            " 13051/50000: episode: 556, duration: 0.294s, episode steps:  39, steps per second: 133, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1010.154050, mae: 312.899297, mean_q: 652.536202, mean_tau: 0.074199\n",
            " 13110/50000: episode: 557, duration: 0.438s, episode steps:  59, steps per second: 135, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1235.366241, mae: 315.859502, mean_q: 661.555903, mean_tau: 0.074102\n",
            " 13134/50000: episode: 558, duration: 0.187s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1545.100327, mae: 314.035774, mean_q: 653.697375, mean_tau: 0.074019\n",
            " 13148/50000: episode: 559, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1536.999333, mae: 303.925810, mean_q: 639.089648, mean_tau: 0.073982\n",
            " 13161/50000: episode: 560, duration: 0.102s, episode steps:  13, steps per second: 128, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1272.095431, mae: 315.880749, mean_q: 667.204839, mean_tau: 0.073955\n",
            " 13215/50000: episode: 561, duration: 0.410s, episode steps:  54, steps per second: 132, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1106.714268, mae: 308.278481, mean_q: 646.028983, mean_tau: 0.073889\n",
            " 13238/50000: episode: 562, duration: 0.197s, episode steps:  23, steps per second: 117, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1321.940328, mae: 316.283770, mean_q: 656.541942, mean_tau: 0.073813\n",
            " 13301/50000: episode: 563, duration: 0.723s, episode steps:  63, steps per second:  87, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1502.257136, mae: 313.434152, mean_q: 653.538635, mean_tau: 0.073727\n",
            " 13316/50000: episode: 564, duration: 0.169s, episode steps:  15, steps per second:  89, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1159.639273, mae: 309.283826, mean_q: 651.248934, mean_tau: 0.073650\n",
            " 13336/50000: episode: 565, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1201.751465, mae: 313.685176, mean_q: 659.396094, mean_tau: 0.073616\n",
            " 13355/50000: episode: 566, duration: 0.232s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1610.456341, mae: 315.181706, mean_q: 659.790293, mean_tau: 0.073577\n",
            " 13369/50000: episode: 567, duration: 0.155s, episode steps:  14, steps per second:  90, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1632.519660, mae: 315.720891, mean_q: 664.646367, mean_tau: 0.073544\n",
            " 13387/50000: episode: 568, duration: 0.217s, episode steps:  18, steps per second:  83, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1107.749551, mae: 315.691928, mean_q: 666.467319, mean_tau: 0.073513\n",
            " 13493/50000: episode: 569, duration: 1.107s, episode steps: 106, steps per second:  96, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1552.424272, mae: 320.189113, mean_q: 671.812113, mean_tau: 0.073390\n",
            " 13526/50000: episode: 570, duration: 0.249s, episode steps:  33, steps per second: 132, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1507.025224, mae: 317.328976, mean_q: 663.969834, mean_tau: 0.073252\n",
            " 13573/50000: episode: 571, duration: 0.361s, episode steps:  47, steps per second: 130, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1340.027326, mae: 318.387232, mean_q: 672.264859, mean_tau: 0.073173\n",
            " 13604/50000: episode: 572, duration: 0.247s, episode steps:  31, steps per second: 125, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1343.114420, mae: 322.526360, mean_q: 674.726931, mean_tau: 0.073096\n",
            " 13621/50000: episode: 573, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1426.717630, mae: 310.783358, mean_q: 646.273513, mean_tau: 0.073048\n",
            " 13634/50000: episode: 574, duration: 0.107s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1436.164297, mae: 309.770001, mean_q: 659.642888, mean_tau: 0.073019\n",
            " 13651/50000: episode: 575, duration: 0.138s, episode steps:  17, steps per second: 123, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1421.567417, mae: 323.567316, mean_q: 676.807136, mean_tau: 0.072989\n",
            " 13677/50000: episode: 576, duration: 0.201s, episode steps:  26, steps per second: 129, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1160.071878, mae: 322.517354, mean_q: 672.783304, mean_tau: 0.072946\n",
            " 13701/50000: episode: 577, duration: 0.209s, episode steps:  24, steps per second: 115, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1252.046570, mae: 321.605442, mean_q: 669.301234, mean_tau: 0.072897\n",
            " 13716/50000: episode: 578, duration: 0.120s, episode steps:  15, steps per second: 125, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 933.390108, mae: 323.053434, mean_q: 685.036007, mean_tau: 0.072858\n",
            " 13731/50000: episode: 579, duration: 0.130s, episode steps:  15, steps per second: 116, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 890.244889, mae: 332.592004, mean_q: 700.087240, mean_tau: 0.072828\n",
            " 13747/50000: episode: 580, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1831.783920, mae: 329.915491, mean_q: 685.179256, mean_tau: 0.072798\n",
            " 13768/50000: episode: 581, duration: 0.169s, episode steps:  21, steps per second: 124, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1268.245370, mae: 327.601044, mean_q: 681.584493, mean_tau: 0.072761\n",
            " 13798/50000: episode: 582, duration: 0.225s, episode steps:  30, steps per second: 133, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1402.670845, mae: 321.687698, mean_q: 672.369442, mean_tau: 0.072711\n",
            " 13822/50000: episode: 583, duration: 0.188s, episode steps:  24, steps per second: 127, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1075.869960, mae: 333.540248, mean_q: 696.612427, mean_tau: 0.072657\n",
            " 13841/50000: episode: 584, duration: 0.155s, episode steps:  19, steps per second: 123, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1921.177380, mae: 331.225305, mean_q: 685.808838, mean_tau: 0.072615\n",
            " 13856/50000: episode: 585, duration: 0.114s, episode steps:  15, steps per second: 131, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1587.878477, mae: 325.149182, mean_q: 686.396895, mean_tau: 0.072581\n",
            " 13880/50000: episode: 586, duration: 0.180s, episode steps:  24, steps per second: 133, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1650.998384, mae: 330.718953, mean_q: 691.951469, mean_tau: 0.072542\n",
            " 13893/50000: episode: 587, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2008.875315, mae: 340.144130, mean_q: 711.479267, mean_tau: 0.072506\n",
            " 13909/50000: episode: 588, duration: 0.132s, episode steps:  16, steps per second: 121, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1414.982046, mae: 326.053120, mean_q: 684.298786, mean_tau: 0.072477\n",
            " 13941/50000: episode: 589, duration: 0.238s, episode steps:  32, steps per second: 135, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1372.601563, mae: 329.060499, mean_q: 685.714476, mean_tau: 0.072429\n",
            " 13965/50000: episode: 590, duration: 0.189s, episode steps:  24, steps per second: 127, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1422.991441, mae: 331.758965, mean_q: 683.056913, mean_tau: 0.072374\n",
            " 13982/50000: episode: 591, duration: 0.143s, episode steps:  17, steps per second: 119, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1795.531947, mae: 327.397488, mean_q: 682.139742, mean_tau: 0.072333\n",
            " 13995/50000: episode: 592, duration: 0.103s, episode steps:  13, steps per second: 126, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1946.670699, mae: 335.854847, mean_q: 693.688528, mean_tau: 0.072304\n",
            " 14021/50000: episode: 593, duration: 0.201s, episode steps:  26, steps per second: 129, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1709.113894, mae: 339.476646, mean_q: 706.636601, mean_tau: 0.072265\n",
            " 14032/50000: episode: 594, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1224.092807, mae: 345.205378, mean_q: 725.612427, mean_tau: 0.072229\n",
            " 14043/50000: episode: 595, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1433.723206, mae: 319.490870, mean_q: 682.985074, mean_tau: 0.072207\n",
            " 14076/50000: episode: 596, duration: 0.260s, episode steps:  33, steps per second: 127, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1666.629629, mae: 330.174973, mean_q: 693.671879, mean_tau: 0.072163\n",
            " 14100/50000: episode: 597, duration: 0.179s, episode steps:  24, steps per second: 134, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1510.664074, mae: 330.629278, mean_q: 697.313950, mean_tau: 0.072107\n",
            " 14113/50000: episode: 598, duration: 0.119s, episode steps:  13, steps per second: 110, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1573.439322, mae: 335.405219, mean_q: 696.583050, mean_tau: 0.072070\n",
            " 14123/50000: episode: 599, duration: 0.081s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 751.307217, mae: 340.445392, mean_q: 721.537982, mean_tau: 0.072047\n",
            " 14134/50000: episode: 600, duration: 0.090s, episode steps:  11, steps per second: 123, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1195.441007, mae: 329.213401, mean_q: 687.948309, mean_tau: 0.072027\n",
            " 14145/50000: episode: 601, duration: 0.105s, episode steps:  11, steps per second: 104, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1423.660645, mae: 329.974021, mean_q: 699.282404, mean_tau: 0.072005\n",
            " 14156/50000: episode: 602, duration: 0.094s, episode steps:  11, steps per second: 118, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1546.894748, mae: 343.990900, mean_q: 712.792569, mean_tau: 0.071983\n",
            " 14185/50000: episode: 603, duration: 0.234s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1457.915116, mae: 332.118560, mean_q: 689.728665, mean_tau: 0.071943\n",
            " 14212/50000: episode: 604, duration: 0.212s, episode steps:  27, steps per second: 127, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1363.768856, mae: 337.814989, mean_q: 697.783063, mean_tau: 0.071888\n",
            " 14244/50000: episode: 605, duration: 0.249s, episode steps:  32, steps per second: 129, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1406.781462, mae: 342.755978, mean_q: 709.885641, mean_tau: 0.071830\n",
            " 14261/50000: episode: 606, duration: 0.126s, episode steps:  17, steps per second: 135, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1161.271902, mae: 333.771685, mean_q: 696.598478, mean_tau: 0.071781\n",
            " 14273/50000: episode: 607, duration: 0.098s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1103.472570, mae: 331.852216, mean_q: 701.802109, mean_tau: 0.071752\n",
            " 14308/50000: episode: 608, duration: 0.270s, episode steps:  35, steps per second: 129, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 1692.687512, mae: 339.036065, mean_q: 697.304545, mean_tau: 0.071706\n",
            " 14326/50000: episode: 609, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1738.363244, mae: 338.946730, mean_q: 700.052307, mean_tau: 0.071653\n",
            " 14341/50000: episode: 610, duration: 0.130s, episode steps:  15, steps per second: 115, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1629.818734, mae: 345.237236, mean_q: 728.534945, mean_tau: 0.071621\n",
            " 14398/50000: episode: 611, duration: 0.443s, episode steps:  57, steps per second: 129, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1193.006071, mae: 339.690607, mean_q: 714.365340, mean_tau: 0.071549\n",
            " 14434/50000: episode: 612, duration: 0.271s, episode steps:  36, steps per second: 133, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1632.551033, mae: 335.341733, mean_q: 703.545402, mean_tau: 0.071457\n",
            " 14451/50000: episode: 613, duration: 0.145s, episode steps:  17, steps per second: 117, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1488.486463, mae: 356.709916, mean_q: 746.568884, mean_tau: 0.071405\n",
            " 14471/50000: episode: 614, duration: 0.173s, episode steps:  20, steps per second: 115, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1527.458287, mae: 333.683727, mean_q: 699.264447, mean_tau: 0.071368\n",
            " 14510/50000: episode: 615, duration: 0.295s, episode steps:  39, steps per second: 132, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1320.920012, mae: 345.385514, mean_q: 716.253753, mean_tau: 0.071310\n",
            " 14554/50000: episode: 616, duration: 0.328s, episode steps:  44, steps per second: 134, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1412.210660, mae: 353.488948, mean_q: 737.503325, mean_tau: 0.071228\n",
            " 14579/50000: episode: 617, duration: 0.183s, episode steps:  25, steps per second: 137, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1349.792802, mae: 344.485719, mean_q: 718.698945, mean_tau: 0.071159\n",
            " 14593/50000: episode: 618, duration: 0.122s, episode steps:  14, steps per second: 115, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1054.389354, mae: 349.978210, mean_q: 729.677765, mean_tau: 0.071121\n",
            " 14621/50000: episode: 619, duration: 0.201s, episode steps:  28, steps per second: 139, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1232.406435, mae: 362.649984, mean_q: 761.824005, mean_tau: 0.071079\n",
            " 14634/50000: episode: 620, duration: 0.114s, episode steps:  13, steps per second: 114, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1270.894957, mae: 339.960534, mean_q: 729.044626, mean_tau: 0.071039\n",
            " 14749/50000: episode: 621, duration: 0.876s, episode steps: 115, steps per second: 131, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1617.045397, mae: 348.804100, mean_q: 731.641895, mean_tau: 0.070912\n",
            " 14771/50000: episode: 622, duration: 0.272s, episode steps:  22, steps per second:  81, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1932.325535, mae: 343.680208, mean_q: 721.863040, mean_tau: 0.070776\n",
            " 14787/50000: episode: 623, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1517.972307, mae: 347.310415, mean_q: 727.928703, mean_tau: 0.070739\n",
            " 14808/50000: episode: 624, duration: 0.254s, episode steps:  21, steps per second:  83, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2328.055186, mae: 341.455440, mean_q: 715.237732, mean_tau: 0.070702\n",
            " 14825/50000: episode: 625, duration: 0.224s, episode steps:  17, steps per second:  76, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2122.044541, mae: 352.321481, mean_q: 730.778744, mean_tau: 0.070664\n",
            " 14856/50000: episode: 626, duration: 0.382s, episode steps:  31, steps per second:  81, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1392.312376, mae: 357.111535, mean_q: 744.299765, mean_tau: 0.070617\n",
            " 14871/50000: episode: 627, duration: 0.172s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1688.455886, mae: 349.292224, mean_q: 736.631632, mean_tau: 0.070571\n",
            " 14883/50000: episode: 628, duration: 0.157s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 854.726157, mae: 355.151031, mean_q: 750.806244, mean_tau: 0.070545\n",
            " 14916/50000: episode: 629, duration: 0.380s, episode steps:  33, steps per second:  87, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1357.869707, mae: 357.520975, mean_q: 745.266385, mean_tau: 0.070500\n",
            " 14933/50000: episode: 630, duration: 0.199s, episode steps:  17, steps per second:  85, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2398.391418, mae: 353.371650, mean_q: 738.107192, mean_tau: 0.070450\n",
            " 15008/50000: episode: 631, duration: 0.765s, episode steps:  75, steps per second:  98, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 1630.944577, mae: 356.540896, mean_q: 748.108608, mean_tau: 0.070359\n",
            " 15019/50000: episode: 632, duration: 0.087s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1188.101374, mae: 348.152580, mean_q: 739.152799, mean_tau: 0.070274\n",
            " 15038/50000: episode: 633, duration: 0.159s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1595.316721, mae: 361.710663, mean_q: 756.837669, mean_tau: 0.070245\n",
            " 15064/50000: episode: 634, duration: 0.211s, episode steps:  26, steps per second: 123, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1573.746297, mae: 356.798498, mean_q: 744.546314, mean_tau: 0.070200\n",
            " 15084/50000: episode: 635, duration: 0.168s, episode steps:  20, steps per second: 119, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1615.897313, mae: 357.307185, mean_q: 751.774716, mean_tau: 0.070154\n",
            " 15112/50000: episode: 636, duration: 0.228s, episode steps:  28, steps per second: 123, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1754.397879, mae: 362.247643, mean_q: 756.573521, mean_tau: 0.070107\n",
            " 15128/50000: episode: 637, duration: 0.147s, episode steps:  16, steps per second: 109, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2200.367901, mae: 362.271414, mean_q: 761.797031, mean_tau: 0.070063\n",
            " 15148/50000: episode: 638, duration: 0.158s, episode steps:  20, steps per second: 126, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1450.711127, mae: 358.594891, mean_q: 748.532855, mean_tau: 0.070028\n",
            " 15179/50000: episode: 639, duration: 0.245s, episode steps:  31, steps per second: 127, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1805.761143, mae: 360.728263, mean_q: 755.957490, mean_tau: 0.069977\n",
            " 15196/50000: episode: 640, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1493.449224, mae: 355.055795, mean_q: 746.359310, mean_tau: 0.069930\n",
            " 15210/50000: episode: 641, duration: 0.124s, episode steps:  14, steps per second: 113, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2139.376404, mae: 359.978093, mean_q: 751.561109, mean_tau: 0.069899\n",
            " 15234/50000: episode: 642, duration: 0.178s, episode steps:  24, steps per second: 134, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1794.451285, mae: 357.254381, mean_q: 745.928098, mean_tau: 0.069861\n",
            " 15250/50000: episode: 643, duration: 0.144s, episode steps:  16, steps per second: 111, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1962.492163, mae: 365.007132, mean_q: 762.502373, mean_tau: 0.069822\n",
            " 15264/50000: episode: 644, duration: 0.106s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1922.502605, mae: 354.723519, mean_q: 750.644213, mean_tau: 0.069792\n",
            " 15293/50000: episode: 645, duration: 0.231s, episode steps:  29, steps per second: 125, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1983.834670, mae: 367.565677, mean_q: 769.454363, mean_tau: 0.069750\n",
            " 15332/50000: episode: 646, duration: 0.302s, episode steps:  39, steps per second: 129, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2049.482625, mae: 371.374227, mean_q: 772.891581, mean_tau: 0.069682\n",
            " 15343/50000: episode: 647, duration: 0.088s, episode steps:  11, steps per second: 125, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1925.401084, mae: 364.865689, mean_q: 774.152904, mean_tau: 0.069633\n",
            " 15365/50000: episode: 648, duration: 0.177s, episode steps:  22, steps per second: 124, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2178.071067, mae: 364.818674, mean_q: 765.489752, mean_tau: 0.069600\n",
            " 15398/50000: episode: 649, duration: 0.252s, episode steps:  33, steps per second: 131, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1782.530989, mae: 366.465412, mean_q: 769.122424, mean_tau: 0.069546\n",
            " 15437/50000: episode: 650, duration: 0.300s, episode steps:  39, steps per second: 130, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1860.314496, mae: 364.347216, mean_q: 754.501329, mean_tau: 0.069474\n",
            " 15481/50000: episode: 651, duration: 0.343s, episode steps:  44, steps per second: 128, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1540.972287, mae: 370.009291, mean_q: 773.010071, mean_tau: 0.069392\n",
            " 15502/50000: episode: 652, duration: 0.160s, episode steps:  21, steps per second: 131, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1527.067123, mae: 373.536455, mean_q: 790.855855, mean_tau: 0.069328\n",
            " 15541/50000: episode: 653, duration: 0.318s, episode steps:  39, steps per second: 122, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1853.103101, mae: 370.210690, mean_q: 770.276452, mean_tau: 0.069268\n",
            " 15574/50000: episode: 654, duration: 0.272s, episode steps:  33, steps per second: 121, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1943.647879, mae: 366.663050, mean_q: 770.758478, mean_tau: 0.069197\n",
            " 15609/50000: episode: 655, duration: 0.287s, episode steps:  35, steps per second: 122, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1756.571113, mae: 365.497254, mean_q: 770.374792, mean_tau: 0.069130\n",
            " 15623/50000: episode: 656, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1778.845415, mae: 378.545916, mean_q: 785.131688, mean_tau: 0.069081\n",
            " 15632/50000: episode: 657, duration: 0.070s, episode steps:   9, steps per second: 129, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1749.309455, mae: 371.295397, mean_q: 775.183573, mean_tau: 0.069059\n",
            " 15645/50000: episode: 658, duration: 0.126s, episode steps:  13, steps per second: 103, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2094.541185, mae: 364.457139, mean_q: 765.920523, mean_tau: 0.069037\n",
            " 15658/50000: episode: 659, duration: 0.103s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1889.558397, mae: 380.873643, mean_q: 787.037241, mean_tau: 0.069011\n",
            " 15669/50000: episode: 660, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3332.271964, mae: 387.137806, mean_q: 813.374018, mean_tau: 0.068987\n",
            " 15680/50000: episode: 661, duration: 0.093s, episode steps:  11, steps per second: 118, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1080.285997, mae: 378.519983, mean_q: 793.462985, mean_tau: 0.068965\n",
            " 15730/50000: episode: 662, duration: 0.381s, episode steps:  50, steps per second: 131, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1789.383659, mae: 372.495789, mean_q: 781.387484, mean_tau: 0.068905\n",
            " 15744/50000: episode: 663, duration: 0.115s, episode steps:  14, steps per second: 122, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1584.216102, mae: 384.245161, mean_q: 797.508933, mean_tau: 0.068842\n",
            " 15779/50000: episode: 664, duration: 0.277s, episode steps:  35, steps per second: 126, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 2009.119283, mae: 374.401161, mean_q: 789.598764, mean_tau: 0.068793\n",
            " 15799/50000: episode: 665, duration: 0.157s, episode steps:  20, steps per second: 127, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2334.346652, mae: 381.099982, mean_q: 783.797507, mean_tau: 0.068739\n",
            " 15816/50000: episode: 666, duration: 0.149s, episode steps:  17, steps per second: 114, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1632.178334, mae: 380.861204, mean_q: 794.209150, mean_tau: 0.068702\n",
            " 15833/50000: episode: 667, duration: 0.147s, episode steps:  17, steps per second: 116, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1711.903439, mae: 379.368557, mean_q: 801.173961, mean_tau: 0.068668\n",
            " 15843/50000: episode: 668, duration: 0.102s, episode steps:  10, steps per second:  98, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1290.182544, mae: 378.544431, mean_q: 783.020428, mean_tau: 0.068642\n",
            " 15856/50000: episode: 669, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2072.355464, mae: 378.224114, mean_q: 790.318923, mean_tau: 0.068619\n",
            " 15881/50000: episode: 670, duration: 0.199s, episode steps:  25, steps per second: 126, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2110.154404, mae: 388.570754, mean_q: 808.428467, mean_tau: 0.068581\n",
            " 15906/50000: episode: 671, duration: 0.216s, episode steps:  25, steps per second: 116, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 1451.876339, mae: 385.888280, mean_q: 810.929666, mean_tau: 0.068532\n",
            " 15921/50000: episode: 672, duration: 0.128s, episode steps:  15, steps per second: 117, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1823.402019, mae: 385.677600, mean_q: 807.791353, mean_tau: 0.068492\n",
            " 15935/50000: episode: 673, duration: 0.134s, episode steps:  14, steps per second: 104, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1468.867065, mae: 376.639470, mean_q: 793.361860, mean_tau: 0.068464\n",
            " 15960/50000: episode: 674, duration: 0.199s, episode steps:  25, steps per second: 126, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1694.449884, mae: 379.439011, mean_q: 796.811294, mean_tau: 0.068425\n",
            " 15978/50000: episode: 675, duration: 0.148s, episode steps:  18, steps per second: 122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2264.265079, mae: 385.285870, mean_q: 810.738905, mean_tau: 0.068382\n",
            " 15989/50000: episode: 676, duration: 0.088s, episode steps:  11, steps per second: 125, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1858.014393, mae: 390.276384, mean_q: 813.599565, mean_tau: 0.068354\n",
            " 16009/50000: episode: 677, duration: 0.172s, episode steps:  20, steps per second: 116, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2010.675478, mae: 384.353485, mean_q: 803.285605, mean_tau: 0.068323\n",
            " 16040/50000: episode: 678, duration: 0.242s, episode steps:  31, steps per second: 128, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2642.863690, mae: 394.241887, mean_q: 813.464432, mean_tau: 0.068272\n",
            " 16062/50000: episode: 679, duration: 0.171s, episode steps:  22, steps per second: 128, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1804.673390, mae: 373.936343, mean_q: 782.943063, mean_tau: 0.068220\n",
            " 16097/50000: episode: 680, duration: 0.289s, episode steps:  35, steps per second: 121, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1769.077786, mae: 385.598206, mean_q: 808.853875, mean_tau: 0.068164\n",
            " 16121/50000: episode: 681, duration: 0.219s, episode steps:  24, steps per second: 110, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1867.085037, mae: 405.802598, mean_q: 845.099528, mean_tau: 0.068105\n",
            " 16147/50000: episode: 682, duration: 0.245s, episode steps:  26, steps per second: 106, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2064.696915, mae: 388.566547, mean_q: 807.403372, mean_tau: 0.068056\n",
            " 16159/50000: episode: 683, duration: 0.099s, episode steps:  12, steps per second: 121, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1940.038615, mae: 391.954536, mean_q: 832.801707, mean_tau: 0.068018\n",
            " 16176/50000: episode: 684, duration: 0.149s, episode steps:  17, steps per second: 114, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2374.105851, mae: 390.287693, mean_q: 817.531279, mean_tau: 0.067989\n",
            " 16190/50000: episode: 685, duration: 0.117s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2189.917110, mae: 385.251554, mean_q: 793.987405, mean_tau: 0.067959\n",
            " 16205/50000: episode: 686, duration: 0.169s, episode steps:  15, steps per second:  89, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1513.573771, mae: 392.098201, mean_q: 820.867627, mean_tau: 0.067930\n",
            " 16224/50000: episode: 687, duration: 0.267s, episode steps:  19, steps per second:  71, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1956.769880, mae: 398.318228, mean_q: 832.547501, mean_tau: 0.067896\n",
            " 16265/50000: episode: 688, duration: 0.477s, episode steps:  41, steps per second:  86, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1801.497674, mae: 387.699276, mean_q: 816.831290, mean_tau: 0.067837\n",
            " 16282/50000: episode: 689, duration: 0.210s, episode steps:  17, steps per second:  81, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1807.863123, mae: 392.411190, mean_q: 826.707721, mean_tau: 0.067779\n",
            " 16297/50000: episode: 690, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1374.923228, mae: 400.345852, mean_q: 838.172286, mean_tau: 0.067748\n",
            " 16307/50000: episode: 691, duration: 0.131s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2131.790497, mae: 382.965323, mean_q: 796.683429, mean_tau: 0.067723\n",
            " 16325/50000: episode: 692, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1607.849489, mae: 391.771100, mean_q: 823.479760, mean_tau: 0.067695\n",
            " 16337/50000: episode: 693, duration: 0.144s, episode steps:  12, steps per second:  83, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3609.607391, mae: 389.259015, mean_q: 810.089834, mean_tau: 0.067666\n",
            " 16355/50000: episode: 694, duration: 0.218s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2580.387770, mae: 386.876934, mean_q: 806.662530, mean_tau: 0.067636\n",
            " 16378/50000: episode: 695, duration: 0.286s, episode steps:  23, steps per second:  80, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1849.637260, mae: 389.634234, mean_q: 813.703515, mean_tau: 0.067595\n",
            " 16392/50000: episode: 696, duration: 0.175s, episode steps:  14, steps per second:  80, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3051.617205, mae: 388.795146, mean_q: 815.536508, mean_tau: 0.067559\n",
            " 16424/50000: episode: 697, duration: 0.402s, episode steps:  32, steps per second:  80, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1832.864873, mae: 396.292896, mean_q: 828.785927, mean_tau: 0.067513\n",
            " 16448/50000: episode: 698, duration: 0.216s, episode steps:  24, steps per second: 111, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2764.741765, mae: 388.464245, mean_q: 814.479444, mean_tau: 0.067458\n",
            " 16473/50000: episode: 699, duration: 0.207s, episode steps:  25, steps per second: 121, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2237.715652, mae: 397.456278, mean_q: 825.624246, mean_tau: 0.067409\n",
            " 16493/50000: episode: 700, duration: 0.153s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 1825.993959, mae: 388.670221, mean_q: 816.970981, mean_tau: 0.067365\n",
            " 16511/50000: episode: 701, duration: 0.169s, episode steps:  18, steps per second: 107, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2292.973982, mae: 389.647827, mean_q: 824.468201, mean_tau: 0.067327\n",
            " 16543/50000: episode: 702, duration: 0.263s, episode steps:  32, steps per second: 122, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2155.202054, mae: 397.624555, mean_q: 825.895651, mean_tau: 0.067278\n",
            " 16556/50000: episode: 703, duration: 0.110s, episode steps:  13, steps per second: 118, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1798.505871, mae: 398.583907, mean_q: 834.571975, mean_tau: 0.067233\n",
            " 16579/50000: episode: 704, duration: 0.186s, episode steps:  23, steps per second: 124, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2755.484428, mae: 388.366379, mean_q: 806.824290, mean_tau: 0.067197\n",
            " 16617/50000: episode: 705, duration: 0.296s, episode steps:  38, steps per second: 128, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2154.338924, mae: 394.586613, mean_q: 833.106296, mean_tau: 0.067137\n",
            " 16633/50000: episode: 706, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2821.493458, mae: 390.286240, mean_q: 812.720818, mean_tau: 0.067083\n",
            " 16648/50000: episode: 707, duration: 0.129s, episode steps:  15, steps per second: 116, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2094.496366, mae: 419.069865, mean_q: 859.316789, mean_tau: 0.067053\n",
            " 16662/50000: episode: 708, duration: 0.115s, episode steps:  14, steps per second: 122, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1997.371423, mae: 404.993685, mean_q: 845.432757, mean_tau: 0.067024\n",
            " 16685/50000: episode: 709, duration: 0.184s, episode steps:  23, steps per second: 125, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1769.331485, mae: 398.690217, mean_q: 829.316560, mean_tau: 0.066987\n",
            " 16711/50000: episode: 710, duration: 0.225s, episode steps:  26, steps per second: 115, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 1585.973688, mae: 407.874173, mean_q: 852.215938, mean_tau: 0.066939\n",
            " 16769/50000: episode: 711, duration: 0.471s, episode steps:  58, steps per second: 123, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2453.538271, mae: 401.587265, mean_q: 834.151044, mean_tau: 0.066856\n",
            " 16798/50000: episode: 712, duration: 0.232s, episode steps:  29, steps per second: 125, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2025.333633, mae: 406.188389, mean_q: 849.312336, mean_tau: 0.066770\n",
            " 16817/50000: episode: 713, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1855.978987, mae: 407.262798, mean_q: 856.243052, mean_tau: 0.066722\n",
            " 16852/50000: episode: 714, duration: 0.285s, episode steps:  35, steps per second: 123, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1845.949152, mae: 407.369627, mean_q: 845.219589, mean_tau: 0.066669\n",
            " 16869/50000: episode: 715, duration: 0.144s, episode steps:  17, steps per second: 118, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3305.886629, mae: 395.926801, mean_q: 833.211595, mean_tau: 0.066617\n",
            " 16923/50000: episode: 716, duration: 0.438s, episode steps:  54, steps per second: 123, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2384.361535, mae: 404.372132, mean_q: 846.007039, mean_tau: 0.066547\n",
            " 16973/50000: episode: 717, duration: 0.435s, episode steps:  50, steps per second: 115, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1976.678088, mae: 414.418678, mean_q: 862.550801, mean_tau: 0.066444\n",
            " 16995/50000: episode: 718, duration: 0.193s, episode steps:  22, steps per second: 114, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1418.998223, mae: 415.628522, mean_q: 882.359325, mean_tau: 0.066373\n",
            " 17028/50000: episode: 719, duration: 0.282s, episode steps:  33, steps per second: 117, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2850.688096, mae: 406.023732, mean_q: 849.149048, mean_tau: 0.066318\n",
            " 17044/50000: episode: 720, duration: 0.132s, episode steps:  16, steps per second: 121, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2813.456432, mae: 417.239359, mean_q: 881.737782, mean_tau: 0.066270\n",
            " 17059/50000: episode: 721, duration: 0.143s, episode steps:  15, steps per second: 105, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3136.307147, mae: 410.504240, mean_q: 843.024491, mean_tau: 0.066239\n",
            " 17087/50000: episode: 722, duration: 0.241s, episode steps:  28, steps per second: 116, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2063.290977, mae: 413.167551, mean_q: 862.412733, mean_tau: 0.066196\n",
            " 17119/50000: episode: 723, duration: 0.271s, episode steps:  32, steps per second: 118, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2302.072404, mae: 428.806295, mean_q: 888.249607, mean_tau: 0.066137\n",
            " 17134/50000: episode: 724, duration: 0.152s, episode steps:  15, steps per second:  99, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2131.389119, mae: 414.666132, mean_q: 853.972563, mean_tau: 0.066091\n",
            " 17194/50000: episode: 725, duration: 0.522s, episode steps:  60, steps per second: 115, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2320.134821, mae: 414.499481, mean_q: 863.236979, mean_tau: 0.066016\n",
            " 17224/50000: episode: 726, duration: 0.238s, episode steps:  30, steps per second: 126, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1952.239364, mae: 422.303841, mean_q: 874.743355, mean_tau: 0.065927\n",
            " 17256/50000: episode: 727, duration: 0.283s, episode steps:  32, steps per second: 113, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 2571.772844, mae: 411.217926, mean_q: 858.242029, mean_tau: 0.065866\n",
            " 17269/50000: episode: 728, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2599.371331, mae: 405.473900, mean_q: 847.640752, mean_tau: 0.065821\n",
            " 17294/50000: episode: 729, duration: 0.199s, episode steps:  25, steps per second: 126, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2413.202781, mae: 415.247214, mean_q: 867.246106, mean_tau: 0.065784\n",
            " 17349/50000: episode: 730, duration: 0.433s, episode steps:  55, steps per second: 127, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2030.056358, mae: 420.863873, mean_q: 879.937882, mean_tau: 0.065704\n",
            " 17362/50000: episode: 731, duration: 0.107s, episode steps:  13, steps per second: 121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1727.083081, mae: 420.222076, mean_q: 879.478624, mean_tau: 0.065637\n",
            " 17412/50000: episode: 732, duration: 0.401s, episode steps:  50, steps per second: 125, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 2557.107602, mae: 424.296265, mean_q: 890.380975, mean_tau: 0.065575\n",
            " 17439/50000: episode: 733, duration: 0.222s, episode steps:  27, steps per second: 122, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1871.864916, mae: 434.477896, mean_q: 908.962561, mean_tau: 0.065499\n",
            " 17468/50000: episode: 734, duration: 0.249s, episode steps:  29, steps per second: 117, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2028.866499, mae: 422.630302, mean_q: 885.544442, mean_tau: 0.065443\n",
            " 17487/50000: episode: 735, duration: 0.167s, episode steps:  19, steps per second: 114, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2102.742038, mae: 428.762308, mean_q: 888.971882, mean_tau: 0.065396\n",
            " 17513/50000: episode: 736, duration: 0.222s, episode steps:  26, steps per second: 117, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2784.435734, mae: 423.119075, mean_q: 886.440927, mean_tau: 0.065351\n",
            " 17536/50000: episode: 737, duration: 0.181s, episode steps:  23, steps per second: 127, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2829.055367, mae: 421.832058, mean_q: 885.764295, mean_tau: 0.065302\n",
            " 17563/50000: episode: 738, duration: 0.220s, episode steps:  27, steps per second: 123, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3070.729558, mae: 433.443674, mean_q: 901.464611, mean_tau: 0.065253\n",
            " 17592/50000: episode: 739, duration: 0.255s, episode steps:  29, steps per second: 114, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2391.819449, mae: 432.283609, mean_q: 890.261885, mean_tau: 0.065198\n",
            " 17637/50000: episode: 740, duration: 0.458s, episode steps:  45, steps per second:  98, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 2039.314863, mae: 425.807514, mean_q: 893.361285, mean_tau: 0.065124\n",
            " 17684/50000: episode: 741, duration: 0.557s, episode steps:  47, steps per second:  84, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2038.086104, mae: 434.428634, mean_q: 904.671171, mean_tau: 0.065033\n",
            " 17709/50000: episode: 742, duration: 0.312s, episode steps:  25, steps per second:  80, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2121.511453, mae: 419.626307, mean_q: 880.717031, mean_tau: 0.064962\n",
            " 17721/50000: episode: 743, duration: 0.162s, episode steps:  12, steps per second:  74, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2623.821777, mae: 431.756605, mean_q: 899.255839, mean_tau: 0.064925\n",
            " 17742/50000: episode: 744, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2552.295066, mae: 422.242294, mean_q: 892.969372, mean_tau: 0.064893\n",
            " 17755/50000: episode: 745, duration: 0.154s, episode steps:  13, steps per second:  84, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2342.398555, mae: 430.956517, mean_q: 890.111859, mean_tau: 0.064859\n",
            " 17775/50000: episode: 746, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2354.948392, mae: 435.626128, mean_q: 901.714844, mean_tau: 0.064826\n",
            " 17846/50000: episode: 747, duration: 0.814s, episode steps:  71, steps per second:  87, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 2225.921602, mae: 434.197661, mean_q: 907.412874, mean_tau: 0.064736\n",
            " 17861/50000: episode: 748, duration: 0.121s, episode steps:  15, steps per second: 124, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1790.265194, mae: 437.358936, mean_q: 922.824154, mean_tau: 0.064651\n",
            " 17893/50000: episode: 749, duration: 0.261s, episode steps:  32, steps per second: 123, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3071.831379, mae: 443.932948, mean_q: 905.021936, mean_tau: 0.064605\n",
            " 17913/50000: episode: 750, duration: 0.163s, episode steps:  20, steps per second: 123, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2487.401746, mae: 421.629274, mean_q: 875.597272, mean_tau: 0.064553\n",
            " 17946/50000: episode: 751, duration: 0.256s, episode steps:  33, steps per second: 129, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2195.159892, mae: 435.523308, mean_q: 914.393669, mean_tau: 0.064501\n",
            " 17962/50000: episode: 752, duration: 0.134s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2438.998594, mae: 449.638552, mean_q: 934.908573, mean_tau: 0.064452\n",
            " 17975/50000: episode: 753, duration: 0.116s, episode steps:  13, steps per second: 112, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3007.654433, mae: 448.906975, mean_q: 930.432594, mean_tau: 0.064423\n",
            " 17994/50000: episode: 754, duration: 0.141s, episode steps:  19, steps per second: 135, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1594.185969, mae: 439.337560, mean_q: 911.361058, mean_tau: 0.064392\n",
            " 18013/50000: episode: 755, duration: 0.170s, episode steps:  19, steps per second: 112, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2931.648836, mae: 425.066984, mean_q: 895.959103, mean_tau: 0.064354\n",
            " 18044/50000: episode: 756, duration: 0.237s, episode steps:  31, steps per second: 131, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 2068.989633, mae: 436.901575, mean_q: 902.793941, mean_tau: 0.064305\n",
            " 18063/50000: episode: 757, duration: 0.155s, episode steps:  19, steps per second: 122, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1718.341389, mae: 461.928091, mean_q: 963.430230, mean_tau: 0.064255\n",
            " 18123/50000: episode: 758, duration: 0.462s, episode steps:  60, steps per second: 130, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2330.162844, mae: 442.078427, mean_q: 925.035665, mean_tau: 0.064177\n",
            " 18142/50000: episode: 759, duration: 0.156s, episode steps:  19, steps per second: 122, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2554.430417, mae: 438.994781, mean_q: 922.832558, mean_tau: 0.064099\n",
            " 18168/50000: episode: 760, duration: 0.203s, episode steps:  26, steps per second: 128, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2234.269454, mae: 448.411612, mean_q: 933.547497, mean_tau: 0.064054\n",
            " 18181/50000: episode: 761, duration: 0.116s, episode steps:  13, steps per second: 112, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2755.582214, mae: 442.429537, mean_q: 925.828055, mean_tau: 0.064015\n",
            " 18196/50000: episode: 762, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1735.872424, mae: 441.037128, mean_q: 914.405200, mean_tau: 0.063988\n",
            " 18216/50000: episode: 763, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2809.994699, mae: 444.583109, mean_q: 920.340863, mean_tau: 0.063953\n",
            " 18225/50000: episode: 764, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 3416.296705, mae: 444.679179, mean_q: 921.819872, mean_tau: 0.063924\n",
            " 18273/50000: episode: 765, duration: 0.370s, episode steps:  48, steps per second: 130, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2175.998519, mae: 445.449647, mean_q: 925.493912, mean_tau: 0.063868\n",
            " 18297/50000: episode: 766, duration: 0.182s, episode steps:  24, steps per second: 132, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1966.930542, mae: 457.407763, mean_q: 949.809499, mean_tau: 0.063797\n",
            " 18310/50000: episode: 767, duration: 0.107s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1866.468562, mae: 448.439563, mean_q: 952.601821, mean_tau: 0.063760\n",
            " 18321/50000: episode: 768, duration: 0.083s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2640.426908, mae: 427.337078, mean_q: 897.865362, mean_tau: 0.063736\n",
            " 18338/50000: episode: 769, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2873.947395, mae: 443.283742, mean_q: 920.589402, mean_tau: 0.063709\n",
            " 18350/50000: episode: 770, duration: 0.090s, episode steps:  12, steps per second: 133, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3331.616603, mae: 448.688067, mean_q: 931.155538, mean_tau: 0.063680\n",
            " 18376/50000: episode: 771, duration: 0.193s, episode steps:  26, steps per second: 134, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2716.588482, mae: 449.861859, mean_q: 929.618800, mean_tau: 0.063642\n",
            " 18418/50000: episode: 772, duration: 0.326s, episode steps:  42, steps per second: 129, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1853.145341, mae: 459.176719, mean_q: 954.357229, mean_tau: 0.063575\n",
            " 18438/50000: episode: 773, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2502.370990, mae: 463.980679, mean_q: 953.263306, mean_tau: 0.063514\n",
            " 18456/50000: episode: 774, duration: 0.146s, episode steps:  18, steps per second: 123, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1868.849397, mae: 455.079032, mean_q: 955.199558, mean_tau: 0.063476\n",
            " 18497/50000: episode: 775, duration: 0.325s, episode steps:  41, steps per second: 126, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2070.307529, mae: 451.307036, mean_q: 938.924991, mean_tau: 0.063418\n",
            " 18528/50000: episode: 776, duration: 0.250s, episode steps:  31, steps per second: 124, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2326.220008, mae: 466.472111, mean_q: 968.652486, mean_tau: 0.063346\n",
            " 18546/50000: episode: 777, duration: 0.131s, episode steps:  18, steps per second: 138, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2227.111608, mae: 449.926220, mean_q: 939.882701, mean_tau: 0.063298\n",
            " 18572/50000: episode: 778, duration: 0.201s, episode steps:  26, steps per second: 130, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2310.071685, mae: 457.870628, mean_q: 950.133050, mean_tau: 0.063254\n",
            " 18611/50000: episode: 779, duration: 0.288s, episode steps:  39, steps per second: 136, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2336.656967, mae: 455.837931, mean_q: 945.518772, mean_tau: 0.063190\n",
            " 18627/50000: episode: 780, duration: 0.122s, episode steps:  16, steps per second: 131, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2090.418549, mae: 461.640896, mean_q: 977.258270, mean_tau: 0.063135\n",
            " 18645/50000: episode: 781, duration: 0.150s, episode steps:  18, steps per second: 120, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2729.843899, mae: 458.433148, mean_q: 959.053965, mean_tau: 0.063102\n",
            " 18667/50000: episode: 782, duration: 0.183s, episode steps:  22, steps per second: 120, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1409.126848, mae: 459.834929, mean_q: 961.068948, mean_tau: 0.063062\n",
            " 18690/50000: episode: 783, duration: 0.184s, episode steps:  23, steps per second: 125, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2471.537279, mae: 466.759589, mean_q: 983.171700, mean_tau: 0.063018\n",
            " 18708/50000: episode: 784, duration: 0.138s, episode steps:  18, steps per second: 130, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2499.104755, mae: 459.447488, mean_q: 965.844655, mean_tau: 0.062977\n",
            " 18765/50000: episode: 785, duration: 0.432s, episode steps:  57, steps per second: 132, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 2671.752079, mae: 465.972543, mean_q: 969.190774, mean_tau: 0.062903\n",
            " 18786/50000: episode: 786, duration: 0.164s, episode steps:  21, steps per second: 128, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2607.013099, mae: 462.872771, mean_q: 962.286136, mean_tau: 0.062826\n",
            " 18804/50000: episode: 787, duration: 0.153s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2825.862827, mae: 455.070992, mean_q: 962.871087, mean_tau: 0.062787\n",
            " 18830/50000: episode: 788, duration: 0.196s, episode steps:  26, steps per second: 133, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1965.798030, mae: 458.284339, mean_q: 964.703719, mean_tau: 0.062743\n",
            " 18862/50000: episode: 789, duration: 0.245s, episode steps:  32, steps per second: 130, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 3037.099201, mae: 468.844695, mean_q: 983.741968, mean_tau: 0.062686\n",
            " 18872/50000: episode: 790, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2451.081970, mae: 456.757498, mean_q: 959.231140, mean_tau: 0.062644\n",
            " 18896/50000: episode: 791, duration: 0.181s, episode steps:  24, steps per second: 133, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2686.655408, mae: 457.040015, mean_q: 961.682332, mean_tau: 0.062611\n",
            " 18912/50000: episode: 792, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2832.725460, mae: 461.657492, mean_q: 970.718632, mean_tau: 0.062571\n",
            " 18935/50000: episode: 793, duration: 0.188s, episode steps:  23, steps per second: 122, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2792.016063, mae: 468.244816, mean_q: 979.972468, mean_tau: 0.062532\n",
            " 18986/50000: episode: 794, duration: 0.405s, episode steps:  51, steps per second: 126, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2611.502885, mae: 468.294252, mean_q: 977.148120, mean_tau: 0.062459\n",
            " 18999/50000: episode: 795, duration: 0.109s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2513.654165, mae: 485.771825, mean_q: 1000.797349, mean_tau: 0.062396\n",
            " 19008/50000: episode: 796, duration: 0.074s, episode steps:   9, steps per second: 122, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 2700.786621, mae: 457.571869, mean_q: 952.219822, mean_tau: 0.062374\n",
            " 19030/50000: episode: 797, duration: 0.177s, episode steps:  22, steps per second: 124, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1605.391893, mae: 470.823044, mean_q: 994.634300, mean_tau: 0.062343\n",
            " 19067/50000: episode: 798, duration: 0.285s, episode steps:  37, steps per second: 130, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 2490.204039, mae: 465.200183, mean_q: 975.863417, mean_tau: 0.062285\n",
            " 19087/50000: episode: 799, duration: 0.148s, episode steps:  20, steps per second: 136, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2589.248145, mae: 474.255739, mean_q: 992.196735, mean_tau: 0.062229\n",
            " 19132/50000: episode: 800, duration: 0.439s, episode steps:  45, steps per second: 102, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2607.609681, mae: 481.307316, mean_q: 1004.438418, mean_tau: 0.062164\n",
            " 19161/50000: episode: 801, duration: 0.341s, episode steps:  29, steps per second:  85, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2688.013851, mae: 485.676377, mean_q: 1013.699859, mean_tau: 0.062091\n",
            " 19171/50000: episode: 802, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4246.786731, mae: 487.916254, mean_q: 1006.359314, mean_tau: 0.062052\n",
            " 19263/50000: episode: 803, duration: 1.038s, episode steps:  92, steps per second:  89, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2899.234519, mae: 482.949196, mean_q: 1009.794621, mean_tau: 0.061951\n",
            " 19284/50000: episode: 804, duration: 0.241s, episode steps:  21, steps per second:  87, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2540.025498, mae: 483.062971, mean_q: 1009.009908, mean_tau: 0.061839\n",
            " 19303/50000: episode: 805, duration: 0.246s, episode steps:  19, steps per second:  77, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2087.865360, mae: 491.642572, mean_q: 1034.442659, mean_tau: 0.061800\n",
            " 19318/50000: episode: 806, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3997.182027, mae: 498.684735, mean_q: 1041.759949, mean_tau: 0.061766\n",
            " 19338/50000: episode: 807, duration: 0.251s, episode steps:  20, steps per second:  80, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2634.946906, mae: 482.907301, mean_q: 1007.231802, mean_tau: 0.061732\n",
            " 19363/50000: episode: 808, duration: 0.230s, episode steps:  25, steps per second: 109, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2533.556448, mae: 486.873688, mean_q: 1018.629446, mean_tau: 0.061687\n",
            " 19397/50000: episode: 809, duration: 0.259s, episode steps:  34, steps per second: 131, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3193.937146, mae: 493.151808, mean_q: 1043.686685, mean_tau: 0.061629\n",
            " 19418/50000: episode: 810, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2486.831959, mae: 505.318102, mean_q: 1049.227972, mean_tau: 0.061574\n",
            " 19442/50000: episode: 811, duration: 0.187s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2928.092056, mae: 496.938117, mean_q: 1039.995450, mean_tau: 0.061530\n",
            " 19465/50000: episode: 812, duration: 0.162s, episode steps:  23, steps per second: 142, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2879.811821, mae: 480.264035, mean_q: 1013.178865, mean_tau: 0.061483\n",
            " 19479/50000: episode: 813, duration: 0.118s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2663.237292, mae: 485.107932, mean_q: 1012.349479, mean_tau: 0.061446\n",
            " 19492/50000: episode: 814, duration: 0.099s, episode steps:  13, steps per second: 131, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4119.134043, mae: 489.691108, mean_q: 1033.279269, mean_tau: 0.061420\n",
            " 19519/50000: episode: 815, duration: 0.201s, episode steps:  27, steps per second: 134, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 4169.135526, mae: 501.867564, mean_q: 1043.363632, mean_tau: 0.061380\n",
            " 19558/50000: episode: 816, duration: 0.296s, episode steps:  39, steps per second: 132, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 3137.424080, mae: 494.381211, mean_q: 1030.790455, mean_tau: 0.061315\n",
            " 19573/50000: episode: 817, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2499.331462, mae: 502.977802, mean_q: 1053.386292, mean_tau: 0.061261\n",
            " 19588/50000: episode: 818, duration: 0.121s, episode steps:  15, steps per second: 124, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3578.926969, mae: 493.143447, mean_q: 1034.397575, mean_tau: 0.061232\n",
            " 19605/50000: episode: 819, duration: 0.137s, episode steps:  17, steps per second: 124, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3966.896635, mae: 509.967312, mean_q: 1054.205215, mean_tau: 0.061200\n",
            " 19627/50000: episode: 820, duration: 0.168s, episode steps:  22, steps per second: 131, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2877.011416, mae: 493.250187, mean_q: 1025.470376, mean_tau: 0.061161\n",
            " 19647/50000: episode: 821, duration: 0.160s, episode steps:  20, steps per second: 125, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3684.160516, mae: 491.489067, mean_q: 1032.072003, mean_tau: 0.061120\n",
            " 19663/50000: episode: 822, duration: 0.133s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3244.567734, mae: 491.407232, mean_q: 1024.776291, mean_tau: 0.061084\n",
            " 19692/50000: episode: 823, duration: 0.228s, episode steps:  29, steps per second: 127, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2603.067235, mae: 501.456046, mean_q: 1042.615024, mean_tau: 0.061040\n",
            " 19742/50000: episode: 824, duration: 0.372s, episode steps:  50, steps per second: 134, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3401.484810, mae: 497.894111, mean_q: 1046.740344, mean_tau: 0.060961\n",
            " 19765/50000: episode: 825, duration: 0.176s, episode steps:  23, steps per second: 131, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3082.136891, mae: 487.445354, mean_q: 1024.145436, mean_tau: 0.060889\n",
            " 19789/50000: episode: 826, duration: 0.196s, episode steps:  24, steps per second: 123, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3079.242416, mae: 511.922250, mean_q: 1072.179845, mean_tau: 0.060843\n",
            " 19804/50000: episode: 827, duration: 0.111s, episode steps:  15, steps per second: 135, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3747.860413, mae: 524.636959, mean_q: 1088.205806, mean_tau: 0.060804\n",
            " 19828/50000: episode: 828, duration: 0.182s, episode steps:  24, steps per second: 132, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3181.542274, mae: 502.892133, mean_q: 1049.616613, mean_tau: 0.060765\n",
            " 19841/50000: episode: 829, duration: 0.095s, episode steps:  13, steps per second: 137, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3558.996962, mae: 475.094358, mean_q: 991.172359, mean_tau: 0.060729\n",
            " 19862/50000: episode: 830, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2609.044102, mae: 499.352913, mean_q: 1048.897926, mean_tau: 0.060695\n",
            " 19884/50000: episode: 831, duration: 0.176s, episode steps:  22, steps per second: 125, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4079.557073, mae: 506.958779, mean_q: 1064.085064, mean_tau: 0.060652\n",
            " 19895/50000: episode: 832, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2835.069674, mae: 527.593714, mean_q: 1090.393893, mean_tau: 0.060620\n",
            " 19920/50000: episode: 833, duration: 0.186s, episode steps:  25, steps per second: 135, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3560.441183, mae: 513.254348, mean_q: 1073.156851, mean_tau: 0.060584\n",
            " 19944/50000: episode: 834, duration: 0.201s, episode steps:  24, steps per second: 119, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1978.993969, mae: 516.963927, mean_q: 1073.513641, mean_tau: 0.060536\n",
            " 19976/50000: episode: 835, duration: 0.234s, episode steps:  32, steps per second: 137, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3061.283854, mae: 507.005928, mean_q: 1071.780153, mean_tau: 0.060480\n",
            " 19992/50000: episode: 836, duration: 0.122s, episode steps:  16, steps per second: 132, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3345.817987, mae: 495.838881, mean_q: 1056.338322, mean_tau: 0.060433\n",
            " 20011/50000: episode: 837, duration: 0.161s, episode steps:  19, steps per second: 118, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3407.589966, mae: 525.079090, mean_q: 1096.013832, mean_tau: 0.060398\n",
            " 20023/50000: episode: 838, duration: 0.097s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1940.392390, mae: 517.059786, mean_q: 1100.828481, mean_tau: 0.060367\n",
            " 20107/50000: episode: 839, duration: 0.602s, episode steps:  84, steps per second: 140, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 3942.854159, mae: 514.914488, mean_q: 1080.175269, mean_tau: 0.060272\n",
            " 20149/50000: episode: 840, duration: 0.311s, episode steps:  42, steps per second: 135, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3666.967990, mae: 512.885140, mean_q: 1078.638383, mean_tau: 0.060148\n",
            " 20164/50000: episode: 841, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2774.078133, mae: 508.041862, mean_q: 1046.133232, mean_tau: 0.060091\n",
            " 20187/50000: episode: 842, duration: 0.192s, episode steps:  23, steps per second: 120, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3089.193293, mae: 525.569690, mean_q: 1098.211054, mean_tau: 0.060054\n",
            " 20232/50000: episode: 843, duration: 0.336s, episode steps:  45, steps per second: 134, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 3344.167626, mae: 524.421501, mean_q: 1096.852314, mean_tau: 0.059986\n",
            " 20248/50000: episode: 844, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2894.842491, mae: 509.270304, mean_q: 1081.225853, mean_tau: 0.059926\n",
            " 20268/50000: episode: 845, duration: 0.167s, episode steps:  20, steps per second: 120, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3185.468384, mae: 505.440013, mean_q: 1070.057758, mean_tau: 0.059890\n",
            " 20288/50000: episode: 846, duration: 0.143s, episode steps:  20, steps per second: 140, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3781.701007, mae: 523.381325, mean_q: 1091.612894, mean_tau: 0.059851\n",
            " 20304/50000: episode: 847, duration: 0.133s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 3379.702168, mae: 514.856375, mean_q: 1063.409294, mean_tau: 0.059815\n",
            " 20321/50000: episode: 848, duration: 0.131s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2798.389171, mae: 510.535555, mean_q: 1080.567595, mean_tau: 0.059782\n",
            " 20334/50000: episode: 849, duration: 0.111s, episode steps:  13, steps per second: 117, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4069.469858, mae: 514.386686, mean_q: 1070.415429, mean_tau: 0.059753\n",
            " 20354/50000: episode: 850, duration: 0.144s, episode steps:  20, steps per second: 139, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5193.099091, mae: 522.368613, mean_q: 1099.644400, mean_tau: 0.059720\n",
            " 20369/50000: episode: 851, duration: 0.119s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4062.638859, mae: 516.314848, mean_q: 1082.770772, mean_tau: 0.059685\n",
            " 20379/50000: episode: 852, duration: 0.082s, episode steps:  10, steps per second: 121, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2904.332642, mae: 502.528882, mean_q: 1064.774237, mean_tau: 0.059660\n",
            " 20406/50000: episode: 853, duration: 0.202s, episode steps:  27, steps per second: 134, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3716.832725, mae: 527.745194, mean_q: 1097.743153, mean_tau: 0.059624\n",
            " 20427/50000: episode: 854, duration: 0.161s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2864.741041, mae: 531.106955, mean_q: 1121.032610, mean_tau: 0.059576\n",
            " 20466/50000: episode: 855, duration: 0.290s, episode steps:  39, steps per second: 135, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3978.873691, mae: 533.140297, mean_q: 1115.977965, mean_tau: 0.059517\n",
            " 20485/50000: episode: 856, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3606.695643, mae: 524.795479, mean_q: 1108.108058, mean_tau: 0.059459\n",
            " 20506/50000: episode: 857, duration: 0.176s, episode steps:  21, steps per second: 119, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 2711.712141, mae: 541.275237, mean_q: 1137.263782, mean_tau: 0.059420\n",
            " 20518/50000: episode: 858, duration: 0.094s, episode steps:  12, steps per second: 128, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3015.903381, mae: 545.621943, mean_q: 1133.878174, mean_tau: 0.059387\n",
            " 20532/50000: episode: 859, duration: 0.122s, episode steps:  14, steps per second: 115, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 4255.213776, mae: 530.695476, mean_q: 1112.896933, mean_tau: 0.059361\n",
            " 20551/50000: episode: 860, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3016.751709, mae: 532.888442, mean_q: 1116.317926, mean_tau: 0.059329\n",
            " 20564/50000: episode: 861, duration: 0.101s, episode steps:  13, steps per second: 128, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3859.958548, mae: 518.405217, mean_q: 1086.191941, mean_tau: 0.059297\n",
            " 20625/50000: episode: 862, duration: 0.437s, episode steps:  61, steps per second: 140, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3897.059766, mae: 531.068158, mean_q: 1105.458883, mean_tau: 0.059224\n",
            " 20641/50000: episode: 863, duration: 0.129s, episode steps:  16, steps per second: 124, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 4281.697067, mae: 541.098726, mean_q: 1124.826004, mean_tau: 0.059148\n",
            " 20658/50000: episode: 864, duration: 0.209s, episode steps:  17, steps per second:  81, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3717.373219, mae: 525.653857, mean_q: 1099.892022, mean_tau: 0.059115\n",
            " 20685/50000: episode: 865, duration: 0.303s, episode steps:  27, steps per second:  89, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2757.539539, mae: 532.748192, mean_q: 1110.579343, mean_tau: 0.059071\n",
            " 20701/50000: episode: 866, duration: 0.182s, episode steps:  16, steps per second:  88, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3807.279716, mae: 520.666967, mean_q: 1098.878216, mean_tau: 0.059029\n",
            " 20739/50000: episode: 867, duration: 0.411s, episode steps:  38, steps per second:  92, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 2534.243799, mae: 548.660539, mean_q: 1153.741304, mean_tau: 0.058975\n",
            " 20769/50000: episode: 868, duration: 0.338s, episode steps:  30, steps per second:  89, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2522.713279, mae: 548.527850, mean_q: 1145.337508, mean_tau: 0.058908\n",
            " 20785/50000: episode: 869, duration: 0.177s, episode steps:  16, steps per second:  90, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3410.003063, mae: 534.144688, mean_q: 1120.216278, mean_tau: 0.058863\n",
            " 20804/50000: episode: 870, duration: 0.212s, episode steps:  19, steps per second:  89, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2496.499178, mae: 540.217910, mean_q: 1140.535645, mean_tau: 0.058828\n",
            " 20820/50000: episode: 871, duration: 0.180s, episode steps:  16, steps per second:  89, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4224.447529, mae: 547.179537, mean_q: 1139.048653, mean_tau: 0.058793\n",
            " 20841/50000: episode: 872, duration: 0.247s, episode steps:  21, steps per second:  85, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 2875.197328, mae: 540.837247, mean_q: 1129.317935, mean_tau: 0.058757\n",
            " 20868/50000: episode: 873, duration: 0.321s, episode steps:  27, steps per second:  84, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3489.326850, mae: 536.770839, mean_q: 1113.402267, mean_tau: 0.058709\n",
            " 20880/50000: episode: 874, duration: 0.137s, episode steps:  12, steps per second:  87, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2988.624685, mae: 540.468956, mean_q: 1137.057363, mean_tau: 0.058670\n",
            " 20896/50000: episode: 875, duration: 0.183s, episode steps:  16, steps per second:  88, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3517.345695, mae: 547.543457, mean_q: 1149.061920, mean_tau: 0.058643\n",
            " 20921/50000: episode: 876, duration: 0.182s, episode steps:  25, steps per second: 137, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4244.316359, mae: 541.584136, mean_q: 1130.490615, mean_tau: 0.058602\n",
            " 20941/50000: episode: 877, duration: 0.163s, episode steps:  20, steps per second: 123, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3331.471475, mae: 539.782018, mean_q: 1133.225827, mean_tau: 0.058558\n",
            " 20965/50000: episode: 878, duration: 0.198s, episode steps:  24, steps per second: 121, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 5665.598506, mae: 528.223246, mean_q: 1104.830981, mean_tau: 0.058514\n",
            " 20986/50000: episode: 879, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3499.479263, mae: 548.097700, mean_q: 1139.571609, mean_tau: 0.058469\n",
            " 20998/50000: episode: 880, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 4504.415202, mae: 525.148097, mean_q: 1102.722595, mean_tau: 0.058437\n",
            " 21057/50000: episode: 881, duration: 0.426s, episode steps:  59, steps per second: 139, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3205.798312, mae: 545.642141, mean_q: 1139.977396, mean_tau: 0.058367\n",
            " 21070/50000: episode: 882, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3787.154701, mae: 560.152668, mean_q: 1184.855309, mean_tau: 0.058295\n",
            " 21109/50000: episode: 883, duration: 0.305s, episode steps:  39, steps per second: 128, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 3635.445995, mae: 549.424766, mean_q: 1159.366818, mean_tau: 0.058244\n",
            " 21144/50000: episode: 884, duration: 0.287s, episode steps:  35, steps per second: 122, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 5405.624217, mae: 537.974541, mean_q: 1136.051132, mean_tau: 0.058171\n",
            " 21166/50000: episode: 885, duration: 0.168s, episode steps:  22, steps per second: 131, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3338.863650, mae: 557.259849, mean_q: 1160.854775, mean_tau: 0.058114\n",
            " 21196/50000: episode: 886, duration: 0.234s, episode steps:  30, steps per second: 128, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3544.740672, mae: 559.805999, mean_q: 1165.372583, mean_tau: 0.058063\n",
            " 21208/50000: episode: 887, duration: 0.110s, episode steps:  12, steps per second: 109, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3816.491430, mae: 562.039586, mean_q: 1148.377248, mean_tau: 0.058021\n",
            " 21222/50000: episode: 888, duration: 0.126s, episode steps:  14, steps per second: 111, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3682.981929, mae: 542.013033, mean_q: 1138.739001, mean_tau: 0.057995\n",
            " 21233/50000: episode: 889, duration: 0.086s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4102.255305, mae: 558.049000, mean_q: 1155.728604, mean_tau: 0.057971\n",
            " 21257/50000: episode: 890, duration: 0.198s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3572.614901, mae: 561.256449, mean_q: 1166.402067, mean_tau: 0.057936\n",
            " 21312/50000: episode: 891, duration: 0.398s, episode steps:  55, steps per second: 138, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 4099.846178, mae: 557.187102, mean_q: 1162.637317, mean_tau: 0.057858\n",
            " 21327/50000: episode: 892, duration: 0.115s, episode steps:  15, steps per second: 131, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4048.543795, mae: 588.799994, mean_q: 1224.211100, mean_tau: 0.057788\n",
            " 21353/50000: episode: 893, duration: 0.214s, episode steps:  26, steps per second: 121, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4045.451132, mae: 557.336954, mean_q: 1164.722914, mean_tau: 0.057748\n",
            " 21366/50000: episode: 894, duration: 0.103s, episode steps:  13, steps per second: 126, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2682.076282, mae: 560.658673, mean_q: 1167.471642, mean_tau: 0.057709\n",
            " 21384/50000: episode: 895, duration: 0.146s, episode steps:  18, steps per second: 123, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3627.564813, mae: 544.158283, mean_q: 1154.078620, mean_tau: 0.057678\n",
            " 21417/50000: episode: 896, duration: 0.262s, episode steps:  33, steps per second: 126, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4150.636325, mae: 563.646825, mean_q: 1179.242454, mean_tau: 0.057628\n",
            " 21431/50000: episode: 897, duration: 0.111s, episode steps:  14, steps per second: 126, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4223.747075, mae: 557.147396, mean_q: 1169.543100, mean_tau: 0.057581\n",
            " 21447/50000: episode: 898, duration: 0.137s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 4787.335575, mae: 566.143995, mean_q: 1181.193207, mean_tau: 0.057552\n",
            " 21462/50000: episode: 899, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4005.636247, mae: 564.851632, mean_q: 1179.852743, mean_tau: 0.057521\n",
            " 21496/50000: episode: 900, duration: 0.269s, episode steps:  34, steps per second: 126, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3500.516386, mae: 569.773785, mean_q: 1182.310590, mean_tau: 0.057473\n",
            " 21531/50000: episode: 901, duration: 0.280s, episode steps:  35, steps per second: 125, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2797.173710, mae: 569.652206, mean_q: 1192.500575, mean_tau: 0.057404\n",
            " 21556/50000: episode: 902, duration: 0.184s, episode steps:  25, steps per second: 136, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 3747.448888, mae: 575.436420, mean_q: 1211.334883, mean_tau: 0.057345\n",
            " 21573/50000: episode: 903, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3679.801883, mae: 561.120596, mean_q: 1184.866118, mean_tau: 0.057303\n",
            " 21597/50000: episode: 904, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6646.931015, mae: 573.065158, mean_q: 1195.694153, mean_tau: 0.057263\n",
            " 21662/50000: episode: 905, duration: 0.476s, episode steps:  65, steps per second: 137, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 3882.166159, mae: 572.856845, mean_q: 1198.026725, mean_tau: 0.057175\n",
            " 21748/50000: episode: 906, duration: 0.644s, episode steps:  86, steps per second: 134, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 4447.205061, mae: 577.158977, mean_q: 1205.353598, mean_tau: 0.057025\n",
            " 21784/50000: episode: 907, duration: 0.270s, episode steps:  36, steps per second: 133, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 4304.211514, mae: 573.166004, mean_q: 1203.914483, mean_tau: 0.056904\n",
            " 21797/50000: episode: 908, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4143.226854, mae: 588.832815, mean_q: 1222.100285, mean_tau: 0.056856\n",
            " 21815/50000: episode: 909, duration: 0.157s, episode steps:  18, steps per second: 114, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4916.253608, mae: 577.828057, mean_q: 1204.027676, mean_tau: 0.056825\n",
            " 21834/50000: episode: 910, duration: 0.160s, episode steps:  19, steps per second: 119, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4449.204095, mae: 573.677741, mean_q: 1195.700754, mean_tau: 0.056788\n",
            " 21848/50000: episode: 911, duration: 0.123s, episode steps:  14, steps per second: 113, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3257.797512, mae: 578.764816, mean_q: 1200.723014, mean_tau: 0.056756\n",
            " 21879/50000: episode: 912, duration: 0.261s, episode steps:  31, steps per second: 119, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4028.587270, mae: 586.117476, mean_q: 1223.536668, mean_tau: 0.056711\n",
            " 21894/50000: episode: 913, duration: 0.114s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 2344.282161, mae: 599.300903, mean_q: 1257.928792, mean_tau: 0.056666\n",
            " 21917/50000: episode: 914, duration: 0.186s, episode steps:  23, steps per second: 124, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 4103.805951, mae: 586.744186, mean_q: 1222.959430, mean_tau: 0.056628\n",
            " 21965/50000: episode: 915, duration: 0.385s, episode steps:  48, steps per second: 125, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 4579.333186, mae: 588.369867, mean_q: 1226.171623, mean_tau: 0.056558\n",
            " 21988/50000: episode: 916, duration: 0.190s, episode steps:  23, steps per second: 121, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3196.590706, mae: 589.022129, mean_q: 1214.159424, mean_tau: 0.056488\n",
            " 22023/50000: episode: 917, duration: 0.283s, episode steps:  35, steps per second: 124, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3371.347276, mae: 588.929181, mean_q: 1225.375024, mean_tau: 0.056430\n",
            " 22035/50000: episode: 918, duration: 0.101s, episode steps:  12, steps per second: 119, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3856.626668, mae: 579.634883, mean_q: 1202.055176, mean_tau: 0.056384\n",
            " 22051/50000: episode: 919, duration: 0.136s, episode steps:  16, steps per second: 117, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3915.539581, mae: 598.450367, mean_q: 1251.815132, mean_tau: 0.056356\n",
            " 22069/50000: episode: 920, duration: 0.136s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4253.804226, mae: 583.295600, mean_q: 1223.910048, mean_tau: 0.056322\n",
            " 22083/50000: episode: 921, duration: 0.120s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 4098.096366, mae: 592.188847, mean_q: 1238.055150, mean_tau: 0.056291\n",
            " 22095/50000: episode: 922, duration: 0.104s, episode steps:  12, steps per second: 115, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3618.305267, mae: 592.299561, mean_q: 1254.736806, mean_tau: 0.056265\n",
            " 22106/50000: episode: 923, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2550.120667, mae: 590.043185, mean_q: 1237.549472, mean_tau: 0.056242\n",
            " 22117/50000: episode: 924, duration: 0.114s, episode steps:  11, steps per second:  97, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3643.197377, mae: 561.057800, mean_q: 1197.463423, mean_tau: 0.056220\n",
            " 22149/50000: episode: 925, duration: 0.266s, episode steps:  32, steps per second: 120, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3770.769564, mae: 586.090950, mean_q: 1231.168392, mean_tau: 0.056178\n",
            " 22161/50000: episode: 926, duration: 0.140s, episode steps:  12, steps per second:  86, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 4912.798045, mae: 576.256210, mean_q: 1216.646022, mean_tau: 0.056134\n",
            " 22186/50000: episode: 927, duration: 0.320s, episode steps:  25, steps per second:  78, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3528.604343, mae: 591.310226, mean_q: 1238.940039, mean_tau: 0.056097\n",
            " 22199/50000: episode: 928, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4644.342651, mae: 584.979417, mean_q: 1212.347947, mean_tau: 0.056060\n",
            " 22211/50000: episode: 929, duration: 0.154s, episode steps:  12, steps per second:  78, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7400.184092, mae: 596.315140, mean_q: 1237.320140, mean_tau: 0.056035\n",
            " 22228/50000: episode: 930, duration: 0.186s, episode steps:  17, steps per second:  91, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3572.173997, mae: 574.613402, mean_q: 1207.820320, mean_tau: 0.056006\n",
            " 22240/50000: episode: 931, duration: 0.143s, episode steps:  12, steps per second:  84, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 5191.353516, mae: 584.396006, mean_q: 1228.732463, mean_tau: 0.055978\n",
            " 22253/50000: episode: 932, duration: 0.157s, episode steps:  13, steps per second:  83, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6265.246976, mae: 603.895930, mean_q: 1248.199904, mean_tau: 0.055953\n",
            " 22292/50000: episode: 933, duration: 0.433s, episode steps:  39, steps per second:  90, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 3855.043588, mae: 580.154260, mean_q: 1218.459335, mean_tau: 0.055901\n",
            " 22303/50000: episode: 934, duration: 0.130s, episode steps:  11, steps per second:  85, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4012.101562, mae: 570.722434, mean_q: 1199.155273, mean_tau: 0.055852\n",
            " 22369/50000: episode: 935, duration: 0.747s, episode steps:  66, steps per second:  88, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4452.300758, mae: 601.448493, mean_q: 1255.870030, mean_tau: 0.055776\n",
            " 22406/50000: episode: 936, duration: 0.402s, episode steps:  37, steps per second:  92, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 4105.413343, mae: 597.639566, mean_q: 1254.193587, mean_tau: 0.055674\n",
            " 22420/50000: episode: 937, duration: 0.113s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3376.205967, mae: 609.335133, mean_q: 1285.281189, mean_tau: 0.055623\n",
            " 22434/50000: episode: 938, duration: 0.120s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3225.821010, mae: 593.788439, mean_q: 1247.271022, mean_tau: 0.055596\n",
            " 22496/50000: episode: 939, duration: 0.503s, episode steps:  62, steps per second: 123, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4228.581598, mae: 596.041944, mean_q: 1256.007182, mean_tau: 0.055520\n",
            " 22509/50000: episode: 940, duration: 0.113s, episode steps:  13, steps per second: 115, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5445.005465, mae: 609.829933, mean_q: 1276.147940, mean_tau: 0.055446\n",
            " 22527/50000: episode: 941, duration: 0.154s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4734.323371, mae: 604.254690, mean_q: 1251.093031, mean_tau: 0.055415\n",
            " 22546/50000: episode: 942, duration: 0.148s, episode steps:  19, steps per second: 129, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3928.203687, mae: 615.852825, mean_q: 1285.675633, mean_tau: 0.055379\n",
            " 22578/50000: episode: 943, duration: 0.244s, episode steps:  32, steps per second: 131, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4633.179644, mae: 610.594503, mean_q: 1284.231560, mean_tau: 0.055328\n",
            " 22595/50000: episode: 944, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4834.685109, mae: 605.584817, mean_q: 1267.140675, mean_tau: 0.055280\n",
            " 22609/50000: episode: 945, duration: 0.119s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4830.362348, mae: 612.751622, mean_q: 1276.408439, mean_tau: 0.055249\n",
            " 22621/50000: episode: 946, duration: 0.101s, episode steps:  12, steps per second: 118, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4367.433792, mae: 623.796733, mean_q: 1297.896505, mean_tau: 0.055223\n",
            " 22644/50000: episode: 947, duration: 0.195s, episode steps:  23, steps per second: 118, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6295.472502, mae: 591.991942, mean_q: 1246.896060, mean_tau: 0.055189\n",
            " 22662/50000: episode: 948, duration: 0.141s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3179.213630, mae: 618.738413, mean_q: 1284.672987, mean_tau: 0.055148\n",
            " 22679/50000: episode: 949, duration: 0.151s, episode steps:  17, steps per second: 113, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5089.280360, mae: 611.364448, mean_q: 1292.436014, mean_tau: 0.055113\n",
            " 22756/50000: episode: 950, duration: 0.635s, episode steps:  77, steps per second: 121, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 4177.309688, mae: 615.420558, mean_q: 1283.497007, mean_tau: 0.055020\n",
            " 22767/50000: episode: 951, duration: 0.097s, episode steps:  11, steps per second: 113, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2369.249184, mae: 632.814620, mean_q: 1319.759788, mean_tau: 0.054933\n",
            " 22812/50000: episode: 952, duration: 0.345s, episode steps:  45, steps per second: 131, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 4741.295958, mae: 612.154686, mean_q: 1291.113219, mean_tau: 0.054878\n",
            " 22837/50000: episode: 953, duration: 0.188s, episode steps:  25, steps per second: 133, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 6612.207769, mae: 622.750916, mean_q: 1316.005083, mean_tau: 0.054808\n",
            " 22859/50000: episode: 954, duration: 0.173s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4788.594893, mae: 638.026445, mean_q: 1327.834062, mean_tau: 0.054762\n",
            " 22877/50000: episode: 955, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5492.026191, mae: 614.940684, mean_q: 1287.245680, mean_tau: 0.054722\n",
            " 22929/50000: episode: 956, duration: 0.416s, episode steps:  52, steps per second: 125, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7668.031088, mae: 629.198341, mean_q: 1312.742507, mean_tau: 0.054653\n",
            " 22947/50000: episode: 957, duration: 0.158s, episode steps:  18, steps per second: 114, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 5147.688232, mae: 627.273468, mean_q: 1317.744195, mean_tau: 0.054584\n",
            " 22963/50000: episode: 958, duration: 0.142s, episode steps:  16, steps per second: 113, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 4069.416885, mae: 617.512150, mean_q: 1293.666031, mean_tau: 0.054550\n",
            " 22979/50000: episode: 959, duration: 0.133s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5113.966125, mae: 643.462132, mean_q: 1335.147713, mean_tau: 0.054518\n",
            " 23011/50000: episode: 960, duration: 0.289s, episode steps:  32, steps per second: 111, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3525.165142, mae: 626.758814, mean_q: 1311.613911, mean_tau: 0.054471\n",
            " 23031/50000: episode: 961, duration: 0.176s, episode steps:  20, steps per second: 114, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4780.726205, mae: 646.750574, mean_q: 1341.115918, mean_tau: 0.054419\n",
            " 23079/50000: episode: 962, duration: 0.400s, episode steps:  48, steps per second: 120, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4443.855375, mae: 638.458392, mean_q: 1324.930224, mean_tau: 0.054352\n",
            " 23102/50000: episode: 963, duration: 0.191s, episode steps:  23, steps per second: 120, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4296.746964, mae: 613.888387, mean_q: 1280.326140, mean_tau: 0.054282\n",
            " 23118/50000: episode: 964, duration: 0.148s, episode steps:  16, steps per second: 108, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3648.037643, mae: 636.475601, mean_q: 1323.922905, mean_tau: 0.054243\n",
            " 23145/50000: episode: 965, duration: 0.228s, episode steps:  27, steps per second: 118, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4250.989934, mae: 638.260478, mean_q: 1346.682215, mean_tau: 0.054201\n",
            " 23158/50000: episode: 966, duration: 0.105s, episode steps:  13, steps per second: 123, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6139.439021, mae: 628.952759, mean_q: 1303.794199, mean_tau: 0.054161\n",
            " 23170/50000: episode: 967, duration: 0.101s, episode steps:  12, steps per second: 119, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3722.764943, mae: 619.883865, mean_q: 1304.611399, mean_tau: 0.054136\n",
            " 23190/50000: episode: 968, duration: 0.182s, episode steps:  20, steps per second: 110, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 4143.193558, mae: 628.089912, mean_q: 1314.219385, mean_tau: 0.054105\n",
            " 23202/50000: episode: 969, duration: 0.106s, episode steps:  12, steps per second: 113, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 4330.389857, mae: 641.334991, mean_q: 1338.316213, mean_tau: 0.054073\n",
            " 23214/50000: episode: 970, duration: 0.097s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 4600.810206, mae: 636.603531, mean_q: 1328.382609, mean_tau: 0.054049\n",
            " 23227/50000: episode: 971, duration: 0.111s, episode steps:  13, steps per second: 117, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4534.950411, mae: 642.624319, mean_q: 1354.451698, mean_tau: 0.054024\n",
            " 23241/50000: episode: 972, duration: 0.112s, episode steps:  14, steps per second: 125, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5640.025251, mae: 633.476118, mean_q: 1342.631557, mean_tau: 0.053998\n",
            " 23252/50000: episode: 973, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5404.984819, mae: 622.228294, mean_q: 1322.038363, mean_tau: 0.053973\n",
            " 23287/50000: episode: 974, duration: 0.289s, episode steps:  35, steps per second: 121, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5001.765995, mae: 636.503214, mean_q: 1329.900471, mean_tau: 0.053927\n",
            " 23313/50000: episode: 975, duration: 0.221s, episode steps:  26, steps per second: 118, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5045.221015, mae: 637.759493, mean_q: 1342.954069, mean_tau: 0.053867\n",
            " 23347/50000: episode: 976, duration: 0.283s, episode steps:  34, steps per second: 120, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 8071.610729, mae: 626.774629, mean_q: 1302.442167, mean_tau: 0.053808\n",
            " 23370/50000: episode: 977, duration: 0.219s, episode steps:  23, steps per second: 105, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 6284.813477, mae: 624.096664, mean_q: 1313.228776, mean_tau: 0.053751\n",
            " 23382/50000: episode: 978, duration: 0.105s, episode steps:  12, steps per second: 114, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9198.884755, mae: 648.292109, mean_q: 1345.856974, mean_tau: 0.053717\n",
            " 23397/50000: episode: 979, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3950.161772, mae: 636.062659, mean_q: 1344.618807, mean_tau: 0.053690\n",
            " 23412/50000: episode: 980, duration: 0.137s, episode steps:  15, steps per second: 110, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 7445.531934, mae: 630.386023, mean_q: 1319.182503, mean_tau: 0.053660\n",
            " 23462/50000: episode: 981, duration: 0.412s, episode steps:  50, steps per second: 121, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 4846.125047, mae: 644.365343, mean_q: 1336.414614, mean_tau: 0.053596\n",
            " 23503/50000: episode: 982, duration: 0.341s, episode steps:  41, steps per second: 120, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 6618.930485, mae: 652.620643, mean_q: 1354.540444, mean_tau: 0.053506\n",
            " 23519/50000: episode: 983, duration: 0.142s, episode steps:  16, steps per second: 112, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 4793.029083, mae: 645.699314, mean_q: 1335.435791, mean_tau: 0.053449\n",
            " 23570/50000: episode: 984, duration: 0.420s, episode steps:  51, steps per second: 122, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4938.668470, mae: 654.662811, mean_q: 1353.973757, mean_tau: 0.053383\n",
            " 23584/50000: episode: 985, duration: 0.134s, episode steps:  14, steps per second: 105, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6104.707921, mae: 652.164219, mean_q: 1358.002947, mean_tau: 0.053319\n",
            " 23599/50000: episode: 986, duration: 0.188s, episode steps:  15, steps per second:  80, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3606.263696, mae: 645.097282, mean_q: 1346.057918, mean_tau: 0.053290\n",
            " 23632/50000: episode: 987, duration: 0.436s, episode steps:  33, steps per second:  76, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 5165.646508, mae: 659.292085, mean_q: 1373.556252, mean_tau: 0.053242\n",
            " 23654/50000: episode: 988, duration: 0.260s, episode steps:  22, steps per second:  85, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4650.767978, mae: 656.132932, mean_q: 1354.678112, mean_tau: 0.053188\n",
            " 23670/50000: episode: 989, duration: 0.186s, episode steps:  16, steps per second:  86, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 4765.215834, mae: 656.807114, mean_q: 1355.657166, mean_tau: 0.053150\n",
            " 23698/50000: episode: 990, duration: 0.351s, episode steps:  28, steps per second:  80, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4518.031873, mae: 652.270536, mean_q: 1355.042332, mean_tau: 0.053107\n",
            " 23729/50000: episode: 991, duration: 0.370s, episode steps:  31, steps per second:  84, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 4612.700270, mae: 643.998738, mean_q: 1330.478110, mean_tau: 0.053048\n",
            " 23759/50000: episode: 992, duration: 0.364s, episode steps:  30, steps per second:  82, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5028.859357, mae: 662.107886, mean_q: 1376.232194, mean_tau: 0.052988\n",
            " 23783/50000: episode: 993, duration: 0.317s, episode steps:  24, steps per second:  76, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3332.848239, mae: 669.508565, mean_q: 1396.300013, mean_tau: 0.052934\n",
            " 23793/50000: episode: 994, duration: 0.122s, episode steps:  10, steps per second:  82, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 8442.335925, mae: 655.692078, mean_q: 1379.605542, mean_tau: 0.052901\n",
            " 23834/50000: episode: 995, duration: 0.425s, episode steps:  41, steps per second:  97, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 4366.127425, mae: 663.086336, mean_q: 1375.719313, mean_tau: 0.052850\n",
            " 23868/50000: episode: 996, duration: 0.265s, episode steps:  34, steps per second: 128, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5810.185059, mae: 645.553305, mean_q: 1347.291741, mean_tau: 0.052776\n",
            " 23883/50000: episode: 997, duration: 0.129s, episode steps:  15, steps per second: 117, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4372.652905, mae: 655.754826, mean_q: 1384.477767, mean_tau: 0.052727\n",
            " 23905/50000: episode: 998, duration: 0.177s, episode steps:  22, steps per second: 124, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5813.435647, mae: 684.478843, mean_q: 1428.731529, mean_tau: 0.052691\n",
            " 23935/50000: episode: 999, duration: 0.240s, episode steps:  30, steps per second: 125, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5401.552067, mae: 663.067440, mean_q: 1373.723169, mean_tau: 0.052639\n",
            " 23954/50000: episode: 1000, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3634.416176, mae: 673.386365, mean_q: 1406.539570, mean_tau: 0.052591\n",
            " 23967/50000: episode: 1001, duration: 0.109s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 5702.526597, mae: 665.498972, mean_q: 1383.908447, mean_tau: 0.052559\n",
            " 23988/50000: episode: 1002, duration: 0.173s, episode steps:  21, steps per second: 121, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 6473.747198, mae: 651.780657, mean_q: 1366.619135, mean_tau: 0.052526\n",
            " 24003/50000: episode: 1003, duration: 0.132s, episode steps:  15, steps per second: 113, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3964.432300, mae: 654.492871, mean_q: 1382.717065, mean_tau: 0.052490\n",
            " 24025/50000: episode: 1004, duration: 0.187s, episode steps:  22, steps per second: 118, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5394.694247, mae: 644.673623, mean_q: 1353.525557, mean_tau: 0.052453\n",
            " 24035/50000: episode: 1005, duration: 0.085s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 11646.489783, mae: 673.062067, mean_q: 1375.954565, mean_tau: 0.052422\n",
            " 24056/50000: episode: 1006, duration: 0.169s, episode steps:  21, steps per second: 124, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 5726.197091, mae: 653.975441, mean_q: 1365.251465, mean_tau: 0.052391\n",
            " 24070/50000: episode: 1007, duration: 0.132s, episode steps:  14, steps per second: 106, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6153.250323, mae: 664.447789, mean_q: 1379.433472, mean_tau: 0.052356\n",
            " 24081/50000: episode: 1008, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 9262.100985, mae: 684.197027, mean_q: 1392.414418, mean_tau: 0.052332\n",
            " 24139/50000: episode: 1009, duration: 0.479s, episode steps:  58, steps per second: 121, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 4589.680754, mae: 674.281829, mean_q: 1403.897861, mean_tau: 0.052263\n",
            " 24150/50000: episode: 1010, duration: 0.092s, episode steps:  11, steps per second: 119, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4484.668646, mae: 674.942050, mean_q: 1420.523648, mean_tau: 0.052195\n",
            " 24182/50000: episode: 1011, duration: 0.266s, episode steps:  32, steps per second: 120, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4582.845772, mae: 665.691921, mean_q: 1394.814674, mean_tau: 0.052152\n",
            " 24199/50000: episode: 1012, duration: 0.135s, episode steps:  17, steps per second: 126, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4389.465185, mae: 686.465389, mean_q: 1434.140388, mean_tau: 0.052104\n",
            " 24209/50000: episode: 1013, duration: 0.088s, episode steps:  10, steps per second: 114, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4200.770569, mae: 662.959369, mean_q: 1382.894360, mean_tau: 0.052077\n",
            " 24234/50000: episode: 1014, duration: 0.198s, episode steps:  25, steps per second: 126, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5306.069512, mae: 671.733464, mean_q: 1408.719961, mean_tau: 0.052042\n",
            " 24259/50000: episode: 1015, duration: 0.226s, episode steps:  25, steps per second: 111, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5627.774966, mae: 669.378079, mean_q: 1399.973667, mean_tau: 0.051993\n",
            " 24304/50000: episode: 1016, duration: 0.366s, episode steps:  45, steps per second: 123, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5262.911708, mae: 677.660665, mean_q: 1410.050909, mean_tau: 0.051924\n",
            " 24319/50000: episode: 1017, duration: 0.128s, episode steps:  15, steps per second: 117, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6234.565194, mae: 680.029911, mean_q: 1402.040560, mean_tau: 0.051864\n",
            " 24333/50000: episode: 1018, duration: 0.121s, episode steps:  14, steps per second: 116, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5530.865147, mae: 669.513960, mean_q: 1407.429600, mean_tau: 0.051836\n",
            " 24344/50000: episode: 1019, duration: 0.097s, episode steps:  11, steps per second: 114, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 4786.542924, mae: 667.323076, mean_q: 1404.521018, mean_tau: 0.051811\n",
            " 24363/50000: episode: 1020, duration: 0.159s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 9154.697497, mae: 669.179620, mean_q: 1396.349391, mean_tau: 0.051781\n",
            " 24395/50000: episode: 1021, duration: 0.270s, episode steps:  32, steps per second: 118, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 5110.640820, mae: 673.454723, mean_q: 1406.491188, mean_tau: 0.051731\n",
            " 24414/50000: episode: 1022, duration: 0.149s, episode steps:  19, steps per second: 127, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5014.528744, mae: 680.277646, mean_q: 1420.962981, mean_tau: 0.051680\n",
            " 24430/50000: episode: 1023, duration: 0.129s, episode steps:  16, steps per second: 124, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 10045.405724, mae: 695.251839, mean_q: 1429.859779, mean_tau: 0.051645\n",
            " 24442/50000: episode: 1024, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 7487.937805, mae: 721.972234, mean_q: 1504.550496, mean_tau: 0.051618\n",
            " 24461/50000: episode: 1025, duration: 0.161s, episode steps:  19, steps per second: 118, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5939.889289, mae: 684.767048, mean_q: 1419.918637, mean_tau: 0.051587\n",
            " 24489/50000: episode: 1026, duration: 0.232s, episode steps:  28, steps per second: 121, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 7743.046849, mae: 679.371896, mean_q: 1416.760219, mean_tau: 0.051540\n",
            " 24507/50000: episode: 1027, duration: 0.155s, episode steps:  18, steps per second: 116, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 7953.274414, mae: 691.958794, mean_q: 1423.012906, mean_tau: 0.051495\n",
            " 24535/50000: episode: 1028, duration: 0.231s, episode steps:  28, steps per second: 121, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6395.343811, mae: 686.815489, mean_q: 1418.484414, mean_tau: 0.051449\n",
            " 24574/50000: episode: 1029, duration: 0.298s, episode steps:  39, steps per second: 131, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5001.835784, mae: 683.720705, mean_q: 1415.962903, mean_tau: 0.051383\n",
            " 24587/50000: episode: 1030, duration: 0.109s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3764.824904, mae: 682.398090, mean_q: 1434.162213, mean_tau: 0.051332\n",
            " 24598/50000: episode: 1031, duration: 0.093s, episode steps:  11, steps per second: 119, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4678.163497, mae: 688.473394, mean_q: 1439.304865, mean_tau: 0.051308\n",
            " 24613/50000: episode: 1032, duration: 0.132s, episode steps:  15, steps per second: 113, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6182.851302, mae: 680.132825, mean_q: 1414.040462, mean_tau: 0.051282\n",
            " 24680/50000: episode: 1033, duration: 0.508s, episode steps:  67, steps per second: 132, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4660.645346, mae: 680.935664, mean_q: 1420.327327, mean_tau: 0.051201\n",
            " 24718/50000: episode: 1034, duration: 0.292s, episode steps:  38, steps per second: 130, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 8597.038751, mae: 696.777008, mean_q: 1434.348633, mean_tau: 0.051097\n",
            " 24729/50000: episode: 1035, duration: 0.086s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4079.403531, mae: 714.742049, mean_q: 1472.844482, mean_tau: 0.051048\n",
            " 24761/50000: episode: 1036, duration: 0.249s, episode steps:  32, steps per second: 129, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 8009.713310, mae: 706.796410, mean_q: 1464.637035, mean_tau: 0.051006\n",
            " 24773/50000: episode: 1037, duration: 0.090s, episode steps:  12, steps per second: 133, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 4295.764384, mae: 690.383479, mean_q: 1444.549835, mean_tau: 0.050962\n",
            " 24821/50000: episode: 1038, duration: 0.364s, episode steps:  48, steps per second: 132, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 6599.398716, mae: 689.687463, mean_q: 1426.889666, mean_tau: 0.050903\n",
            " 24845/50000: episode: 1039, duration: 0.184s, episode steps:  24, steps per second: 130, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4306.294174, mae: 688.977150, mean_q: 1426.637736, mean_tau: 0.050832\n",
            " 24863/50000: episode: 1040, duration: 0.147s, episode steps:  18, steps per second: 122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 6827.489231, mae: 700.824632, mean_q: 1452.351732, mean_tau: 0.050790\n",
            " 24900/50000: episode: 1041, duration: 0.294s, episode steps:  37, steps per second: 126, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 6496.202897, mae: 712.045643, mean_q: 1488.884644, mean_tau: 0.050736\n",
            " 24913/50000: episode: 1042, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5419.516897, mae: 696.298706, mean_q: 1473.081271, mean_tau: 0.050686\n",
            " 24933/50000: episode: 1043, duration: 0.155s, episode steps:  20, steps per second: 129, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 7407.263416, mae: 702.314264, mean_q: 1440.794391, mean_tau: 0.050653\n",
            " 24948/50000: episode: 1044, duration: 0.110s, episode steps:  15, steps per second: 136, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6812.553678, mae: 687.246452, mean_q: 1422.783431, mean_tau: 0.050619\n",
            " 24964/50000: episode: 1045, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 7056.958069, mae: 690.074268, mean_q: 1436.351303, mean_tau: 0.050588\n",
            " 24978/50000: episode: 1046, duration: 0.101s, episode steps:  14, steps per second: 138, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3429.844434, mae: 681.985683, mean_q: 1417.035723, mean_tau: 0.050558\n",
            " 24988/50000: episode: 1047, duration: 0.081s, episode steps:  10, steps per second: 124, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 3318.658362, mae: 720.524774, mean_q: 1513.905164, mean_tau: 0.050535\n",
            " 25006/50000: episode: 1048, duration: 0.148s, episode steps:  18, steps per second: 121, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 5291.525370, mae: 707.320190, mean_q: 1478.458164, mean_tau: 0.050507\n",
            " 25018/50000: episode: 1049, duration: 0.098s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5400.124090, mae: 691.768845, mean_q: 1440.602559, mean_tau: 0.050477\n",
            " 25059/50000: episode: 1050, duration: 0.378s, episode steps:  41, steps per second: 108, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 6578.343918, mae: 712.442529, mean_q: 1472.163634, mean_tau: 0.050425\n",
            " 25084/50000: episode: 1051, duration: 0.300s, episode steps:  25, steps per second:  83, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 3836.944944, mae: 696.283433, mean_q: 1453.473115, mean_tau: 0.050359\n",
            " 25097/50000: episode: 1052, duration: 0.153s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4089.428312, mae: 708.977347, mean_q: 1473.273935, mean_tau: 0.050322\n",
            " 25129/50000: episode: 1053, duration: 0.363s, episode steps:  32, steps per second:  88, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7284.971264, mae: 707.226431, mean_q: 1466.607754, mean_tau: 0.050277\n",
            " 25150/50000: episode: 1054, duration: 0.229s, episode steps:  21, steps per second:  92, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6173.942891, mae: 693.999698, mean_q: 1461.295265, mean_tau: 0.050225\n",
            " 25177/50000: episode: 1055, duration: 0.301s, episode steps:  27, steps per second:  90, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6272.065326, mae: 718.607105, mean_q: 1493.279306, mean_tau: 0.050177\n",
            " 25192/50000: episode: 1056, duration: 0.170s, episode steps:  15, steps per second:  88, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5043.388314, mae: 711.955135, mean_q: 1481.309066, mean_tau: 0.050136\n",
            " 25212/50000: episode: 1057, duration: 0.241s, episode steps:  20, steps per second:  83, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 8127.225076, mae: 723.523245, mean_q: 1504.860828, mean_tau: 0.050101\n",
            " 25232/50000: episode: 1058, duration: 0.242s, episode steps:  20, steps per second:  83, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 3201.763232, mae: 716.745871, mean_q: 1496.841626, mean_tau: 0.050061\n",
            " 25242/50000: episode: 1059, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 5179.593933, mae: 728.468585, mean_q: 1505.388684, mean_tau: 0.050032\n",
            " 25262/50000: episode: 1060, duration: 0.245s, episode steps:  20, steps per second:  82, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5791.525555, mae: 707.102765, mean_q: 1471.989868, mean_tau: 0.050002\n",
            " 25275/50000: episode: 1061, duration: 0.153s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4348.500211, mae: 728.756334, mean_q: 1505.632334, mean_tau: 0.049969\n",
            " 25294/50000: episode: 1062, duration: 0.211s, episode steps:  19, steps per second:  90, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 5556.344560, mae: 731.652379, mean_q: 1521.546592, mean_tau: 0.049938\n",
            " 25312/50000: episode: 1063, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4693.577610, mae: 707.351332, mean_q: 1488.600484, mean_tau: 0.049901\n",
            " 25332/50000: episode: 1064, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 4239.546539, mae: 723.607886, mean_q: 1509.744031, mean_tau: 0.049863\n",
            " 25348/50000: episode: 1065, duration: 0.118s, episode steps:  16, steps per second: 135, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 8438.813393, mae: 732.638462, mean_q: 1528.319206, mean_tau: 0.049828\n",
            " 25366/50000: episode: 1066, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5415.410251, mae: 717.544033, mean_q: 1487.130100, mean_tau: 0.049794\n",
            " 25380/50000: episode: 1067, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4410.742379, mae: 732.533595, mean_q: 1527.729449, mean_tau: 0.049762\n",
            " 25401/50000: episode: 1068, duration: 0.163s, episode steps:  21, steps per second: 129, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 7338.553813, mae: 709.309004, mean_q: 1493.996849, mean_tau: 0.049728\n",
            " 25424/50000: episode: 1069, duration: 0.180s, episode steps:  23, steps per second: 128, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5961.535700, mae: 698.550280, mean_q: 1476.519669, mean_tau: 0.049684\n",
            " 25435/50000: episode: 1070, duration: 0.098s, episode steps:  11, steps per second: 113, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 11614.033336, mae: 727.493281, mean_q: 1501.933749, mean_tau: 0.049651\n",
            " 25463/50000: episode: 1071, duration: 0.228s, episode steps:  28, steps per second: 123, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 6415.541114, mae: 726.966653, mean_q: 1505.882965, mean_tau: 0.049612\n",
            " 25482/50000: episode: 1072, duration: 0.163s, episode steps:  19, steps per second: 116, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4384.554006, mae: 716.332542, mean_q: 1478.765933, mean_tau: 0.049565\n",
            " 25503/50000: episode: 1073, duration: 0.179s, episode steps:  21, steps per second: 118, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 7426.323632, mae: 725.989790, mean_q: 1509.414975, mean_tau: 0.049526\n",
            " 25514/50000: episode: 1074, duration: 0.104s, episode steps:  11, steps per second: 106, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 5475.123147, mae: 753.540372, mean_q: 1576.339067, mean_tau: 0.049494\n",
            " 25536/50000: episode: 1075, duration: 0.176s, episode steps:  22, steps per second: 125, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5979.963900, mae: 715.863875, mean_q: 1494.748641, mean_tau: 0.049461\n",
            " 25555/50000: episode: 1076, duration: 0.156s, episode steps:  19, steps per second: 122, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 8383.244205, mae: 737.054119, mean_q: 1527.318469, mean_tau: 0.049421\n",
            " 25584/50000: episode: 1077, duration: 0.222s, episode steps:  29, steps per second: 131, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5438.504174, mae: 742.741068, mean_q: 1532.906162, mean_tau: 0.049373\n",
            " 25596/50000: episode: 1078, duration: 0.102s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 4226.985118, mae: 720.910741, mean_q: 1508.418701, mean_tau: 0.049333\n",
            " 25618/50000: episode: 1079, duration: 0.181s, episode steps:  22, steps per second: 121, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 5101.382172, mae: 726.966569, mean_q: 1519.770286, mean_tau: 0.049299\n",
            " 25634/50000: episode: 1080, duration: 0.137s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5383.520798, mae: 741.357044, mean_q: 1543.840508, mean_tau: 0.049262\n",
            " 25651/50000: episode: 1081, duration: 0.147s, episode steps:  17, steps per second: 115, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6103.311739, mae: 734.234264, mean_q: 1530.945600, mean_tau: 0.049229\n",
            " 25665/50000: episode: 1082, duration: 0.119s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3878.675982, mae: 736.954206, mean_q: 1525.665057, mean_tau: 0.049198\n",
            " 25687/50000: episode: 1083, duration: 0.179s, episode steps:  22, steps per second: 123, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5225.623191, mae: 734.296057, mean_q: 1531.920732, mean_tau: 0.049163\n",
            " 25713/50000: episode: 1084, duration: 0.208s, episode steps:  26, steps per second: 125, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 10534.912790, mae: 747.850823, mean_q: 1556.885855, mean_tau: 0.049115\n",
            " 25741/50000: episode: 1085, duration: 0.233s, episode steps:  28, steps per second: 120, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 8183.317132, mae: 740.372062, mean_q: 1540.971553, mean_tau: 0.049062\n",
            " 25759/50000: episode: 1086, duration: 0.150s, episode steps:  18, steps per second: 120, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 4460.446743, mae: 740.181088, mean_q: 1542.839661, mean_tau: 0.049016\n",
            " 25771/50000: episode: 1087, duration: 0.110s, episode steps:  12, steps per second: 109, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 6817.729848, mae: 757.082153, mean_q: 1562.104095, mean_tau: 0.048986\n",
            " 25790/50000: episode: 1088, duration: 0.167s, episode steps:  19, steps per second: 114, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 11638.828311, mae: 742.318141, mean_q: 1530.420526, mean_tau: 0.048956\n",
            " 25801/50000: episode: 1089, duration: 0.097s, episode steps:  11, steps per second: 114, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 7389.524858, mae: 715.414756, mean_q: 1483.186357, mean_tau: 0.048926\n",
            " 25836/50000: episode: 1090, duration: 0.286s, episode steps:  35, steps per second: 122, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5315.087017, mae: 748.609511, mean_q: 1543.908018, mean_tau: 0.048880\n",
            " 25847/50000: episode: 1091, duration: 0.084s, episode steps:  11, steps per second: 132, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 8180.502319, mae: 716.953896, mean_q: 1509.655529, mean_tau: 0.048835\n",
            " 25860/50000: episode: 1092, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 10080.642982, mae: 758.166480, mean_q: 1573.514132, mean_tau: 0.048811\n",
            " 25882/50000: episode: 1093, duration: 0.180s, episode steps:  22, steps per second: 122, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6524.817502, mae: 740.117579, mean_q: 1527.171231, mean_tau: 0.048776\n",
            " 25899/50000: episode: 1094, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5804.195880, mae: 750.355846, mean_q: 1554.667208, mean_tau: 0.048738\n",
            " 25919/50000: episode: 1095, duration: 0.172s, episode steps:  20, steps per second: 116, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7017.092950, mae: 747.175278, mean_q: 1539.236157, mean_tau: 0.048701\n",
            " 25970/50000: episode: 1096, duration: 0.397s, episode steps:  51, steps per second: 129, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5929.081707, mae: 733.885760, mean_q: 1533.355225, mean_tau: 0.048631\n",
            " 25986/50000: episode: 1097, duration: 0.135s, episode steps:  16, steps per second: 119, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6053.216942, mae: 741.023167, mean_q: 1532.419479, mean_tau: 0.048565\n",
            " 26007/50000: episode: 1098, duration: 0.171s, episode steps:  21, steps per second: 123, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6219.514457, mae: 736.868391, mean_q: 1519.845628, mean_tau: 0.048528\n",
            " 26021/50000: episode: 1099, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5305.429095, mae: 769.289311, mean_q: 1602.137800, mean_tau: 0.048493\n",
            " 26039/50000: episode: 1100, duration: 0.156s, episode steps:  18, steps per second: 116, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4936.692451, mae: 755.833577, mean_q: 1553.415839, mean_tau: 0.048462\n",
            " 26107/50000: episode: 1101, duration: 0.507s, episode steps:  68, steps per second: 134, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8253.879621, mae: 745.949448, mean_q: 1539.582146, mean_tau: 0.048376\n",
            " 26131/50000: episode: 1102, duration: 0.183s, episode steps:  24, steps per second: 131, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6719.218567, mae: 758.000216, mean_q: 1572.149607, mean_tau: 0.048285\n",
            " 26175/50000: episode: 1103, duration: 0.331s, episode steps:  44, steps per second: 133, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 7311.073775, mae: 760.083812, mean_q: 1575.247495, mean_tau: 0.048218\n",
            " 26185/50000: episode: 1104, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 8619.442505, mae: 761.014258, mean_q: 1571.149475, mean_tau: 0.048165\n",
            " 26196/50000: episode: 1105, duration: 0.081s, episode steps:  11, steps per second: 135, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 10481.283447, mae: 777.986173, mean_q: 1591.256525, mean_tau: 0.048144\n",
            " 26234/50000: episode: 1106, duration: 0.282s, episode steps:  38, steps per second: 135, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5965.773055, mae: 753.847488, mean_q: 1567.640362, mean_tau: 0.048095\n",
            " 26252/50000: episode: 1107, duration: 0.152s, episode steps:  18, steps per second: 119, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9245.945740, mae: 747.172733, mean_q: 1545.896342, mean_tau: 0.048040\n",
            " 26263/50000: episode: 1108, duration: 0.093s, episode steps:  11, steps per second: 119, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5984.610884, mae: 736.807734, mean_q: 1531.889782, mean_tau: 0.048011\n",
            " 26289/50000: episode: 1109, duration: 0.197s, episode steps:  26, steps per second: 132, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7027.786044, mae: 761.318451, mean_q: 1581.376470, mean_tau: 0.047975\n",
            " 26315/50000: episode: 1110, duration: 0.209s, episode steps:  26, steps per second: 124, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5818.354441, mae: 758.451634, mean_q: 1577.975332, mean_tau: 0.047923\n",
            " 26330/50000: episode: 1111, duration: 0.121s, episode steps:  15, steps per second: 124, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 4630.970190, mae: 780.935331, mean_q: 1611.347437, mean_tau: 0.047882\n",
            " 26343/50000: episode: 1112, duration: 0.111s, episode steps:  13, steps per second: 117, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5292.626897, mae: 761.600079, mean_q: 1603.225032, mean_tau: 0.047855\n",
            " 26378/50000: episode: 1113, duration: 0.262s, episode steps:  35, steps per second: 134, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 10166.590055, mae: 755.816319, mean_q: 1586.382192, mean_tau: 0.047807\n",
            " 26392/50000: episode: 1114, duration: 0.102s, episode steps:  14, steps per second: 137, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4838.107806, mae: 763.863652, mean_q: 1592.705296, mean_tau: 0.047759\n",
            " 26409/50000: episode: 1115, duration: 0.134s, episode steps:  17, steps per second: 127, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 9557.089047, mae: 773.492561, mean_q: 1613.141135, mean_tau: 0.047728\n",
            " 26419/50000: episode: 1116, duration: 0.080s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 10767.789697, mae: 760.353864, mean_q: 1587.941870, mean_tau: 0.047701\n",
            " 26438/50000: episode: 1117, duration: 0.161s, episode steps:  19, steps per second: 118, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 7785.422607, mae: 764.596342, mean_q: 1590.496923, mean_tau: 0.047673\n",
            " 26463/50000: episode: 1118, duration: 0.184s, episode steps:  25, steps per second: 136, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 5587.008584, mae: 748.798169, mean_q: 1572.313169, mean_tau: 0.047629\n",
            " 26506/50000: episode: 1119, duration: 0.328s, episode steps:  43, steps per second: 131, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 6702.696800, mae: 758.446254, mean_q: 1581.890443, mean_tau: 0.047562\n",
            " 26519/50000: episode: 1120, duration: 0.103s, episode steps:  13, steps per second: 126, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 21871.834585, mae: 766.906785, mean_q: 1569.610906, mean_tau: 0.047506\n",
            " 26542/50000: episode: 1121, duration: 0.250s, episode steps:  23, steps per second:  92, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 10764.292618, mae: 762.066664, mean_q: 1579.496885, mean_tau: 0.047471\n",
            " 26565/50000: episode: 1122, duration: 0.267s, episode steps:  23, steps per second:  86, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6151.313381, mae: 736.123872, mean_q: 1526.715523, mean_tau: 0.047425\n",
            " 26576/50000: episode: 1123, duration: 0.130s, episode steps:  11, steps per second:  85, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 12502.131281, mae: 772.955816, mean_q: 1589.681130, mean_tau: 0.047391\n",
            " 26594/50000: episode: 1124, duration: 0.208s, episode steps:  18, steps per second:  87, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6793.662001, mae: 790.762743, mean_q: 1645.678996, mean_tau: 0.047363\n",
            " 26604/50000: episode: 1125, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 11867.867017, mae: 775.756799, mean_q: 1605.080493, mean_tau: 0.047335\n",
            " 26624/50000: episode: 1126, duration: 0.229s, episode steps:  20, steps per second:  87, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 7198.399622, mae: 787.096307, mean_q: 1646.840674, mean_tau: 0.047305\n",
            " 26648/50000: episode: 1127, duration: 0.274s, episode steps:  24, steps per second:  88, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7808.656509, mae: 768.246541, mean_q: 1576.615077, mean_tau: 0.047262\n",
            " 26663/50000: episode: 1128, duration: 0.171s, episode steps:  15, steps per second:  88, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 9432.770898, mae: 743.230212, mean_q: 1539.070654, mean_tau: 0.047223\n",
            " 26698/50000: episode: 1129, duration: 0.392s, episode steps:  35, steps per second:  89, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 9629.472461, mae: 780.194756, mean_q: 1605.654510, mean_tau: 0.047174\n",
            " 26718/50000: episode: 1130, duration: 0.233s, episode steps:  20, steps per second:  86, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 12259.276331, mae: 748.269623, mean_q: 1561.166449, mean_tau: 0.047119\n",
            " 26734/50000: episode: 1131, duration: 0.194s, episode steps:  16, steps per second:  82, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8597.217232, mae: 767.486492, mean_q: 1577.019814, mean_tau: 0.047084\n",
            " 26771/50000: episode: 1132, duration: 0.428s, episode steps:  37, steps per second:  86, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 8182.299636, mae: 763.714783, mean_q: 1592.184983, mean_tau: 0.047031\n",
            " 26816/50000: episode: 1133, duration: 0.376s, episode steps:  45, steps per second: 120, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 7584.680063, mae: 775.202734, mean_q: 1604.110463, mean_tau: 0.046950\n",
            " 26830/50000: episode: 1134, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7186.904637, mae: 764.132102, mean_q: 1573.222935, mean_tau: 0.046891\n",
            " 26847/50000: episode: 1135, duration: 0.162s, episode steps:  17, steps per second: 105, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 5941.770903, mae: 772.417366, mean_q: 1597.899665, mean_tau: 0.046861\n",
            " 26876/50000: episode: 1136, duration: 0.229s, episode steps:  29, steps per second: 126, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 7241.048984, mae: 785.922117, mean_q: 1624.023530, mean_tau: 0.046815\n",
            " 26900/50000: episode: 1137, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5933.457225, mae: 768.779846, mean_q: 1609.288401, mean_tau: 0.046763\n",
            " 26914/50000: episode: 1138, duration: 0.118s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 9640.044625, mae: 779.343070, mean_q: 1595.492231, mean_tau: 0.046725\n",
            " 26942/50000: episode: 1139, duration: 0.217s, episode steps:  28, steps per second: 129, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6616.485003, mae: 775.438389, mean_q: 1615.287406, mean_tau: 0.046684\n",
            " 26960/50000: episode: 1140, duration: 0.137s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 5592.546882, mae: 783.908440, mean_q: 1616.560140, mean_tau: 0.046638\n",
            " 26969/50000: episode: 1141, duration: 0.100s, episode steps:   9, steps per second:  90, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 5389.549906, mae: 782.281623, mean_q: 1645.722114, mean_tau: 0.046611\n",
            " 26979/50000: episode: 1142, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 12504.932690, mae: 807.244788, mean_q: 1627.189490, mean_tau: 0.046592\n",
            " 27018/50000: episode: 1143, duration: 0.297s, episode steps:  39, steps per second: 131, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 7850.259387, mae: 781.412127, mean_q: 1608.556024, mean_tau: 0.046544\n",
            " 27056/50000: episode: 1144, duration: 0.286s, episode steps:  38, steps per second: 133, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 7657.330640, mae: 786.584253, mean_q: 1624.608739, mean_tau: 0.046468\n",
            " 27067/50000: episode: 1145, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 6377.354659, mae: 784.097418, mean_q: 1638.546731, mean_tau: 0.046419\n",
            " 27086/50000: episode: 1146, duration: 0.141s, episode steps:  19, steps per second: 134, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6675.991481, mae: 794.422415, mean_q: 1638.381033, mean_tau: 0.046390\n",
            " 27105/50000: episode: 1147, duration: 0.148s, episode steps:  19, steps per second: 128, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4386.207275, mae: 803.619215, mean_q: 1653.521870, mean_tau: 0.046352\n",
            " 27126/50000: episode: 1148, duration: 0.166s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 8862.843448, mae: 766.668044, mean_q: 1585.924776, mean_tau: 0.046312\n",
            " 27170/50000: episode: 1149, duration: 0.339s, episode steps:  44, steps per second: 130, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5704.027333, mae: 787.178496, mean_q: 1622.006323, mean_tau: 0.046248\n",
            " 27192/50000: episode: 1150, duration: 0.169s, episode steps:  22, steps per second: 130, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6620.640181, mae: 774.357999, mean_q: 1592.715787, mean_tau: 0.046183\n",
            " 27212/50000: episode: 1151, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 5927.206638, mae: 784.680347, mean_q: 1618.008136, mean_tau: 0.046141\n",
            " 27229/50000: episode: 1152, duration: 0.142s, episode steps:  17, steps per second: 120, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 9306.112125, mae: 777.552393, mean_q: 1599.507087, mean_tau: 0.046104\n",
            " 27243/50000: episode: 1153, duration: 0.118s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12935.524109, mae: 789.384940, mean_q: 1624.732797, mean_tau: 0.046074\n",
            " 27252/50000: episode: 1154, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 11360.018351, mae: 810.103326, mean_q: 1644.993462, mean_tau: 0.046051\n",
            " 27275/50000: episode: 1155, duration: 0.194s, episode steps:  23, steps per second: 119, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5194.281680, mae: 801.378991, mean_q: 1675.555298, mean_tau: 0.046019\n",
            " 27293/50000: episode: 1156, duration: 0.136s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6740.464138, mae: 765.956662, mean_q: 1598.290188, mean_tau: 0.045979\n",
            " 27309/50000: episode: 1157, duration: 0.128s, episode steps:  16, steps per second: 125, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5271.215706, mae: 791.764725, mean_q: 1651.813583, mean_tau: 0.045945\n",
            " 27319/50000: episode: 1158, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4877.371387, mae: 798.932770, mean_q: 1642.100769, mean_tau: 0.045919\n",
            " 27331/50000: episode: 1159, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9573.812281, mae: 752.896367, mean_q: 1545.513784, mean_tau: 0.045897\n",
            " 27356/50000: episode: 1160, duration: 0.214s, episode steps:  25, steps per second: 117, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 7098.516294, mae: 790.336733, mean_q: 1642.590576, mean_tau: 0.045861\n",
            " 27409/50000: episode: 1161, duration: 0.406s, episode steps:  53, steps per second: 131, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 7767.781100, mae: 787.744881, mean_q: 1632.558965, mean_tau: 0.045784\n",
            " 27419/50000: episode: 1162, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 10525.671765, mae: 757.745996, mean_q: 1575.148022, mean_tau: 0.045721\n",
            " 27462/50000: episode: 1163, duration: 0.330s, episode steps:  43, steps per second: 130, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 8942.318578, mae: 809.502697, mean_q: 1674.019466, mean_tau: 0.045669\n",
            " 27486/50000: episode: 1164, duration: 0.201s, episode steps:  24, steps per second: 120, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 8765.104797, mae: 789.547473, mean_q: 1636.422465, mean_tau: 0.045602\n",
            " 27544/50000: episode: 1165, duration: 0.484s, episode steps:  58, steps per second: 120, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 6663.473057, mae: 793.132374, mean_q: 1641.703100, mean_tau: 0.045521\n",
            " 27566/50000: episode: 1166, duration: 0.174s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6463.179277, mae: 795.799280, mean_q: 1645.062916, mean_tau: 0.045442\n",
            " 27582/50000: episode: 1167, duration: 0.131s, episode steps:  16, steps per second: 122, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 6940.511070, mae: 808.270035, mean_q: 1679.933441, mean_tau: 0.045404\n",
            " 27594/50000: episode: 1168, duration: 0.102s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 8526.335678, mae: 784.363663, mean_q: 1637.474915, mean_tau: 0.045377\n",
            " 27609/50000: episode: 1169, duration: 0.131s, episode steps:  15, steps per second: 115, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3699.085213, mae: 812.817936, mean_q: 1679.235368, mean_tau: 0.045350\n",
            " 27626/50000: episode: 1170, duration: 0.133s, episode steps:  17, steps per second: 128, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 9720.330976, mae: 811.002018, mean_q: 1668.705760, mean_tau: 0.045318\n",
            " 27640/50000: episode: 1171, duration: 0.134s, episode steps:  14, steps per second: 104, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 9282.411063, mae: 803.198486, mean_q: 1660.595895, mean_tau: 0.045288\n",
            " 27654/50000: episode: 1172, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7528.842032, mae: 799.405945, mean_q: 1667.378653, mean_tau: 0.045260\n",
            " 27665/50000: episode: 1173, duration: 0.089s, episode steps:  11, steps per second: 124, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 5184.960971, mae: 791.188909, mean_q: 1656.539762, mean_tau: 0.045235\n",
            " 27678/50000: episode: 1174, duration: 0.110s, episode steps:  13, steps per second: 118, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 5180.134728, mae: 815.010245, mean_q: 1703.998084, mean_tau: 0.045211\n",
            " 27699/50000: episode: 1175, duration: 0.163s, episode steps:  21, steps per second: 129, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 9184.646897, mae: 806.115891, mean_q: 1658.989589, mean_tau: 0.045178\n",
            " 27709/50000: episode: 1176, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 7719.307251, mae: 782.472058, mean_q: 1629.322034, mean_tau: 0.045147\n",
            " 27736/50000: episode: 1177, duration: 0.230s, episode steps:  27, steps per second: 118, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 6529.915536, mae: 792.419425, mean_q: 1640.522705, mean_tau: 0.045110\n",
            " 27752/50000: episode: 1178, duration: 0.123s, episode steps:  16, steps per second: 130, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 9965.584816, mae: 786.506889, mean_q: 1634.460030, mean_tau: 0.045068\n",
            " 27770/50000: episode: 1179, duration: 0.147s, episode steps:  18, steps per second: 122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6044.637465, mae: 797.335880, mean_q: 1666.585164, mean_tau: 0.045034\n",
            " 27799/50000: episode: 1180, duration: 0.221s, episode steps:  29, steps per second: 131, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6067.580869, mae: 825.912288, mean_q: 1704.916521, mean_tau: 0.044988\n",
            " 27830/50000: episode: 1181, duration: 0.238s, episode steps:  31, steps per second: 130, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 6910.958374, mae: 801.485113, mean_q: 1655.163625, mean_tau: 0.044928\n",
            " 27841/50000: episode: 1182, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 6451.487038, mae: 792.110429, mean_q: 1657.457353, mean_tau: 0.044887\n",
            " 27856/50000: episode: 1183, duration: 0.110s, episode steps:  15, steps per second: 136, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 7491.798242, mae: 798.596769, mean_q: 1645.808293, mean_tau: 0.044861\n",
            " 27890/50000: episode: 1184, duration: 0.266s, episode steps:  34, steps per second: 128, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6871.101742, mae: 808.396299, mean_q: 1661.990382, mean_tau: 0.044812\n",
            " 27912/50000: episode: 1185, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5711.703597, mae: 820.275088, mean_q: 1691.884105, mean_tau: 0.044757\n",
            " 27936/50000: episode: 1186, duration: 0.185s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6439.605153, mae: 832.777723, mean_q: 1715.352783, mean_tau: 0.044711\n",
            " 27958/50000: episode: 1187, duration: 0.158s, episode steps:  22, steps per second: 139, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7890.323176, mae: 826.142478, mean_q: 1716.906056, mean_tau: 0.044666\n",
            " 27970/50000: episode: 1188, duration: 0.111s, episode steps:  12, steps per second: 108, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 8487.934652, mae: 819.903371, mean_q: 1694.151031, mean_tau: 0.044632\n",
            " 28036/50000: episode: 1189, duration: 0.530s, episode steps:  66, steps per second: 124, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8096.047637, mae: 813.740448, mean_q: 1677.522849, mean_tau: 0.044555\n",
            " 28057/50000: episode: 1190, duration: 0.240s, episode steps:  21, steps per second:  87, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 9379.495960, mae: 840.177947, mean_q: 1720.557454, mean_tau: 0.044469\n",
            " 28079/50000: episode: 1191, duration: 0.263s, episode steps:  22, steps per second:  84, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4556.913225, mae: 828.929965, mean_q: 1718.216869, mean_tau: 0.044426\n",
            " 28134/50000: episode: 1192, duration: 0.622s, episode steps:  55, steps per second:  88, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 8552.858849, mae: 817.186446, mean_q: 1681.246930, mean_tau: 0.044350\n",
            " 28150/50000: episode: 1193, duration: 0.179s, episode steps:  16, steps per second:  89, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 9826.914055, mae: 830.896973, mean_q: 1727.440872, mean_tau: 0.044280\n",
            " 28165/50000: episode: 1194, duration: 0.189s, episode steps:  15, steps per second:  79, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 5341.029443, mae: 838.127507, mean_q: 1726.015389, mean_tau: 0.044249\n",
            " 28181/50000: episode: 1195, duration: 0.183s, episode steps:  16, steps per second:  87, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 7073.723991, mae: 814.443222, mean_q: 1682.132271, mean_tau: 0.044218\n",
            " 28207/50000: episode: 1196, duration: 0.315s, episode steps:  26, steps per second:  83, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5731.432272, mae: 827.663034, mean_q: 1723.211609, mean_tau: 0.044177\n",
            " 28249/50000: episode: 1197, duration: 0.467s, episode steps:  42, steps per second:  90, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 9206.830441, mae: 828.962718, mean_q: 1712.468113, mean_tau: 0.044110\n",
            " 28263/50000: episode: 1198, duration: 0.163s, episode steps:  14, steps per second:  86, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 8729.558908, mae: 807.378662, mean_q: 1675.834821, mean_tau: 0.044054\n",
            " 28279/50000: episode: 1199, duration: 0.193s, episode steps:  16, steps per second:  83, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 8477.449142, mae: 820.229221, mean_q: 1688.894249, mean_tau: 0.044024\n",
            " 28330/50000: episode: 1200, duration: 0.411s, episode steps:  51, steps per second: 124, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 8062.094439, mae: 829.615007, mean_q: 1717.470483, mean_tau: 0.043958\n",
            " 28377/50000: episode: 1201, duration: 0.361s, episode steps:  47, steps per second: 130, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 7241.935344, mae: 816.960692, mean_q: 1692.091436, mean_tau: 0.043861\n",
            " 28395/50000: episode: 1202, duration: 0.138s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 7988.679681, mae: 817.490794, mean_q: 1701.043572, mean_tau: 0.043797\n",
            " 28429/50000: episode: 1203, duration: 0.270s, episode steps:  34, steps per second: 126, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 6108.490360, mae: 839.820959, mean_q: 1748.384069, mean_tau: 0.043745\n",
            " 28455/50000: episode: 1204, duration: 0.186s, episode steps:  26, steps per second: 139, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.346 [0.000, 1.000],  loss: 10933.675993, mae: 830.114436, mean_q: 1714.103933, mean_tau: 0.043686\n",
            " 28466/50000: episode: 1205, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3916.433350, mae: 863.326383, mean_q: 1790.591586, mean_tau: 0.043649\n",
            " 28476/50000: episode: 1206, duration: 0.078s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 8752.037305, mae: 841.757788, mean_q: 1747.046448, mean_tau: 0.043628\n",
            " 28508/50000: episode: 1207, duration: 0.237s, episode steps:  32, steps per second: 135, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8330.807652, mae: 834.027691, mean_q: 1733.988056, mean_tau: 0.043587\n",
            " 28528/50000: episode: 1208, duration: 0.151s, episode steps:  20, steps per second: 133, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 7022.528613, mae: 836.915829, mean_q: 1744.972876, mean_tau: 0.043535\n",
            " 28555/50000: episode: 1209, duration: 0.218s, episode steps:  27, steps per second: 124, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 8677.582655, mae: 821.277147, mean_q: 1711.407380, mean_tau: 0.043489\n",
            " 28571/50000: episode: 1210, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6570.367424, mae: 858.842632, mean_q: 1774.394310, mean_tau: 0.043446\n",
            " 28588/50000: episode: 1211, duration: 0.142s, episode steps:  17, steps per second: 120, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4884.106862, mae: 836.568270, mean_q: 1748.972089, mean_tau: 0.043414\n",
            " 28603/50000: episode: 1212, duration: 0.114s, episode steps:  15, steps per second: 131, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6626.496122, mae: 869.335986, mean_q: 1806.722909, mean_tau: 0.043382\n",
            " 28615/50000: episode: 1213, duration: 0.095s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5551.762695, mae: 830.225077, mean_q: 1742.558868, mean_tau: 0.043355\n",
            " 28683/50000: episode: 1214, duration: 0.533s, episode steps:  68, steps per second: 128, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10784.042351, mae: 831.339979, mean_q: 1731.958713, mean_tau: 0.043276\n",
            " 28697/50000: episode: 1215, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5768.860944, mae: 843.675759, mean_q: 1729.455296, mean_tau: 0.043195\n",
            " 28707/50000: episode: 1216, duration: 0.079s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4790.372821, mae: 866.258850, mean_q: 1783.885925, mean_tau: 0.043171\n",
            " 28726/50000: episode: 1217, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4352.418528, mae: 855.697770, mean_q: 1763.969258, mean_tau: 0.043142\n",
            " 28783/50000: episode: 1218, duration: 0.420s, episode steps:  57, steps per second: 136, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 10371.019889, mae: 855.382063, mean_q: 1766.788028, mean_tau: 0.043067\n",
            " 28797/50000: episode: 1219, duration: 0.120s, episode steps:  14, steps per second: 116, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 11574.905491, mae: 876.019440, mean_q: 1803.781974, mean_tau: 0.042997\n",
            " 28818/50000: episode: 1220, duration: 0.167s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5972.932431, mae: 876.840361, mean_q: 1797.493437, mean_tau: 0.042962\n",
            " 28846/50000: episode: 1221, duration: 0.218s, episode steps:  28, steps per second: 129, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 8563.355776, mae: 875.319711, mean_q: 1802.775905, mean_tau: 0.042914\n",
            " 28865/50000: episode: 1222, duration: 0.140s, episode steps:  19, steps per second: 135, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 10031.853027, mae: 854.782069, mean_q: 1758.437796, mean_tau: 0.042867\n",
            " 28880/50000: episode: 1223, duration: 0.121s, episode steps:  15, steps per second: 123, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 8228.125146, mae: 871.667700, mean_q: 1787.220369, mean_tau: 0.042833\n",
            " 28904/50000: episode: 1224, duration: 0.178s, episode steps:  24, steps per second: 135, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 14424.186422, mae: 858.647456, mean_q: 1774.464096, mean_tau: 0.042795\n",
            " 28931/50000: episode: 1225, duration: 0.228s, episode steps:  27, steps per second: 118, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 8115.830508, mae: 871.808824, mean_q: 1801.391240, mean_tau: 0.042744\n",
            " 28943/50000: episode: 1226, duration: 0.089s, episode steps:  12, steps per second: 134, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 14420.426432, mae: 853.444855, mean_q: 1751.251617, mean_tau: 0.042706\n",
            " 28967/50000: episode: 1227, duration: 0.183s, episode steps:  24, steps per second: 131, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4868.620305, mae: 861.506813, mean_q: 1776.471634, mean_tau: 0.042670\n",
            " 28996/50000: episode: 1228, duration: 0.213s, episode steps:  29, steps per second: 136, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 5201.229551, mae: 859.492804, mean_q: 1793.172772, mean_tau: 0.042618\n",
            " 29019/50000: episode: 1229, duration: 0.171s, episode steps:  23, steps per second: 135, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6773.010615, mae: 876.290137, mean_q: 1805.446257, mean_tau: 0.042566\n",
            " 29047/50000: episode: 1230, duration: 0.211s, episode steps:  28, steps per second: 133, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 8131.364323, mae: 849.836428, mean_q: 1745.540750, mean_tau: 0.042516\n",
            " 29067/50000: episode: 1231, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15219.671490, mae: 879.905020, mean_q: 1800.837677, mean_tau: 0.042468\n",
            " 29081/50000: episode: 1232, duration: 0.116s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8307.367135, mae: 877.612784, mean_q: 1820.483939, mean_tau: 0.042434\n",
            " 29098/50000: episode: 1233, duration: 0.141s, episode steps:  17, steps per second: 120, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6453.115866, mae: 858.900674, mean_q: 1792.745555, mean_tau: 0.042404\n",
            " 29124/50000: episode: 1234, duration: 0.195s, episode steps:  26, steps per second: 134, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8196.278602, mae: 872.288734, mean_q: 1811.274353, mean_tau: 0.042361\n",
            " 29142/50000: episode: 1235, duration: 0.137s, episode steps:  18, steps per second: 132, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11486.680556, mae: 854.832737, mean_q: 1758.259467, mean_tau: 0.042318\n",
            " 29160/50000: episode: 1236, duration: 0.151s, episode steps:  18, steps per second: 119, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8979.413188, mae: 887.492310, mean_q: 1815.292386, mean_tau: 0.042282\n",
            " 29175/50000: episode: 1237, duration: 0.114s, episode steps:  15, steps per second: 131, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 6425.532235, mae: 894.767692, mean_q: 1843.482731, mean_tau: 0.042249\n",
            " 29186/50000: episode: 1238, duration: 0.082s, episode steps:  11, steps per second: 134, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 19304.235396, mae: 867.726845, mean_q: 1756.421797, mean_tau: 0.042224\n",
            " 29221/50000: episode: 1239, duration: 0.274s, episode steps:  35, steps per second: 128, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 7853.005992, mae: 857.268687, mean_q: 1774.161482, mean_tau: 0.042178\n",
            " 29236/50000: episode: 1240, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6867.937606, mae: 874.674581, mean_q: 1819.946924, mean_tau: 0.042129\n",
            " 29304/50000: episode: 1241, duration: 0.498s, episode steps:  68, steps per second: 137, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 9764.732849, mae: 864.907999, mean_q: 1785.646535, mean_tau: 0.042046\n",
            " 29352/50000: episode: 1242, duration: 0.380s, episode steps:  48, steps per second: 126, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 7951.877022, mae: 883.813405, mean_q: 1837.465744, mean_tau: 0.041932\n",
            " 29370/50000: episode: 1243, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13317.607625, mae: 872.751007, mean_q: 1816.899292, mean_tau: 0.041866\n",
            " 29403/50000: episode: 1244, duration: 0.282s, episode steps:  33, steps per second: 117, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 8409.393858, mae: 877.890910, mean_q: 1821.832501, mean_tau: 0.041816\n",
            " 29417/50000: episode: 1245, duration: 0.110s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6426.435128, mae: 873.191812, mean_q: 1807.262451, mean_tau: 0.041769\n",
            " 29428/50000: episode: 1246, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 8733.286377, mae: 875.571677, mean_q: 1819.377164, mean_tau: 0.041744\n",
            " 29456/50000: episode: 1247, duration: 0.233s, episode steps:  28, steps per second: 120, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 12574.255114, mae: 904.746102, mean_q: 1859.965620, mean_tau: 0.041706\n",
            " 29477/50000: episode: 1248, duration: 0.166s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 5495.148368, mae: 889.665327, mean_q: 1855.748896, mean_tau: 0.041657\n",
            " 29496/50000: episode: 1249, duration: 0.151s, episode steps:  19, steps per second: 126, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 9502.936793, mae: 887.167153, mean_q: 1818.800499, mean_tau: 0.041618\n",
            " 29521/50000: episode: 1250, duration: 0.180s, episode steps:  25, steps per second: 139, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 11252.283081, mae: 894.766528, mean_q: 1837.540220, mean_tau: 0.041574\n",
            " 29535/50000: episode: 1251, duration: 0.117s, episode steps:  14, steps per second: 120, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11282.206752, mae: 887.163212, mean_q: 1815.410662, mean_tau: 0.041536\n",
            " 29544/50000: episode: 1252, duration: 0.071s, episode steps:   9, steps per second: 126, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 8925.292969, mae: 880.594950, mean_q: 1820.904622, mean_tau: 0.041513\n",
            " 29558/50000: episode: 1253, duration: 0.155s, episode steps:  14, steps per second:  91, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8890.647714, mae: 881.989493, mean_q: 1816.924874, mean_tau: 0.041490\n",
            " 29579/50000: episode: 1254, duration: 0.273s, episode steps:  21, steps per second:  77, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 8381.437488, mae: 890.224792, mean_q: 1844.727277, mean_tau: 0.041455\n",
            " 29599/50000: episode: 1255, duration: 0.227s, episode steps:  20, steps per second:  88, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 9684.855524, mae: 881.662393, mean_q: 1820.442999, mean_tau: 0.041415\n",
            " 29614/50000: episode: 1256, duration: 0.181s, episode steps:  15, steps per second:  83, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 6014.048893, mae: 891.943970, mean_q: 1843.710742, mean_tau: 0.041380\n",
            " 29630/50000: episode: 1257, duration: 0.185s, episode steps:  16, steps per second:  86, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 6591.487236, mae: 893.173672, mean_q: 1831.998566, mean_tau: 0.041349\n",
            " 29643/50000: episode: 1258, duration: 0.152s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 9547.615741, mae: 890.948838, mean_q: 1846.056350, mean_tau: 0.041321\n",
            " 29664/50000: episode: 1259, duration: 0.274s, episode steps:  21, steps per second:  77, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 12445.106596, mae: 908.580575, mean_q: 1857.589082, mean_tau: 0.041287\n",
            " 29674/50000: episode: 1260, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 9898.665417, mae: 884.522150, mean_q: 1817.543994, mean_tau: 0.041256\n",
            " 29690/50000: episode: 1261, duration: 0.194s, episode steps:  16, steps per second:  83, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 9944.693825, mae: 863.598988, mean_q: 1802.621887, mean_tau: 0.041231\n",
            " 29710/50000: episode: 1262, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 8392.594202, mae: 889.011407, mean_q: 1833.304974, mean_tau: 0.041195\n",
            " 29809/50000: episode: 1263, duration: 1.056s, episode steps:  99, steps per second:  94, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7684.631655, mae: 887.803514, mean_q: 1821.053664, mean_tau: 0.041077\n",
            " 29819/50000: episode: 1264, duration: 0.076s, episode steps:  10, steps per second: 132, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 10129.078760, mae: 908.759900, mean_q: 1836.767944, mean_tau: 0.040969\n",
            " 29829/50000: episode: 1265, duration: 0.083s, episode steps:  10, steps per second: 121, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 14199.284131, mae: 865.003717, mean_q: 1762.299658, mean_tau: 0.040949\n",
            " 29839/50000: episode: 1266, duration: 0.086s, episode steps:  10, steps per second: 116, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 10095.320703, mae: 916.244257, mean_q: 1865.342957, mean_tau: 0.040930\n",
            " 29884/50000: episode: 1267, duration: 0.370s, episode steps:  45, steps per second: 122, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 7160.708016, mae: 909.864861, mean_q: 1862.393286, mean_tau: 0.040875\n",
            " 29949/50000: episode: 1268, duration: 0.492s, episode steps:  65, steps per second: 132, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 11164.429017, mae: 901.603844, mean_q: 1844.079133, mean_tau: 0.040766\n",
            " 29976/50000: episode: 1269, duration: 0.203s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 5333.943454, mae: 895.824983, mean_q: 1840.922693, mean_tau: 0.040675\n",
            " 29996/50000: episode: 1270, duration: 0.166s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 8684.660889, mae: 895.399356, mean_q: 1842.258307, mean_tau: 0.040629\n",
            " 30019/50000: episode: 1271, duration: 0.194s, episode steps:  23, steps per second: 118, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 8891.935711, mae: 902.744658, mean_q: 1846.703958, mean_tau: 0.040586\n",
            " 30064/50000: episode: 1272, duration: 0.347s, episode steps:  45, steps per second: 130, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 7408.943289, mae: 887.509261, mean_q: 1822.279614, mean_tau: 0.040519\n",
            " 30113/50000: episode: 1273, duration: 0.363s, episode steps:  49, steps per second: 135, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5986.647755, mae: 891.309107, mean_q: 1837.024095, mean_tau: 0.040426\n",
            " 30156/50000: episode: 1274, duration: 0.326s, episode steps:  43, steps per second: 132, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 9890.897691, mae: 901.360122, mean_q: 1840.086400, mean_tau: 0.040335\n",
            " 30206/50000: episode: 1275, duration: 0.365s, episode steps:  50, steps per second: 137, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 9336.636521, mae: 900.282567, mean_q: 1847.107354, mean_tau: 0.040243\n",
            " 30253/50000: episode: 1276, duration: 0.343s, episode steps:  47, steps per second: 137, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 7127.204964, mae: 913.873733, mean_q: 1883.169987, mean_tau: 0.040147\n",
            " 30268/50000: episode: 1277, duration: 0.116s, episode steps:  15, steps per second: 130, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 5230.199886, mae: 942.999638, mean_q: 1948.204272, mean_tau: 0.040085\n",
            " 30294/50000: episode: 1278, duration: 0.206s, episode steps:  26, steps per second: 126, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7866.339909, mae: 916.695629, mean_q: 1883.482004, mean_tau: 0.040045\n",
            " 30309/50000: episode: 1279, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 10323.048096, mae: 893.428121, mean_q: 1851.308024, mean_tau: 0.040004\n",
            " 30323/50000: episode: 1280, duration: 0.118s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 19632.251421, mae: 927.911229, mean_q: 1873.386300, mean_tau: 0.039975\n",
            " 30346/50000: episode: 1281, duration: 0.178s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 8609.696241, mae: 891.750748, mean_q: 1846.710985, mean_tau: 0.039939\n",
            " 30368/50000: episode: 1282, duration: 0.179s, episode steps:  22, steps per second: 123, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4287.943731, mae: 928.245253, mean_q: 1905.606878, mean_tau: 0.039894\n",
            " 30392/50000: episode: 1283, duration: 0.192s, episode steps:  24, steps per second: 125, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 7901.046941, mae: 924.288172, mean_q: 1894.481659, mean_tau: 0.039849\n",
            " 30412/50000: episode: 1284, duration: 0.176s, episode steps:  20, steps per second: 113, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11889.104529, mae: 918.830933, mean_q: 1882.205200, mean_tau: 0.039805\n",
            " 30440/50000: episode: 1285, duration: 0.222s, episode steps:  28, steps per second: 126, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 9344.100909, mae: 884.990718, mean_q: 1834.200500, mean_tau: 0.039758\n",
            " 30462/50000: episode: 1286, duration: 0.172s, episode steps:  22, steps per second: 128, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11388.201788, mae: 894.726010, mean_q: 1828.640470, mean_tau: 0.039708\n",
            " 30477/50000: episode: 1287, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 8760.364925, mae: 877.909668, mean_q: 1804.739958, mean_tau: 0.039671\n",
            " 30525/50000: episode: 1288, duration: 0.370s, episode steps:  48, steps per second: 130, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 10019.845965, mae: 916.247260, mean_q: 1889.170728, mean_tau: 0.039609\n",
            " 30547/50000: episode: 1289, duration: 0.176s, episode steps:  22, steps per second: 125, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8525.118885, mae: 916.463617, mean_q: 1900.759327, mean_tau: 0.039540\n",
            " 30565/50000: episode: 1290, duration: 0.155s, episode steps:  18, steps per second: 116, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 7162.351468, mae: 912.297567, mean_q: 1867.300334, mean_tau: 0.039500\n",
            " 30588/50000: episode: 1291, duration: 0.177s, episode steps:  23, steps per second: 130, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 8197.676991, mae: 916.188776, mean_q: 1883.771426, mean_tau: 0.039460\n",
            " 30614/50000: episode: 1292, duration: 0.215s, episode steps:  26, steps per second: 121, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 8540.696134, mae: 889.950806, mean_q: 1835.866042, mean_tau: 0.039411\n",
            " 30626/50000: episode: 1293, duration: 0.093s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 6161.952861, mae: 878.868449, mean_q: 1835.209768, mean_tau: 0.039373\n",
            " 30650/50000: episode: 1294, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 12607.501104, mae: 921.415080, mean_q: 1880.036285, mean_tau: 0.039338\n",
            " 30699/50000: episode: 1295, duration: 0.370s, episode steps:  49, steps per second: 132, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 7060.581291, mae: 926.637490, mean_q: 1899.790719, mean_tau: 0.039265\n",
            " 30731/50000: episode: 1296, duration: 0.240s, episode steps:  32, steps per second: 133, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 11763.161541, mae: 922.330976, mean_q: 1896.829884, mean_tau: 0.039185\n",
            " 30742/50000: episode: 1297, duration: 0.087s, episode steps:  11, steps per second: 126, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 8238.052801, mae: 912.456898, mean_q: 1873.228571, mean_tau: 0.039143\n",
            " 30770/50000: episode: 1298, duration: 0.218s, episode steps:  28, steps per second: 128, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 9902.789211, mae: 919.286157, mean_q: 1897.806192, mean_tau: 0.039104\n",
            " 30782/50000: episode: 1299, duration: 0.095s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 8798.817098, mae: 927.055908, mean_q: 1882.135946, mean_tau: 0.039065\n",
            " 30802/50000: episode: 1300, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6388.513989, mae: 917.368900, mean_q: 1883.025726, mean_tau: 0.039033\n",
            " 30821/50000: episode: 1301, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 12726.810213, mae: 931.438181, mean_q: 1898.017630, mean_tau: 0.038994\n",
            " 30856/50000: episode: 1302, duration: 0.274s, episode steps:  35, steps per second: 128, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 17336.954206, mae: 921.580265, mean_q: 1869.802710, mean_tau: 0.038941\n",
            " 30873/50000: episode: 1303, duration: 0.134s, episode steps:  17, steps per second: 127, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5270.903019, mae: 925.979086, mean_q: 1915.401504, mean_tau: 0.038889\n",
            " 30900/50000: episode: 1304, duration: 0.228s, episode steps:  27, steps per second: 118, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 8406.339100, mae: 944.983717, mean_q: 1965.004711, mean_tau: 0.038846\n",
            " 30923/50000: episode: 1305, duration: 0.170s, episode steps:  23, steps per second: 135, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 9464.894054, mae: 938.597404, mean_q: 1926.645550, mean_tau: 0.038796\n",
            " 30940/50000: episode: 1306, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 10683.415283, mae: 910.811430, mean_q: 1873.316485, mean_tau: 0.038757\n",
            " 30970/50000: episode: 1307, duration: 0.221s, episode steps:  30, steps per second: 136, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10345.489213, mae: 934.899422, mean_q: 1914.075903, mean_tau: 0.038710\n",
            " 30991/50000: episode: 1308, duration: 0.160s, episode steps:  21, steps per second: 131, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 8308.500110, mae: 943.466573, mean_q: 1922.497216, mean_tau: 0.038660\n",
            " 31051/50000: episode: 1309, duration: 0.437s, episode steps:  60, steps per second: 137, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 10187.966854, mae: 926.568678, mean_q: 1913.102435, mean_tau: 0.038579\n",
            " 31061/50000: episode: 1310, duration: 0.079s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 9223.771851, mae: 958.329980, mean_q: 1969.410132, mean_tau: 0.038510\n",
            " 31081/50000: episode: 1311, duration: 0.202s, episode steps:  20, steps per second:  99, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 13603.571436, mae: 932.366241, mean_q: 1923.946393, mean_tau: 0.038480\n",
            " 31113/50000: episode: 1312, duration: 0.378s, episode steps:  32, steps per second:  85, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16416.445347, mae: 942.212830, mean_q: 1933.054344, mean_tau: 0.038429\n",
            " 31127/50000: episode: 1313, duration: 0.160s, episode steps:  14, steps per second:  88, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7936.760847, mae: 969.529903, mean_q: 2002.909372, mean_tau: 0.038383\n",
            " 31143/50000: episode: 1314, duration: 0.211s, episode steps:  16, steps per second:  76, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 14083.453812, mae: 941.912060, mean_q: 1926.369598, mean_tau: 0.038354\n",
            " 31170/50000: episode: 1315, duration: 0.308s, episode steps:  27, steps per second:  88, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 11077.980980, mae: 942.855754, mean_q: 1936.021285, mean_tau: 0.038311\n",
            " 31191/50000: episode: 1316, duration: 0.248s, episode steps:  21, steps per second:  85, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 10110.909098, mae: 937.413202, mean_q: 1940.922857, mean_tau: 0.038264\n",
            " 31204/50000: episode: 1317, duration: 0.146s, episode steps:  13, steps per second:  89, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 8054.666354, mae: 933.648212, mean_q: 1951.941829, mean_tau: 0.038230\n",
            " 31219/50000: episode: 1318, duration: 0.171s, episode steps:  15, steps per second:  88, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 10740.786149, mae: 953.901746, mean_q: 1968.782544, mean_tau: 0.038202\n",
            " 31237/50000: episode: 1319, duration: 0.211s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11522.327542, mae: 946.439718, mean_q: 1937.640110, mean_tau: 0.038170\n",
            " 31258/50000: episode: 1320, duration: 0.276s, episode steps:  21, steps per second:  76, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 9339.124378, mae: 964.350941, mean_q: 1971.891648, mean_tau: 0.038131\n",
            " 31279/50000: episode: 1321, duration: 0.281s, episode steps:  21, steps per second:  75, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 10986.454357, mae: 942.817365, mean_q: 1950.196196, mean_tau: 0.038089\n",
            " 31295/50000: episode: 1322, duration: 0.212s, episode steps:  16, steps per second:  75, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 14856.894051, mae: 949.701523, mean_q: 1948.241974, mean_tau: 0.038053\n",
            " 31307/50000: episode: 1323, duration: 0.140s, episode steps:  12, steps per second:  86, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 9687.287455, mae: 952.992035, mean_q: 1962.665100, mean_tau: 0.038025\n",
            " 31321/50000: episode: 1324, duration: 0.117s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 11811.150007, mae: 905.949864, mean_q: 1875.392944, mean_tau: 0.037999\n",
            " 31332/50000: episode: 1325, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 10572.265936, mae: 943.520985, mean_q: 1950.596924, mean_tau: 0.037975\n",
            " 31363/50000: episode: 1326, duration: 0.254s, episode steps:  31, steps per second: 122, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 6242.153119, mae: 963.024325, mean_q: 1989.913618, mean_tau: 0.037933\n",
            " 31399/50000: episode: 1327, duration: 0.270s, episode steps:  36, steps per second: 133, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.639 [0.000, 1.000],  loss: 9743.840210, mae: 906.150214, mean_q: 1889.194722, mean_tau: 0.037867\n",
            " 31416/50000: episode: 1328, duration: 0.125s, episode steps:  17, steps per second: 136, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7822.099731, mae: 979.766422, mean_q: 1995.379667, mean_tau: 0.037814\n",
            " 31429/50000: episode: 1329, duration: 0.124s, episode steps:  13, steps per second: 105, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 7945.674607, mae: 952.921837, mean_q: 1964.034865, mean_tau: 0.037784\n",
            " 31463/50000: episode: 1330, duration: 0.277s, episode steps:  34, steps per second: 123, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8243.137932, mae: 943.597843, mean_q: 1948.382978, mean_tau: 0.037738\n",
            " 31501/50000: episode: 1331, duration: 0.291s, episode steps:  38, steps per second: 130, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 12189.770366, mae: 937.436716, mean_q: 1932.606237, mean_tau: 0.037667\n",
            " 31515/50000: episode: 1332, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8638.829965, mae: 946.309226, mean_q: 1943.926601, mean_tau: 0.037615\n",
            " 31529/50000: episode: 1333, duration: 0.120s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 7578.350943, mae: 952.604623, mean_q: 1967.447170, mean_tau: 0.037587\n",
            " 31545/50000: episode: 1334, duration: 0.133s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9060.403549, mae: 945.190834, mean_q: 1968.702553, mean_tau: 0.037558\n",
            " 31558/50000: episode: 1335, duration: 0.135s, episode steps:  13, steps per second:  96, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 23048.012545, mae: 952.030856, mean_q: 1950.166710, mean_tau: 0.037529\n",
            " 31596/50000: episode: 1336, duration: 0.332s, episode steps:  38, steps per second: 114, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 7021.227362, mae: 956.674969, mean_q: 1982.734838, mean_tau: 0.037479\n",
            " 31608/50000: episode: 1337, duration: 0.098s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 17348.593506, mae: 938.651342, mean_q: 1928.209839, mean_tau: 0.037429\n",
            " 31665/50000: episode: 1338, duration: 0.466s, episode steps:  57, steps per second: 122, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 10920.793202, mae: 961.678259, mean_q: 1974.100436, mean_tau: 0.037361\n",
            " 31680/50000: episode: 1339, duration: 0.136s, episode steps:  15, steps per second: 110, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 9543.507959, mae: 936.711739, mean_q: 1932.824276, mean_tau: 0.037289\n",
            " 31697/50000: episode: 1340, duration: 0.151s, episode steps:  17, steps per second: 112, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 12123.854980, mae: 988.536039, mean_q: 2017.935008, mean_tau: 0.037258\n",
            " 31708/50000: episode: 1341, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 8463.848899, mae: 971.026939, mean_q: 1963.589910, mean_tau: 0.037230\n",
            " 31740/50000: episode: 1342, duration: 0.262s, episode steps:  32, steps per second: 122, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 8805.152241, mae: 961.693144, mean_q: 1998.030369, mean_tau: 0.037187\n",
            " 31756/50000: episode: 1343, duration: 0.119s, episode steps:  16, steps per second: 134, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17989.116013, mae: 958.606476, mean_q: 1940.766068, mean_tau: 0.037140\n",
            " 31801/50000: episode: 1344, duration: 0.353s, episode steps:  45, steps per second: 128, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 11346.070475, mae: 953.438251, mean_q: 1966.498375, mean_tau: 0.037080\n",
            " 31819/50000: episode: 1345, duration: 0.159s, episode steps:  18, steps per second: 114, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 14546.179986, mae: 984.755836, mean_q: 1989.727492, mean_tau: 0.037017\n",
            " 31831/50000: episode: 1346, duration: 0.103s, episode steps:  12, steps per second: 116, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 10484.347921, mae: 952.780075, mean_q: 1980.740458, mean_tau: 0.036987\n",
            " 31848/50000: episode: 1347, duration: 0.134s, episode steps:  17, steps per second: 127, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 11478.865471, mae: 973.416339, mean_q: 2005.826854, mean_tau: 0.036959\n",
            " 31876/50000: episode: 1348, duration: 0.231s, episode steps:  28, steps per second: 121, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 15919.562012, mae: 958.432467, mean_q: 1944.244363, mean_tau: 0.036914\n",
            " 31888/50000: episode: 1349, duration: 0.093s, episode steps:  12, steps per second: 129, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5822.260437, mae: 949.214956, mean_q: 1953.944132, mean_tau: 0.036875\n",
            " 31901/50000: episode: 1350, duration: 0.107s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 6971.365779, mae: 961.923049, mean_q: 1983.769738, mean_tau: 0.036850\n",
            " 31924/50000: episode: 1351, duration: 0.187s, episode steps:  23, steps per second: 123, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 10786.732454, mae: 959.498219, mean_q: 1960.354858, mean_tau: 0.036814\n",
            " 31942/50000: episode: 1352, duration: 0.143s, episode steps:  18, steps per second: 126, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5142.019151, mae: 989.089844, mean_q: 2038.302802, mean_tau: 0.036774\n",
            " 31988/50000: episode: 1353, duration: 0.368s, episode steps:  46, steps per second: 125, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11486.903572, mae: 963.422369, mean_q: 1978.627043, mean_tau: 0.036710\n",
            " 32018/50000: episode: 1354, duration: 0.253s, episode steps:  30, steps per second: 119, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10241.910270, mae: 967.836444, mean_q: 1971.098149, mean_tau: 0.036635\n",
            " 32032/50000: episode: 1355, duration: 0.112s, episode steps:  14, steps per second: 124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14828.476754, mae: 969.422267, mean_q: 1988.870553, mean_tau: 0.036591\n",
            " 32053/50000: episode: 1356, duration: 0.185s, episode steps:  21, steps per second: 114, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 6020.099197, mae: 967.995356, mean_q: 1998.074730, mean_tau: 0.036557\n",
            " 32064/50000: episode: 1357, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 16809.400080, mae: 992.112122, mean_q: 1994.727406, mean_tau: 0.036525\n",
            " 32089/50000: episode: 1358, duration: 0.211s, episode steps:  25, steps per second: 119, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 13163.869209, mae: 961.735779, mean_q: 1966.342954, mean_tau: 0.036490\n",
            " 32107/50000: episode: 1359, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12667.789103, mae: 954.498471, mean_q: 1973.602498, mean_tau: 0.036447\n",
            " 32130/50000: episode: 1360, duration: 0.179s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.348 [0.000, 1.000],  loss: 14370.302724, mae: 953.290026, mean_q: 1934.569835, mean_tau: 0.036406\n",
            " 32142/50000: episode: 1361, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 8486.242330, mae: 984.045171, mean_q: 2005.346842, mean_tau: 0.036372\n",
            " 32172/50000: episode: 1362, duration: 0.247s, episode steps:  30, steps per second: 121, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20373.289111, mae: 989.817499, mean_q: 2007.794535, mean_tau: 0.036330\n",
            " 32192/50000: episode: 1363, duration: 0.152s, episode steps:  20, steps per second: 132, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 9170.517267, mae: 965.320209, mean_q: 1997.055634, mean_tau: 0.036281\n",
            " 32215/50000: episode: 1364, duration: 0.186s, episode steps:  23, steps per second: 124, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 11280.538017, mae: 996.413442, mean_q: 2033.008173, mean_tau: 0.036238\n",
            " 32229/50000: episode: 1365, duration: 0.127s, episode steps:  14, steps per second: 111, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 15194.412092, mae: 980.852709, mean_q: 1995.502781, mean_tau: 0.036201\n",
            " 32241/50000: episode: 1366, duration: 0.112s, episode steps:  12, steps per second: 107, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 9777.153158, mae: 931.784637, mean_q: 1951.078512, mean_tau: 0.036176\n",
            " 32262/50000: episode: 1367, duration: 0.170s, episode steps:  21, steps per second: 124, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 9740.184535, mae: 972.573475, mean_q: 2007.046515, mean_tau: 0.036143\n",
            " 32279/50000: episode: 1368, duration: 0.150s, episode steps:  17, steps per second: 113, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7008.060073, mae: 966.247968, mean_q: 2002.649558, mean_tau: 0.036105\n",
            " 32304/50000: episode: 1369, duration: 0.230s, episode steps:  25, steps per second: 109, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 12038.795151, mae: 990.935427, mean_q: 2040.610381, mean_tau: 0.036064\n",
            " 32327/50000: episode: 1370, duration: 0.183s, episode steps:  23, steps per second: 126, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 10270.518013, mae: 982.779156, mean_q: 2024.376210, mean_tau: 0.036016\n",
            " 32339/50000: episode: 1371, duration: 0.112s, episode steps:  12, steps per second: 107, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 7409.938253, mae: 968.093943, mean_q: 1995.908763, mean_tau: 0.035982\n",
            " 32348/50000: episode: 1372, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 9775.586900, mae: 960.252204, mean_q: 2015.754300, mean_tau: 0.035961\n",
            " 32371/50000: episode: 1373, duration: 0.193s, episode steps:  23, steps per second: 119, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 10744.673563, mae: 982.613945, mean_q: 2013.027970, mean_tau: 0.035929\n",
            " 32388/50000: episode: 1374, duration: 0.140s, episode steps:  17, steps per second: 121, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 8825.090289, mae: 994.264806, mean_q: 2042.295661, mean_tau: 0.035890\n",
            " 32415/50000: episode: 1375, duration: 0.231s, episode steps:  27, steps per second: 117, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 9917.568283, mae: 1001.103290, mean_q: 2040.288552, mean_tau: 0.035846\n",
            " 32444/50000: episode: 1376, duration: 0.230s, episode steps:  29, steps per second: 126, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.345 [0.000, 1.000],  loss: 12267.756129, mae: 987.045084, mean_q: 2030.069711, mean_tau: 0.035791\n",
            " 32519/50000: episode: 1377, duration: 0.627s, episode steps:  75, steps per second: 120, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 7838.637705, mae: 978.869241, mean_q: 2016.994445, mean_tau: 0.035688\n",
            " 32532/50000: episode: 1378, duration: 0.172s, episode steps:  13, steps per second:  75, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 22717.327421, mae: 994.272423, mean_q: 2011.468412, mean_tau: 0.035601\n",
            " 32597/50000: episode: 1379, duration: 0.737s, episode steps:  65, steps per second:  88, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 10433.701748, mae: 994.975944, mean_q: 2050.559339, mean_tau: 0.035523\n",
            " 32607/50000: episode: 1380, duration: 0.118s, episode steps:  10, steps per second:  85, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 8939.585144, mae: 1029.806659, mean_q: 2101.758948, mean_tau: 0.035449\n",
            " 32620/50000: episode: 1381, duration: 0.182s, episode steps:  13, steps per second:  71, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 8868.532696, mae: 1025.526203, mean_q: 2104.518404, mean_tau: 0.035426\n",
            " 32630/50000: episode: 1382, duration: 0.117s, episode steps:  10, steps per second:  86, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 7900.290747, mae: 990.747571, mean_q: 2042.416748, mean_tau: 0.035403\n",
            " 32646/50000: episode: 1383, duration: 0.192s, episode steps:  16, steps per second:  83, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5207.141327, mae: 1007.375801, mean_q: 2076.537796, mean_tau: 0.035378\n",
            " 32665/50000: episode: 1384, duration: 0.245s, episode steps:  19, steps per second:  78, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 13557.607846, mae: 976.174586, mean_q: 1984.413529, mean_tau: 0.035343\n",
            " 32678/50000: episode: 1385, duration: 0.151s, episode steps:  13, steps per second:  86, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8986.198463, mae: 1027.831177, mean_q: 2109.940890, mean_tau: 0.035311\n",
            " 32701/50000: episode: 1386, duration: 0.291s, episode steps:  23, steps per second:  79, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 13610.934687, mae: 976.158970, mean_q: 2025.770906, mean_tau: 0.035276\n",
            " 32720/50000: episode: 1387, duration: 0.245s, episode steps:  19, steps per second:  77, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 14176.939260, mae: 1011.232823, mean_q: 2062.544536, mean_tau: 0.035234\n",
            " 32737/50000: episode: 1388, duration: 0.214s, episode steps:  17, steps per second:  79, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 10100.792344, mae: 995.678653, mean_q: 2071.293098, mean_tau: 0.035199\n",
            " 32759/50000: episode: 1389, duration: 0.205s, episode steps:  22, steps per second: 107, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9705.218911, mae: 976.608482, mean_q: 2012.872442, mean_tau: 0.035160\n",
            " 32780/50000: episode: 1390, duration: 0.169s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 11329.858677, mae: 1016.822876, mean_q: 2080.512312, mean_tau: 0.035117\n",
            " 32797/50000: episode: 1391, duration: 0.139s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 9599.955581, mae: 995.278496, mean_q: 2054.057007, mean_tau: 0.035080\n",
            " 32831/50000: episode: 1392, duration: 0.270s, episode steps:  34, steps per second: 126, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13611.509787, mae: 990.817806, mean_q: 2029.254330, mean_tau: 0.035029\n",
            " 32852/50000: episode: 1393, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 8859.642520, mae: 999.119254, mean_q: 2040.144764, mean_tau: 0.034975\n",
            " 32942/50000: episode: 1394, duration: 0.722s, episode steps:  90, steps per second: 125, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 10761.689807, mae: 1000.950989, mean_q: 2065.819569, mean_tau: 0.034865\n",
            " 32956/50000: episode: 1395, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 5671.790144, mae: 995.937020, mean_q: 2062.642569, mean_tau: 0.034762\n",
            " 32972/50000: episode: 1396, duration: 0.134s, episode steps:  16, steps per second: 119, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11117.339508, mae: 1017.457180, mean_q: 2112.402618, mean_tau: 0.034732\n",
            " 32987/50000: episode: 1397, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 14499.275293, mae: 1000.837097, mean_q: 2051.006038, mean_tau: 0.034702\n",
            " 33002/50000: episode: 1398, duration: 0.127s, episode steps:  15, steps per second: 118, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 11248.298372, mae: 1004.302616, mean_q: 2054.774390, mean_tau: 0.034672\n",
            " 33014/50000: episode: 1399, duration: 0.094s, episode steps:  12, steps per second: 128, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 19059.834290, mae: 1005.049103, mean_q: 2056.284261, mean_tau: 0.034645\n",
            " 33062/50000: episode: 1400, duration: 0.368s, episode steps:  48, steps per second: 130, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 13830.790616, mae: 1008.682051, mean_q: 2066.912557, mean_tau: 0.034586\n",
            " 33075/50000: episode: 1401, duration: 0.109s, episode steps:  13, steps per second: 120, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 15017.792800, mae: 984.858079, mean_q: 2021.853074, mean_tau: 0.034525\n",
            " 33102/50000: episode: 1402, duration: 0.214s, episode steps:  27, steps per second: 126, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 15201.701656, mae: 995.924698, mean_q: 2042.940565, mean_tau: 0.034486\n",
            " 33115/50000: episode: 1403, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 10005.084698, mae: 1055.879287, mean_q: 2173.553711, mean_tau: 0.034446\n",
            " 33128/50000: episode: 1404, duration: 0.099s, episode steps:  13, steps per second: 131, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 17202.550284, mae: 982.399954, mean_q: 2006.284565, mean_tau: 0.034420\n",
            " 33144/50000: episode: 1405, duration: 0.143s, episode steps:  16, steps per second: 112, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 10175.450058, mae: 984.054516, mean_q: 2033.287712, mean_tau: 0.034392\n",
            " 33154/50000: episode: 1406, duration: 0.091s, episode steps:  10, steps per second: 109, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 12917.727441, mae: 1003.187445, mean_q: 2053.798853, mean_tau: 0.034366\n",
            " 33201/50000: episode: 1407, duration: 0.377s, episode steps:  47, steps per second: 125, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 13679.209265, mae: 1030.806617, mean_q: 2109.503937, mean_tau: 0.034310\n",
            " 33212/50000: episode: 1408, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 4655.058527, mae: 1026.713224, mean_q: 2114.910467, mean_tau: 0.034252\n",
            " 33222/50000: episode: 1409, duration: 0.078s, episode steps:  10, steps per second: 129, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 4610.829761, mae: 1018.410370, mean_q: 2108.134888, mean_tau: 0.034231\n",
            " 33255/50000: episode: 1410, duration: 0.253s, episode steps:  33, steps per second: 131, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 16452.879535, mae: 996.318051, mean_q: 2038.243305, mean_tau: 0.034189\n",
            " 33282/50000: episode: 1411, duration: 0.199s, episode steps:  27, steps per second: 136, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 8151.003513, mae: 972.199895, mean_q: 2019.607467, mean_tau: 0.034129\n",
            " 33310/50000: episode: 1412, duration: 0.228s, episode steps:  28, steps per second: 123, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 10339.035993, mae: 1009.404742, mean_q: 2066.237841, mean_tau: 0.034075\n",
            " 33330/50000: episode: 1413, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 6846.355615, mae: 979.119302, mean_q: 2042.184387, mean_tau: 0.034027\n",
            " 33375/50000: episode: 1414, duration: 0.363s, episode steps:  45, steps per second: 124, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 11565.906299, mae: 1013.018962, mean_q: 2085.209820, mean_tau: 0.033963\n",
            " 33391/50000: episode: 1415, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 11640.858040, mae: 1023.665928, mean_q: 2117.103432, mean_tau: 0.033903\n",
            " 33420/50000: episode: 1416, duration: 0.246s, episode steps:  29, steps per second: 118, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 15467.747154, mae: 1005.825244, mean_q: 2064.774469, mean_tau: 0.033858\n",
            " 33434/50000: episode: 1417, duration: 0.109s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 8678.020926, mae: 985.216666, mean_q: 2047.818124, mean_tau: 0.033816\n",
            " 33445/50000: episode: 1418, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 20579.042525, mae: 1049.685747, mean_q: 2088.518455, mean_tau: 0.033791\n",
            " 33496/50000: episode: 1419, duration: 0.414s, episode steps:  51, steps per second: 123, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 14059.070801, mae: 1001.097626, mean_q: 2060.281257, mean_tau: 0.033729\n",
            " 33509/50000: episode: 1420, duration: 0.123s, episode steps:  13, steps per second: 106, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 12700.908297, mae: 1010.922063, mean_q: 2077.983182, mean_tau: 0.033666\n",
            " 33520/50000: episode: 1421, duration: 0.109s, episode steps:  11, steps per second: 101, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 15818.523415, mae: 1049.696056, mean_q: 2135.680597, mean_tau: 0.033642\n",
            " 33552/50000: episode: 1422, duration: 0.241s, episode steps:  32, steps per second: 133, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.406 [0.000, 1.000],  loss: 10615.039234, mae: 1024.274982, mean_q: 2098.679050, mean_tau: 0.033600\n",
            " 33588/50000: episode: 1423, duration: 0.290s, episode steps:  36, steps per second: 124, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 14898.309252, mae: 1026.146354, mean_q: 2120.802063, mean_tau: 0.033532\n",
            " 33603/50000: episode: 1424, duration: 0.111s, episode steps:  15, steps per second: 135, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 10118.005729, mae: 1051.625500, mean_q: 2146.560197, mean_tau: 0.033482\n",
            " 33618/50000: episode: 1425, duration: 0.136s, episode steps:  15, steps per second: 110, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 6742.998649, mae: 1027.073433, mean_q: 2129.062679, mean_tau: 0.033452\n",
            " 33636/50000: episode: 1426, duration: 0.133s, episode steps:  18, steps per second: 136, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8362.098097, mae: 1035.640842, mean_q: 2131.033183, mean_tau: 0.033420\n",
            " 33653/50000: episode: 1427, duration: 0.145s, episode steps:  17, steps per second: 117, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 10177.017190, mae: 1007.183816, mean_q: 2081.425882, mean_tau: 0.033385\n",
            " 33670/50000: episode: 1428, duration: 0.138s, episode steps:  17, steps per second: 123, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 11359.015510, mae: 1033.966021, mean_q: 2126.982501, mean_tau: 0.033351\n",
            " 33739/50000: episode: 1429, duration: 0.543s, episode steps:  69, steps per second: 127, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 17322.659038, mae: 1037.916393, mean_q: 2117.646352, mean_tau: 0.033266\n",
            " 33766/50000: episode: 1430, duration: 0.208s, episode steps:  27, steps per second: 130, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 12095.154387, mae: 1006.522594, mean_q: 2073.698328, mean_tau: 0.033171\n",
            " 33781/50000: episode: 1431, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 13438.320182, mae: 1060.958773, mean_q: 2170.758260, mean_tau: 0.033129\n",
            " 33804/50000: episode: 1432, duration: 0.180s, episode steps:  23, steps per second: 128, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 11010.196560, mae: 1040.525327, mean_q: 2137.682431, mean_tau: 0.033092\n",
            " 33816/50000: episode: 1433, duration: 0.088s, episode steps:  12, steps per second: 137, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 9501.823730, mae: 1008.428869, mean_q: 2081.978190, mean_tau: 0.033057\n",
            " 33827/50000: episode: 1434, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 5356.079989, mae: 1043.335388, mean_q: 2177.757224, mean_tau: 0.033034\n",
            " 33842/50000: episode: 1435, duration: 0.121s, episode steps:  15, steps per second: 124, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 15784.406722, mae: 1053.654732, mean_q: 2135.662663, mean_tau: 0.033009\n",
            " 33856/50000: episode: 1436, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 12284.001273, mae: 1049.550515, mean_q: 2154.947614, mean_tau: 0.032980\n",
            " 33883/50000: episode: 1437, duration: 0.230s, episode steps:  27, steps per second: 118, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 11728.827781, mae: 1040.678175, mean_q: 2154.645657, mean_tau: 0.032939\n",
            " 33904/50000: episode: 1438, duration: 0.172s, episode steps:  21, steps per second: 122, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 12063.751476, mae: 1042.090242, mean_q: 2138.500453, mean_tau: 0.032892\n",
            " 33926/50000: episode: 1439, duration: 0.184s, episode steps:  22, steps per second: 120, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 15575.234686, mae: 1042.933103, mean_q: 2140.648110, mean_tau: 0.032849\n",
            " 33945/50000: episode: 1440, duration: 0.140s, episode steps:  19, steps per second: 136, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 10230.728194, mae: 1055.309124, mean_q: 2155.032792, mean_tau: 0.032809\n",
            " 33959/50000: episode: 1441, duration: 0.130s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 8932.064558, mae: 1006.322889, mean_q: 2074.502790, mean_tau: 0.032776\n",
            " 34008/50000: episode: 1442, duration: 0.473s, episode steps:  49, steps per second: 104, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 9729.726767, mae: 1051.349770, mean_q: 2171.364290, mean_tau: 0.032714\n",
            " 34028/50000: episode: 1443, duration: 0.244s, episode steps:  20, steps per second:  82, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10720.309729, mae: 1050.874661, mean_q: 2166.010583, mean_tau: 0.032645\n",
            " 34054/50000: episode: 1444, duration: 0.291s, episode steps:  26, steps per second:  89, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10984.479182, mae: 1031.437991, mean_q: 2117.931857, mean_tau: 0.032600\n",
            " 34070/50000: episode: 1445, duration: 0.175s, episode steps:  16, steps per second:  91, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27360.307358, mae: 1046.646950, mean_q: 2120.278946, mean_tau: 0.032558\n",
            " 34098/50000: episode: 1446, duration: 0.319s, episode steps:  28, steps per second:  88, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 11506.860295, mae: 1048.787314, mean_q: 2140.607234, mean_tau: 0.032515\n",
            " 34119/50000: episode: 1447, duration: 0.259s, episode steps:  21, steps per second:  81, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 15696.811791, mae: 1017.440206, mean_q: 2098.064238, mean_tau: 0.032466\n",
            " 34135/50000: episode: 1448, duration: 0.179s, episode steps:  16, steps per second:  89, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 8978.227768, mae: 1048.910809, mean_q: 2171.141495, mean_tau: 0.032430\n",
            " 34152/50000: episode: 1449, duration: 0.220s, episode steps:  17, steps per second:  77, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 13910.647030, mae: 1046.329698, mean_q: 2156.743229, mean_tau: 0.032397\n",
            " 34168/50000: episode: 1450, duration: 0.188s, episode steps:  16, steps per second:  85, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 29679.683609, mae: 1054.998341, mean_q: 2137.040131, mean_tau: 0.032364\n",
            " 34186/50000: episode: 1451, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 15071.398777, mae: 1042.977258, mean_q: 2124.331835, mean_tau: 0.032331\n",
            " 34200/50000: episode: 1452, duration: 0.162s, episode steps:  14, steps per second:  86, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 11719.251229, mae: 1055.335632, mean_q: 2187.374826, mean_tau: 0.032299\n",
            " 34217/50000: episode: 1453, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 22480.872875, mae: 1092.447876, mean_q: 2224.307294, mean_tau: 0.032268\n",
            " 34264/50000: episode: 1454, duration: 0.371s, episode steps:  47, steps per second: 127, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 17789.628683, mae: 1027.565762, mean_q: 2105.408273, mean_tau: 0.032205\n",
            " 34275/50000: episode: 1455, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 11012.850675, mae: 1018.842330, mean_q: 2095.600997, mean_tau: 0.032147\n",
            " 34326/50000: episode: 1456, duration: 0.397s, episode steps:  51, steps per second: 128, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 18777.607379, mae: 1048.208895, mean_q: 2144.556389, mean_tau: 0.032086\n",
            " 34338/50000: episode: 1457, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 32048.058472, mae: 1037.192678, mean_q: 2125.757121, mean_tau: 0.032024\n",
            " 34349/50000: episode: 1458, duration: 0.085s, episode steps:  11, steps per second: 130, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 11157.385653, mae: 1016.020375, mean_q: 2125.696100, mean_tau: 0.032001\n",
            " 34361/50000: episode: 1459, duration: 0.117s, episode steps:  12, steps per second: 102, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 5142.167908, mae: 1084.308050, mean_q: 2248.981710, mean_tau: 0.031978\n",
            " 34376/50000: episode: 1460, duration: 0.123s, episode steps:  15, steps per second: 122, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 15089.916113, mae: 1053.707768, mean_q: 2167.345638, mean_tau: 0.031951\n",
            " 34389/50000: episode: 1461, duration: 0.107s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 13636.824838, mae: 1063.794326, mean_q: 2179.401724, mean_tau: 0.031924\n",
            " 34399/50000: episode: 1462, duration: 0.085s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 10333.020679, mae: 1021.454572, mean_q: 2110.037427, mean_tau: 0.031901\n",
            " 34410/50000: episode: 1463, duration: 0.093s, episode steps:  11, steps per second: 118, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 21135.237393, mae: 1031.558216, mean_q: 2106.426514, mean_tau: 0.031880\n",
            " 34428/50000: episode: 1464, duration: 0.146s, episode steps:  18, steps per second: 123, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 9111.288561, mae: 1056.535160, mean_q: 2197.218526, mean_tau: 0.031851\n",
            " 34443/50000: episode: 1465, duration: 0.114s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 14283.100228, mae: 1068.487537, mean_q: 2171.115885, mean_tau: 0.031819\n",
            " 34459/50000: episode: 1466, duration: 0.131s, episode steps:  16, steps per second: 122, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 8388.578644, mae: 1042.129387, mean_q: 2155.342407, mean_tau: 0.031788\n",
            " 34484/50000: episode: 1467, duration: 0.209s, episode steps:  25, steps per second: 120, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 17672.571270, mae: 1052.131047, mean_q: 2151.954575, mean_tau: 0.031747\n",
            " 34494/50000: episode: 1468, duration: 0.085s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 23298.948779, mae: 1092.558667, mean_q: 2212.082861, mean_tau: 0.031713\n",
            " 34515/50000: episode: 1469, duration: 0.167s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 17192.915583, mae: 1050.294006, mean_q: 2131.325451, mean_tau: 0.031682\n",
            " 34528/50000: episode: 1470, duration: 0.112s, episode steps:  13, steps per second: 117, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 12606.965182, mae: 1058.980830, mean_q: 2214.851440, mean_tau: 0.031648\n",
            " 34573/50000: episode: 1471, duration: 0.359s, episode steps:  45, steps per second: 125, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 10462.231331, mae: 1072.649647, mean_q: 2215.174927, mean_tau: 0.031591\n",
            " 34592/50000: episode: 1472, duration: 0.169s, episode steps:  19, steps per second: 112, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 12296.908807, mae: 1080.737552, mean_q: 2215.548044, mean_tau: 0.031528\n",
            " 34610/50000: episode: 1473, duration: 0.156s, episode steps:  18, steps per second: 116, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 19972.199422, mae: 1055.107649, mean_q: 2151.899773, mean_tau: 0.031491\n",
            " 34641/50000: episode: 1474, duration: 0.273s, episode steps:  31, steps per second: 114, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 16905.293725, mae: 1057.233704, mean_q: 2168.065426, mean_tau: 0.031442\n",
            " 34656/50000: episode: 1475, duration: 0.127s, episode steps:  15, steps per second: 118, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 14119.740894, mae: 1066.170532, mean_q: 2162.534090, mean_tau: 0.031397\n",
            " 34668/50000: episode: 1476, duration: 0.105s, episode steps:  12, steps per second: 114, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 8547.484985, mae: 1047.072713, mean_q: 2159.283549, mean_tau: 0.031370\n",
            " 34686/50000: episode: 1477, duration: 0.135s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11531.596646, mae: 1050.228092, mean_q: 2155.143921, mean_tau: 0.031341\n",
            " 34702/50000: episode: 1478, duration: 0.138s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 15456.497360, mae: 1050.347294, mean_q: 2139.390289, mean_tau: 0.031307\n",
            " 34730/50000: episode: 1479, duration: 0.227s, episode steps:  28, steps per second: 123, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13958.616263, mae: 1047.104385, mean_q: 2137.860548, mean_tau: 0.031263\n",
            " 34749/50000: episode: 1480, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.368 [0.000, 1.000],  loss: 10279.409475, mae: 1031.466360, mean_q: 2136.373889, mean_tau: 0.031217\n",
            " 34761/50000: episode: 1481, duration: 0.103s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 7141.604197, mae: 1047.792007, mean_q: 2152.041443, mean_tau: 0.031186\n",
            " 34799/50000: episode: 1482, duration: 0.309s, episode steps:  38, steps per second: 123, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15505.898065, mae: 1059.736061, mean_q: 2171.641871, mean_tau: 0.031137\n",
            " 34814/50000: episode: 1483, duration: 0.122s, episode steps:  15, steps per second: 123, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 21831.443815, mae: 1079.865979, mean_q: 2225.933537, mean_tau: 0.031084\n",
            " 34853/50000: episode: 1484, duration: 0.330s, episode steps:  39, steps per second: 118, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 14929.623429, mae: 1059.945383, mean_q: 2157.633958, mean_tau: 0.031031\n",
            " 34867/50000: episode: 1485, duration: 0.108s, episode steps:  14, steps per second: 129, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 25969.837926, mae: 1079.647151, mean_q: 2173.081438, mean_tau: 0.030978\n",
            " 34888/50000: episode: 1486, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 8987.207055, mae: 1061.183710, mean_q: 2183.377906, mean_tau: 0.030944\n",
            " 34897/50000: episode: 1487, duration: 0.072s, episode steps:   9, steps per second: 124, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 12261.512207, mae: 1062.785353, mean_q: 2209.389038, mean_tau: 0.030914\n",
            " 34919/50000: episode: 1488, duration: 0.179s, episode steps:  22, steps per second: 123, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 20016.347423, mae: 1035.651237, mean_q: 2142.223955, mean_tau: 0.030883\n",
            " 34932/50000: episode: 1489, duration: 0.100s, episode steps:  13, steps per second: 129, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 13775.846605, mae: 1050.343280, mean_q: 2166.030405, mean_tau: 0.030849\n",
            " 34949/50000: episode: 1490, duration: 0.138s, episode steps:  17, steps per second: 123, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 15950.263298, mae: 1047.967375, mean_q: 2137.397540, mean_tau: 0.030819\n",
            " 34961/50000: episode: 1491, duration: 0.093s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 14747.711243, mae: 1085.525462, mean_q: 2214.788849, mean_tau: 0.030790\n",
            " 34973/50000: episode: 1492, duration: 0.103s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9611.084167, mae: 1087.369746, mean_q: 2237.052714, mean_tau: 0.030766\n",
            " 35006/50000: episode: 1493, duration: 0.254s, episode steps:  33, steps per second: 130, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 9192.500044, mae: 1070.162220, mean_q: 2204.075402, mean_tau: 0.030722\n",
            " 35031/50000: episode: 1494, duration: 0.192s, episode steps:  25, steps per second: 130, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 14382.136060, mae: 1041.433779, mean_q: 2138.947397, mean_tau: 0.030664\n",
            " 35050/50000: episode: 1495, duration: 0.158s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 13613.012181, mae: 1063.699029, mean_q: 2178.217921, mean_tau: 0.030621\n",
            " 35096/50000: episode: 1496, duration: 0.355s, episode steps:  46, steps per second: 130, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13033.961328, mae: 1083.328268, mean_q: 2212.369719, mean_tau: 0.030556\n",
            " 35132/50000: episode: 1497, duration: 0.286s, episode steps:  36, steps per second: 126, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 12604.489373, mae: 1050.501309, mean_q: 2151.229747, mean_tau: 0.030475\n",
            " 35142/50000: episode: 1498, duration: 0.079s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 18001.709399, mae: 1097.041077, mean_q: 2218.771460, mean_tau: 0.030430\n",
            " 35159/50000: episode: 1499, duration: 0.153s, episode steps:  17, steps per second: 111, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 9606.492159, mae: 1083.282176, mean_q: 2218.067785, mean_tau: 0.030403\n",
            " 35171/50000: episode: 1500, duration: 0.105s, episode steps:  12, steps per second: 114, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 11359.764404, mae: 1087.827128, mean_q: 2219.820221, mean_tau: 0.030374\n",
            " 35183/50000: episode: 1501, duration: 0.094s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 9177.921224, mae: 1054.393743, mean_q: 2175.363719, mean_tau: 0.030351\n",
            " 35214/50000: episode: 1502, duration: 0.257s, episode steps:  31, steps per second: 121, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 19795.598302, mae: 1044.863758, mean_q: 2153.259155, mean_tau: 0.030308\n",
            " 35227/50000: episode: 1503, duration: 0.114s, episode steps:  13, steps per second: 114, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 18378.494629, mae: 1099.456350, mean_q: 2241.552687, mean_tau: 0.030264\n",
            " 35267/50000: episode: 1504, duration: 0.307s, episode steps:  40, steps per second: 130, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14963.246997, mae: 1069.705966, mean_q: 2188.276672, mean_tau: 0.030212\n",
            " 35281/50000: episode: 1505, duration: 0.109s, episode steps:  14, steps per second: 129, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17594.730172, mae: 1077.862972, mean_q: 2199.342529, mean_tau: 0.030158\n",
            " 35292/50000: episode: 1506, duration: 0.087s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 11140.864790, mae: 1053.525491, mean_q: 2156.543368, mean_tau: 0.030134\n",
            " 35306/50000: episode: 1507, duration: 0.125s, episode steps:  14, steps per second: 112, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 8014.578596, mae: 1109.412763, mean_q: 2256.242440, mean_tau: 0.030109\n",
            " 35339/50000: episode: 1508, duration: 0.256s, episode steps:  33, steps per second: 129, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 17389.926033, mae: 1092.466732, mean_q: 2228.286447, mean_tau: 0.030062\n",
            " 35356/50000: episode: 1509, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 13907.694695, mae: 1061.670870, mean_q: 2182.841912, mean_tau: 0.030013\n",
            " 35391/50000: episode: 1510, duration: 0.272s, episode steps:  35, steps per second: 129, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 12172.733168, mae: 1067.778992, mean_q: 2188.039659, mean_tau: 0.029961\n",
            " 35400/50000: episode: 1511, duration: 0.075s, episode steps:   9, steps per second: 121, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 9248.303006, mae: 1082.909682, mean_q: 2242.330838, mean_tau: 0.029918\n",
            " 35419/50000: episode: 1512, duration: 0.158s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 14530.060521, mae: 1077.188085, mean_q: 2193.935598, mean_tau: 0.029890\n",
            " 35455/50000: episode: 1513, duration: 0.311s, episode steps:  36, steps per second: 116, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14478.839216, mae: 1092.228283, mean_q: 2236.281436, mean_tau: 0.029836\n",
            " 35477/50000: episode: 1514, duration: 0.258s, episode steps:  22, steps per second:  85, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 12997.571428, mae: 1093.585349, mean_q: 2251.810547, mean_tau: 0.029778\n",
            " 35527/50000: episode: 1515, duration: 0.546s, episode steps:  50, steps per second:  91, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 18617.141482, mae: 1108.601891, mean_q: 2264.412688, mean_tau: 0.029707\n",
            " 35543/50000: episode: 1516, duration: 0.182s, episode steps:  16, steps per second:  88, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8401.316330, mae: 1089.060898, mean_q: 2230.716103, mean_tau: 0.029642\n",
            " 35561/50000: episode: 1517, duration: 0.236s, episode steps:  18, steps per second:  76, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 12098.239163, mae: 1076.256907, mean_q: 2206.559937, mean_tau: 0.029608\n",
            " 35583/50000: episode: 1518, duration: 0.260s, episode steps:  22, steps per second:  84, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 8221.470931, mae: 1117.668909, mean_q: 2312.157648, mean_tau: 0.029568\n",
            " 35604/50000: episode: 1519, duration: 0.246s, episode steps:  21, steps per second:  85, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 7710.091059, mae: 1137.712304, mean_q: 2358.682571, mean_tau: 0.029526\n",
            " 35643/50000: episode: 1520, duration: 0.432s, episode steps:  39, steps per second:  90, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 14222.320319, mae: 1097.257803, mean_q: 2240.229239, mean_tau: 0.029466\n",
            " 35663/50000: episode: 1521, duration: 0.231s, episode steps:  20, steps per second:  87, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 16082.485010, mae: 1138.343417, mean_q: 2313.494153, mean_tau: 0.029408\n",
            " 35683/50000: episode: 1522, duration: 0.254s, episode steps:  20, steps per second:  79, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 11503.487183, mae: 1084.983685, mean_q: 2244.873810, mean_tau: 0.029368\n",
            " 35708/50000: episode: 1523, duration: 0.238s, episode steps:  25, steps per second: 105, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 13046.363242, mae: 1080.988938, mean_q: 2229.341274, mean_tau: 0.029324\n",
            " 35726/50000: episode: 1524, duration: 0.136s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 23148.863241, mae: 1121.432970, mean_q: 2282.799072, mean_tau: 0.029281\n",
            " 35747/50000: episode: 1525, duration: 0.179s, episode steps:  21, steps per second: 118, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 18625.049595, mae: 1107.380621, mean_q: 2257.337472, mean_tau: 0.029243\n",
            " 35758/50000: episode: 1526, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 11674.301969, mae: 1089.056996, mean_q: 2236.861927, mean_tau: 0.029211\n",
            " 35808/50000: episode: 1527, duration: 0.399s, episode steps:  50, steps per second: 125, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 18149.673462, mae: 1095.937928, mean_q: 2241.871030, mean_tau: 0.029151\n",
            " 35847/50000: episode: 1528, duration: 0.302s, episode steps:  39, steps per second: 129, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 13605.708283, mae: 1114.004121, mean_q: 2284.142706, mean_tau: 0.029063\n",
            " 35859/50000: episode: 1529, duration: 0.099s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 16437.081930, mae: 1112.604675, mean_q: 2322.751424, mean_tau: 0.029012\n",
            " 35887/50000: episode: 1530, duration: 0.227s, episode steps:  28, steps per second: 123, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.607 [0.000, 1.000],  loss: 10528.704136, mae: 1077.983614, mean_q: 2254.807713, mean_tau: 0.028972\n",
            " 35906/50000: episode: 1531, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 11316.665926, mae: 1131.716919, mean_q: 2329.967979, mean_tau: 0.028926\n",
            " 35920/50000: episode: 1532, duration: 0.115s, episode steps:  14, steps per second: 122, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13259.653843, mae: 1087.023433, mean_q: 2256.399519, mean_tau: 0.028893\n",
            " 35933/50000: episode: 1533, duration: 0.097s, episode steps:  13, steps per second: 134, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 11569.645790, mae: 1080.946890, mean_q: 2239.369338, mean_tau: 0.028867\n",
            " 35946/50000: episode: 1534, duration: 0.098s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 11747.018517, mae: 1080.868019, mean_q: 2215.049476, mean_tau: 0.028841\n",
            " 35982/50000: episode: 1535, duration: 0.285s, episode steps:  36, steps per second: 126, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 19492.677300, mae: 1136.108117, mean_q: 2314.448412, mean_tau: 0.028792\n",
            " 36015/50000: episode: 1536, duration: 0.263s, episode steps:  33, steps per second: 126, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 9699.729943, mae: 1142.850641, mean_q: 2361.257324, mean_tau: 0.028724\n",
            " 36036/50000: episode: 1537, duration: 0.157s, episode steps:  21, steps per second: 134, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.381 [0.000, 1.000],  loss: 20299.867990, mae: 1154.867792, mean_q: 2347.383405, mean_tau: 0.028671\n",
            " 36053/50000: episode: 1538, duration: 0.146s, episode steps:  17, steps per second: 116, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 16737.736500, mae: 1114.690659, mean_q: 2280.486335, mean_tau: 0.028633\n",
            " 36064/50000: episode: 1539, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 15287.697887, mae: 1131.707675, mean_q: 2298.547186, mean_tau: 0.028605\n",
            " 36088/50000: episode: 1540, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13315.667308, mae: 1139.175135, mean_q: 2346.622335, mean_tau: 0.028571\n",
            " 36104/50000: episode: 1541, duration: 0.121s, episode steps:  16, steps per second: 133, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 14897.861572, mae: 1124.781269, mean_q: 2324.372787, mean_tau: 0.028531\n",
            " 36131/50000: episode: 1542, duration: 0.210s, episode steps:  27, steps per second: 128, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 18634.595025, mae: 1112.201432, mean_q: 2282.595256, mean_tau: 0.028488\n",
            " 36143/50000: episode: 1543, duration: 0.094s, episode steps:  12, steps per second: 128, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 7527.451782, mae: 1157.132757, mean_q: 2355.957326, mean_tau: 0.028450\n",
            " 36155/50000: episode: 1544, duration: 0.095s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 7042.738566, mae: 1127.685893, mean_q: 2324.015910, mean_tau: 0.028426\n",
            " 36209/50000: episode: 1545, duration: 0.387s, episode steps:  54, steps per second: 140, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 15291.489954, mae: 1133.223428, mean_q: 2329.013710, mean_tau: 0.028361\n",
            " 36249/50000: episode: 1546, duration: 0.316s, episode steps:  40, steps per second: 127, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 14606.158893, mae: 1127.094223, mean_q: 2324.961249, mean_tau: 0.028268\n",
            " 36265/50000: episode: 1547, duration: 0.121s, episode steps:  16, steps per second: 132, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 20414.501328, mae: 1135.076111, mean_q: 2321.641190, mean_tau: 0.028212\n",
            " 36278/50000: episode: 1548, duration: 0.127s, episode steps:  13, steps per second: 103, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 11527.964017, mae: 1112.488065, mean_q: 2311.161668, mean_tau: 0.028183\n",
            " 36290/50000: episode: 1549, duration: 0.099s, episode steps:  12, steps per second: 121, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 16538.308919, mae: 1158.856191, mean_q: 2346.125224, mean_tau: 0.028159\n",
            " 36302/50000: episode: 1550, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 12981.517110, mae: 1117.800003, mean_q: 2321.985779, mean_tau: 0.028135\n",
            " 36314/50000: episode: 1551, duration: 0.102s, episode steps:  12, steps per second: 118, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 13699.554545, mae: 1118.862152, mean_q: 2316.036499, mean_tau: 0.028111\n",
            " 36326/50000: episode: 1552, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 8830.266663, mae: 1129.086456, mean_q: 2350.421000, mean_tau: 0.028087\n",
            " 36354/50000: episode: 1553, duration: 0.225s, episode steps:  28, steps per second: 125, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22822.405439, mae: 1122.346484, mean_q: 2278.957812, mean_tau: 0.028048\n",
            " 36370/50000: episode: 1554, duration: 0.117s, episode steps:  16, steps per second: 137, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 23322.201965, mae: 1130.382477, mean_q: 2316.252396, mean_tau: 0.028004\n",
            " 36388/50000: episode: 1555, duration: 0.142s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 23654.636068, mae: 1130.395721, mean_q: 2323.624729, mean_tau: 0.027971\n",
            " 36421/50000: episode: 1556, duration: 0.277s, episode steps:  33, steps per second: 119, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 16785.004742, mae: 1129.505558, mean_q: 2333.499023, mean_tau: 0.027920\n",
            " 36447/50000: episode: 1557, duration: 0.188s, episode steps:  26, steps per second: 138, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12055.278442, mae: 1135.789574, mean_q: 2357.288311, mean_tau: 0.027862\n",
            " 36458/50000: episode: 1558, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 15532.678089, mae: 1132.451971, mean_q: 2321.551358, mean_tau: 0.027825\n",
            " 36481/50000: episode: 1559, duration: 0.167s, episode steps:  23, steps per second: 138, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 14049.962700, mae: 1139.933190, mean_q: 2335.622283, mean_tau: 0.027791\n",
            " 36511/50000: episode: 1560, duration: 0.234s, episode steps:  30, steps per second: 128, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 13912.559220, mae: 1122.410268, mean_q: 2316.056657, mean_tau: 0.027739\n",
            " 36528/50000: episode: 1561, duration: 0.123s, episode steps:  17, steps per second: 138, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 22127.885512, mae: 1154.890560, mean_q: 2363.489459, mean_tau: 0.027692\n",
            " 36543/50000: episode: 1562, duration: 0.127s, episode steps:  15, steps per second: 118, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 14264.257292, mae: 1112.147937, mean_q: 2288.211865, mean_tau: 0.027661\n",
            " 36571/50000: episode: 1563, duration: 0.214s, episode steps:  28, steps per second: 131, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14458.953988, mae: 1154.190608, mean_q: 2376.571891, mean_tau: 0.027618\n",
            " 36588/50000: episode: 1564, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 13940.003964, mae: 1160.794355, mean_q: 2384.287325, mean_tau: 0.027574\n",
            " 36620/50000: episode: 1565, duration: 0.247s, episode steps:  32, steps per second: 129, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 14320.547058, mae: 1133.761265, mean_q: 2339.658211, mean_tau: 0.027525\n",
            " 36630/50000: episode: 1566, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 12249.653516, mae: 1169.675537, mean_q: 2411.303784, mean_tau: 0.027483\n",
            " 36644/50000: episode: 1567, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 8062.111363, mae: 1157.027448, mean_q: 2415.537249, mean_tau: 0.027460\n",
            " 36668/50000: episode: 1568, duration: 0.201s, episode steps:  24, steps per second: 120, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 14845.770884, mae: 1127.030301, mean_q: 2327.412186, mean_tau: 0.027422\n",
            " 36685/50000: episode: 1569, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 15236.232465, mae: 1136.323486, mean_q: 2334.095459, mean_tau: 0.027382\n",
            " 36700/50000: episode: 1570, duration: 0.127s, episode steps:  15, steps per second: 118, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 20172.280355, mae: 1129.299760, mean_q: 2290.746672, mean_tau: 0.027350\n",
            " 36712/50000: episode: 1571, duration: 0.097s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 20568.579427, mae: 1164.777222, mean_q: 2370.750407, mean_tau: 0.027323\n",
            " 36727/50000: episode: 1572, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 27740.407145, mae: 1135.453861, mean_q: 2291.903988, mean_tau: 0.027296\n",
            " 36743/50000: episode: 1573, duration: 0.131s, episode steps:  16, steps per second: 122, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 17148.545547, mae: 1142.581963, mean_q: 2355.607651, mean_tau: 0.027266\n",
            " 36755/50000: episode: 1574, duration: 0.107s, episode steps:  12, steps per second: 112, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 8300.583811, mae: 1159.732442, mean_q: 2391.582458, mean_tau: 0.027238\n",
            " 36788/50000: episode: 1575, duration: 0.269s, episode steps:  33, steps per second: 123, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 21825.653091, mae: 1148.211111, mean_q: 2348.682195, mean_tau: 0.027193\n",
            " 36813/50000: episode: 1576, duration: 0.195s, episode steps:  25, steps per second: 128, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 18921.456260, mae: 1128.428940, mean_q: 2320.433999, mean_tau: 0.027136\n",
            " 36845/50000: episode: 1577, duration: 0.252s, episode steps:  32, steps per second: 127, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16056.120640, mae: 1153.650715, mean_q: 2340.851482, mean_tau: 0.027080\n",
            " 36859/50000: episode: 1578, duration: 0.118s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 18293.487531, mae: 1129.160287, mean_q: 2334.823800, mean_tau: 0.027034\n",
            " 36880/50000: episode: 1579, duration: 0.192s, episode steps:  21, steps per second: 109, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 16940.908087, mae: 1154.401495, mean_q: 2368.789911, mean_tau: 0.026999\n",
            " 36895/50000: episode: 1580, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 30583.542692, mae: 1156.177515, mean_q: 2355.450895, mean_tau: 0.026964\n",
            " 36915/50000: episode: 1581, duration: 0.170s, episode steps:  20, steps per second: 117, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 14557.372412, mae: 1166.342383, mean_q: 2416.464990, mean_tau: 0.026929\n",
            " 36937/50000: episode: 1582, duration: 0.167s, episode steps:  22, steps per second: 131, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 14199.197277, mae: 1178.391707, mean_q: 2422.663197, mean_tau: 0.026888\n",
            " 36951/50000: episode: 1583, duration: 0.143s, episode steps:  14, steps per second:  98, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 27667.681850, mae: 1154.428911, mean_q: 2336.874233, mean_tau: 0.026852\n",
            " 36983/50000: episode: 1584, duration: 0.366s, episode steps:  32, steps per second:  88, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13416.343353, mae: 1140.889391, mean_q: 2350.472588, mean_tau: 0.026806\n",
            " 37001/50000: episode: 1585, duration: 0.195s, episode steps:  18, steps per second:  92, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 17969.868171, mae: 1147.802236, mean_q: 2341.429504, mean_tau: 0.026757\n",
            " 37023/50000: episode: 1586, duration: 0.247s, episode steps:  22, steps per second:  89, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 25312.753462, mae: 1172.661094, mean_q: 2372.538358, mean_tau: 0.026717\n",
            " 37061/50000: episode: 1587, duration: 0.439s, episode steps:  38, steps per second:  87, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17873.741545, mae: 1158.214717, mean_q: 2374.842481, mean_tau: 0.026658\n",
            " 37081/50000: episode: 1588, duration: 0.239s, episode steps:  20, steps per second:  84, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18881.802393, mae: 1133.554733, mean_q: 2320.667236, mean_tau: 0.026600\n",
            " 37111/50000: episode: 1589, duration: 0.352s, episode steps:  30, steps per second:  85, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15237.641121, mae: 1179.412811, mean_q: 2414.248942, mean_tau: 0.026551\n",
            " 37123/50000: episode: 1590, duration: 0.137s, episode steps:  12, steps per second:  88, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 15992.243001, mae: 1152.677856, mean_q: 2400.735697, mean_tau: 0.026509\n",
            " 37141/50000: episode: 1591, duration: 0.311s, episode steps:  18, steps per second:  58, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 15982.438626, mae: 1140.732761, mean_q: 2374.601997, mean_tau: 0.026480\n",
            " 37165/50000: episode: 1592, duration: 0.289s, episode steps:  24, steps per second:  83, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 14921.173396, mae: 1159.410988, mean_q: 2399.431732, mean_tau: 0.026438\n",
            " 37196/50000: episode: 1593, duration: 0.354s, episode steps:  31, steps per second:  88, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 24483.938083, mae: 1164.536863, mean_q: 2388.093104, mean_tau: 0.026384\n",
            " 37220/50000: episode: 1594, duration: 0.176s, episode steps:  24, steps per second: 136, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 26452.694672, mae: 1176.558322, mean_q: 2407.129924, mean_tau: 0.026329\n",
            " 37260/50000: episode: 1595, duration: 0.306s, episode steps:  40, steps per second: 131, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 15445.834540, mae: 1186.037048, mean_q: 2445.827551, mean_tau: 0.026266\n",
            " 37285/50000: episode: 1596, duration: 0.187s, episode steps:  25, steps per second: 134, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 15749.017148, mae: 1172.620293, mean_q: 2406.997188, mean_tau: 0.026201\n",
            " 37319/50000: episode: 1597, duration: 0.264s, episode steps:  34, steps per second: 129, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14449.278442, mae: 1199.134572, mean_q: 2469.653995, mean_tau: 0.026143\n",
            " 37340/50000: episode: 1598, duration: 0.156s, episode steps:  21, steps per second: 135, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 12969.495489, mae: 1200.585252, mean_q: 2456.934163, mean_tau: 0.026089\n",
            " 37359/50000: episode: 1599, duration: 0.147s, episode steps:  19, steps per second: 129, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 17556.559596, mae: 1164.747758, mean_q: 2391.080528, mean_tau: 0.026049\n",
            " 37371/50000: episode: 1600, duration: 0.095s, episode steps:  12, steps per second: 126, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 15124.220785, mae: 1146.945496, mean_q: 2378.449280, mean_tau: 0.026018\n",
            " 37389/50000: episode: 1601, duration: 0.143s, episode steps:  18, steps per second: 126, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23369.291558, mae: 1196.308695, mean_q: 2435.455444, mean_tau: 0.025989\n",
            " 37409/50000: episode: 1602, duration: 0.170s, episode steps:  20, steps per second: 117, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 11435.664990, mae: 1193.153241, mean_q: 2443.313660, mean_tau: 0.025951\n",
            " 37425/50000: episode: 1603, duration: 0.137s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 23488.385803, mae: 1187.065216, mean_q: 2420.109581, mean_tau: 0.025915\n",
            " 37450/50000: episode: 1604, duration: 0.215s, episode steps:  25, steps per second: 116, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 16886.634795, mae: 1164.857092, mean_q: 2409.373115, mean_tau: 0.025875\n",
            " 37462/50000: episode: 1605, duration: 0.095s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 13840.459849, mae: 1231.586761, mean_q: 2525.707011, mean_tau: 0.025838\n",
            " 37483/50000: episode: 1606, duration: 0.175s, episode steps:  21, steps per second: 120, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 23663.751081, mae: 1149.668524, mean_q: 2392.170061, mean_tau: 0.025805\n",
            " 37508/50000: episode: 1607, duration: 0.196s, episode steps:  25, steps per second: 128, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.320 [0.000, 1.000],  loss: 14191.596611, mae: 1188.950063, mean_q: 2455.152285, mean_tau: 0.025760\n",
            " 37542/50000: episode: 1608, duration: 0.276s, episode steps:  34, steps per second: 123, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21623.118013, mae: 1188.994719, mean_q: 2432.738170, mean_tau: 0.025701\n",
            " 37584/50000: episode: 1609, duration: 0.327s, episode steps:  42, steps per second: 128, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 18138.830276, mae: 1209.468828, mean_q: 2500.927287, mean_tau: 0.025626\n",
            " 37604/50000: episode: 1610, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 20021.529980, mae: 1204.622906, mean_q: 2460.041443, mean_tau: 0.025565\n",
            " 37623/50000: episode: 1611, duration: 0.155s, episode steps:  19, steps per second: 122, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 18884.699810, mae: 1243.272397, mean_q: 2552.763261, mean_tau: 0.025526\n",
            " 37637/50000: episode: 1612, duration: 0.105s, episode steps:  14, steps per second: 133, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 14047.667149, mae: 1180.657671, mean_q: 2462.524885, mean_tau: 0.025494\n",
            " 37647/50000: episode: 1613, duration: 0.088s, episode steps:  10, steps per second: 113, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 41635.441699, mae: 1235.727625, mean_q: 2477.350415, mean_tau: 0.025470\n",
            " 37657/50000: episode: 1614, duration: 0.090s, episode steps:  10, steps per second: 111, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 30240.048584, mae: 1122.718231, mean_q: 2317.974023, mean_tau: 0.025450\n",
            " 37669/50000: episode: 1615, duration: 0.094s, episode steps:  12, steps per second: 128, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 22434.813395, mae: 1182.467061, mean_q: 2438.336151, mean_tau: 0.025428\n",
            " 37682/50000: episode: 1616, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 25302.084923, mae: 1168.475079, mean_q: 2411.580116, mean_tau: 0.025404\n",
            " 37693/50000: episode: 1617, duration: 0.108s, episode steps:  11, steps per second: 101, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 26899.962469, mae: 1179.153398, mean_q: 2392.464822, mean_tau: 0.025380\n",
            " 37737/50000: episode: 1618, duration: 0.340s, episode steps:  44, steps per second: 130, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 15164.850675, mae: 1197.398127, mean_q: 2473.371932, mean_tau: 0.025325\n",
            " 37761/50000: episode: 1619, duration: 0.181s, episode steps:  24, steps per second: 133, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 26923.282033, mae: 1178.326548, mean_q: 2401.509460, mean_tau: 0.025258\n",
            " 37772/50000: episode: 1620, duration: 0.098s, episode steps:  11, steps per second: 112, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 15559.255771, mae: 1255.760332, mean_q: 2577.618608, mean_tau: 0.025223\n",
            " 37797/50000: episode: 1621, duration: 0.194s, episode steps:  25, steps per second: 129, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 16877.054678, mae: 1215.155215, mean_q: 2496.053555, mean_tau: 0.025188\n",
            " 37817/50000: episode: 1622, duration: 0.169s, episode steps:  20, steps per second: 118, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 14417.277722, mae: 1204.863809, mean_q: 2492.542773, mean_tau: 0.025143\n",
            " 37828/50000: episode: 1623, duration: 0.086s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 24682.353604, mae: 1175.204545, mean_q: 2399.523460, mean_tau: 0.025112\n",
            " 37842/50000: episode: 1624, duration: 0.104s, episode steps:  14, steps per second: 134, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 19058.566057, mae: 1185.926906, mean_q: 2440.857178, mean_tau: 0.025088\n",
            " 37861/50000: episode: 1625, duration: 0.152s, episode steps:  19, steps per second: 125, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 22144.623831, mae: 1180.690057, mean_q: 2435.887001, mean_tau: 0.025055\n",
            " 37871/50000: episode: 1626, duration: 0.077s, episode steps:  10, steps per second: 130, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 16949.224512, mae: 1238.302869, mean_q: 2548.294312, mean_tau: 0.025026\n",
            " 37907/50000: episode: 1627, duration: 0.277s, episode steps:  36, steps per second: 130, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 18810.825127, mae: 1197.754551, mean_q: 2465.051883, mean_tau: 0.024981\n",
            " 37933/50000: episode: 1628, duration: 0.193s, episode steps:  26, steps per second: 135, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 16951.451998, mae: 1235.829820, mean_q: 2531.648222, mean_tau: 0.024919\n",
            " 37961/50000: episode: 1629, duration: 0.249s, episode steps:  28, steps per second: 113, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19003.771589, mae: 1197.397757, mean_q: 2465.123099, mean_tau: 0.024866\n",
            " 37971/50000: episode: 1630, duration: 0.081s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 19179.046924, mae: 1223.637622, mean_q: 2521.381274, mean_tau: 0.024828\n",
            " 38026/50000: episode: 1631, duration: 0.424s, episode steps:  55, steps per second: 130, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 18531.898850, mae: 1202.284560, mean_q: 2479.264928, mean_tau: 0.024764\n",
            " 38056/50000: episode: 1632, duration: 0.248s, episode steps:  30, steps per second: 121, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 18758.543742, mae: 1210.528178, mean_q: 2470.869686, mean_tau: 0.024680\n",
            " 38075/50000: episode: 1633, duration: 0.159s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 15837.179482, mae: 1223.219640, mean_q: 2514.254266, mean_tau: 0.024631\n",
            " 38097/50000: episode: 1634, duration: 0.172s, episode steps:  22, steps per second: 128, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 14259.191695, mae: 1211.622387, mean_q: 2481.402177, mean_tau: 0.024591\n",
            " 38128/50000: episode: 1635, duration: 0.242s, episode steps:  31, steps per second: 128, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 23374.629300, mae: 1198.286239, mean_q: 2467.070100, mean_tau: 0.024538\n",
            " 38147/50000: episode: 1636, duration: 0.143s, episode steps:  19, steps per second: 133, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 11379.128893, mae: 1228.299336, mean_q: 2532.551411, mean_tau: 0.024489\n",
            " 38159/50000: episode: 1637, duration: 0.103s, episode steps:  12, steps per second: 116, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 34010.078084, mae: 1211.452799, mean_q: 2469.637736, mean_tau: 0.024458\n",
            " 38170/50000: episode: 1638, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 11838.354181, mae: 1200.364258, mean_q: 2475.980069, mean_tau: 0.024435\n",
            " 38188/50000: episode: 1639, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 18590.533040, mae: 1222.217278, mean_q: 2527.459866, mean_tau: 0.024407\n",
            " 38207/50000: episode: 1640, duration: 0.160s, episode steps:  19, steps per second: 118, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 25132.458509, mae: 1208.195338, mean_q: 2467.127878, mean_tau: 0.024370\n",
            " 38226/50000: episode: 1641, duration: 0.152s, episode steps:  19, steps per second: 125, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 20097.946238, mae: 1243.391563, mean_q: 2546.936690, mean_tau: 0.024332\n",
            " 38242/50000: episode: 1642, duration: 0.124s, episode steps:  16, steps per second: 129, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 20446.533905, mae: 1195.628914, mean_q: 2454.060577, mean_tau: 0.024298\n",
            " 38277/50000: episode: 1643, duration: 0.285s, episode steps:  35, steps per second: 123, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 26983.221973, mae: 1205.266169, mean_q: 2441.926008, mean_tau: 0.024247\n",
            " 38298/50000: episode: 1644, duration: 0.167s, episode steps:  21, steps per second: 126, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 15516.289853, mae: 1224.420218, mean_q: 2514.782564, mean_tau: 0.024192\n",
            " 38319/50000: episode: 1645, duration: 0.164s, episode steps:  21, steps per second: 128, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 29026.185082, mae: 1231.319749, mean_q: 2513.695661, mean_tau: 0.024150\n",
            " 38351/50000: episode: 1646, duration: 0.247s, episode steps:  32, steps per second: 130, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 14926.406601, mae: 1211.040974, mean_q: 2496.300140, mean_tau: 0.024098\n",
            " 38374/50000: episode: 1647, duration: 0.169s, episode steps:  23, steps per second: 136, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 24359.021484, mae: 1235.623036, mean_q: 2528.451819, mean_tau: 0.024043\n",
            " 38385/50000: episode: 1648, duration: 0.093s, episode steps:  11, steps per second: 118, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 13972.428112, mae: 1196.318703, mean_q: 2478.620162, mean_tau: 0.024010\n",
            " 38427/50000: episode: 1649, duration: 0.308s, episode steps:  42, steps per second: 136, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 20322.126104, mae: 1220.317002, mean_q: 2495.889954, mean_tau: 0.023957\n",
            " 38459/50000: episode: 1650, duration: 0.287s, episode steps:  32, steps per second: 111, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 22116.515091, mae: 1197.572441, mean_q: 2461.419968, mean_tau: 0.023884\n",
            " 38481/50000: episode: 1651, duration: 0.270s, episode steps:  22, steps per second:  82, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 24923.981601, mae: 1246.196322, mean_q: 2510.682506, mean_tau: 0.023830\n",
            " 38511/50000: episode: 1652, duration: 0.329s, episode steps:  30, steps per second:  91, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 19617.301855, mae: 1229.224908, mean_q: 2534.395402, mean_tau: 0.023779\n",
            " 38538/50000: episode: 1653, duration: 0.302s, episode steps:  27, steps per second:  89, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 24132.504349, mae: 1204.798231, mean_q: 2472.141864, mean_tau: 0.023722\n",
            " 38563/50000: episode: 1654, duration: 0.326s, episode steps:  25, steps per second:  77, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 20008.177461, mae: 1206.788533, mean_q: 2486.688086, mean_tau: 0.023671\n",
            " 38626/50000: episode: 1655, duration: 0.688s, episode steps:  63, steps per second:  92, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 18559.341754, mae: 1223.303599, mean_q: 2511.415206, mean_tau: 0.023584\n",
            " 38659/50000: episode: 1656, duration: 0.394s, episode steps:  33, steps per second:  84, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.394 [0.000, 1.000],  loss: 20386.549316, mae: 1231.866137, mean_q: 2540.894502, mean_tau: 0.023489\n",
            " 38680/50000: episode: 1657, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 26648.485003, mae: 1267.349063, mean_q: 2590.164097, mean_tau: 0.023435\n",
            " 38693/50000: episode: 1658, duration: 0.161s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 26820.049730, mae: 1244.585665, mean_q: 2500.351769, mean_tau: 0.023402\n",
            " 38724/50000: episode: 1659, duration: 0.248s, episode steps:  31, steps per second: 125, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 19775.249693, mae: 1244.971105, mean_q: 2531.517728, mean_tau: 0.023358\n",
            " 38735/50000: episode: 1660, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 16434.431552, mae: 1186.014249, mean_q: 2458.934104, mean_tau: 0.023317\n",
            " 38749/50000: episode: 1661, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 14323.033691, mae: 1226.053249, mean_q: 2520.007568, mean_tau: 0.023292\n",
            " 38803/50000: episode: 1662, duration: 0.422s, episode steps:  54, steps per second: 128, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 21831.133527, mae: 1236.930354, mean_q: 2533.888595, mean_tau: 0.023225\n",
            " 38815/50000: episode: 1663, duration: 0.098s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 17797.694336, mae: 1247.235067, mean_q: 2546.906535, mean_tau: 0.023159\n",
            " 38828/50000: episode: 1664, duration: 0.099s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 13016.688552, mae: 1191.644681, mean_q: 2461.027400, mean_tau: 0.023134\n",
            " 38843/50000: episode: 1665, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 31802.086133, mae: 1233.206087, mean_q: 2491.479427, mean_tau: 0.023107\n",
            " 38862/50000: episode: 1666, duration: 0.147s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 22055.575041, mae: 1253.022339, mean_q: 2563.203600, mean_tau: 0.023073\n",
            " 38872/50000: episode: 1667, duration: 0.088s, episode steps:  10, steps per second: 114, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 12716.423413, mae: 1252.732495, mean_q: 2598.632861, mean_tau: 0.023044\n",
            " 38887/50000: episode: 1668, duration: 0.127s, episode steps:  15, steps per second: 118, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 16174.097363, mae: 1221.391081, mean_q: 2514.503564, mean_tau: 0.023020\n",
            " 38901/50000: episode: 1669, duration: 0.118s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17727.730399, mae: 1206.828727, mean_q: 2509.570940, mean_tau: 0.022991\n",
            " 38934/50000: episode: 1670, duration: 0.244s, episode steps:  33, steps per second: 135, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 20588.379616, mae: 1215.227328, mean_q: 2504.944869, mean_tau: 0.022944\n",
            " 38962/50000: episode: 1671, duration: 0.218s, episode steps:  28, steps per second: 129, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 19809.328692, mae: 1219.457415, mean_q: 2506.435486, mean_tau: 0.022884\n",
            " 38983/50000: episode: 1672, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 23133.662609, mae: 1242.700288, mean_q: 2541.051525, mean_tau: 0.022835\n",
            " 39013/50000: episode: 1673, duration: 0.236s, episode steps:  30, steps per second: 127, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14524.471940, mae: 1267.714083, mean_q: 2608.241659, mean_tau: 0.022785\n",
            " 39026/50000: episode: 1674, duration: 0.101s, episode steps:  13, steps per second: 128, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 47796.428805, mae: 1243.526250, mean_q: 2480.073899, mean_tau: 0.022742\n",
            " 39041/50000: episode: 1675, duration: 0.131s, episode steps:  15, steps per second: 114, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 17389.149056, mae: 1250.298755, mean_q: 2582.628320, mean_tau: 0.022715\n",
            " 39070/50000: episode: 1676, duration: 0.219s, episode steps:  29, steps per second: 132, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 24377.006937, mae: 1228.500189, mean_q: 2530.715290, mean_tau: 0.022671\n",
            " 39105/50000: episode: 1677, duration: 0.264s, episode steps:  35, steps per second: 132, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 19046.310589, mae: 1225.345295, mean_q: 2520.678306, mean_tau: 0.022608\n",
            " 39144/50000: episode: 1678, duration: 0.311s, episode steps:  39, steps per second: 126, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 19464.560954, mae: 1231.022273, mean_q: 2547.665628, mean_tau: 0.022534\n",
            " 39169/50000: episode: 1679, duration: 0.216s, episode steps:  25, steps per second: 116, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 28098.462480, mae: 1235.925791, mean_q: 2522.828271, mean_tau: 0.022471\n",
            " 39191/50000: episode: 1680, duration: 0.185s, episode steps:  22, steps per second: 119, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13347.752697, mae: 1280.152161, mean_q: 2645.189642, mean_tau: 0.022425\n",
            " 39202/50000: episode: 1681, duration: 0.099s, episode steps:  11, steps per second: 111, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 23852.244118, mae: 1262.554410, mean_q: 2621.842551, mean_tau: 0.022392\n",
            " 39231/50000: episode: 1682, duration: 0.234s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 29366.845568, mae: 1256.669223, mean_q: 2566.619797, mean_tau: 0.022352\n",
            " 39257/50000: episode: 1683, duration: 0.213s, episode steps:  26, steps per second: 122, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22155.278827, mae: 1248.264625, mean_q: 2577.762273, mean_tau: 0.022298\n",
            " 39289/50000: episode: 1684, duration: 0.264s, episode steps:  32, steps per second: 121, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.406 [0.000, 1.000],  loss: 16594.718872, mae: 1283.018101, mean_q: 2632.314857, mean_tau: 0.022240\n",
            " 39325/50000: episode: 1685, duration: 0.277s, episode steps:  36, steps per second: 130, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 18518.611864, mae: 1250.952098, mean_q: 2574.556776, mean_tau: 0.022173\n",
            " 39341/50000: episode: 1686, duration: 0.124s, episode steps:  16, steps per second: 129, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 25389.701797, mae: 1263.508911, mean_q: 2577.321198, mean_tau: 0.022122\n",
            " 39364/50000: episode: 1687, duration: 0.193s, episode steps:  23, steps per second: 119, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 15738.954165, mae: 1299.964435, mean_q: 2676.983356, mean_tau: 0.022083\n",
            " 39391/50000: episode: 1688, duration: 0.241s, episode steps:  27, steps per second: 112, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 22071.663683, mae: 1255.150088, mean_q: 2581.138138, mean_tau: 0.022034\n",
            " 39443/50000: episode: 1689, duration: 0.407s, episode steps:  52, steps per second: 128, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 22138.066810, mae: 1281.976937, mean_q: 2642.447585, mean_tau: 0.021955\n",
            " 39469/50000: episode: 1690, duration: 0.203s, episode steps:  26, steps per second: 128, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 22611.863187, mae: 1271.093046, mean_q: 2620.542311, mean_tau: 0.021878\n",
            " 39489/50000: episode: 1691, duration: 0.162s, episode steps:  20, steps per second: 123, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 20784.977466, mae: 1251.173248, mean_q: 2603.139905, mean_tau: 0.021833\n",
            " 39511/50000: episode: 1692, duration: 0.189s, episode steps:  22, steps per second: 116, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 24706.411677, mae: 1265.040206, mean_q: 2602.940507, mean_tau: 0.021791\n",
            " 39536/50000: episode: 1693, duration: 0.203s, episode steps:  25, steps per second: 123, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 19545.413174, mae: 1289.142231, mean_q: 2653.154014, mean_tau: 0.021744\n",
            " 39559/50000: episode: 1694, duration: 0.193s, episode steps:  23, steps per second: 119, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 21689.470990, mae: 1307.463496, mean_q: 2675.592752, mean_tau: 0.021697\n",
            " 39583/50000: episode: 1695, duration: 0.200s, episode steps:  24, steps per second: 120, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 16994.728353, mae: 1295.991364, mean_q: 2665.538401, mean_tau: 0.021650\n",
            " 39604/50000: episode: 1696, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 23278.648786, mae: 1246.157192, mean_q: 2584.319185, mean_tau: 0.021606\n",
            " 39622/50000: episode: 1697, duration: 0.153s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 16753.939155, mae: 1266.621630, mean_q: 2634.990343, mean_tau: 0.021567\n",
            " 39632/50000: episode: 1698, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 41108.389600, mae: 1253.138159, mean_q: 2548.690137, mean_tau: 0.021540\n",
            " 39643/50000: episode: 1699, duration: 0.102s, episode steps:  11, steps per second: 108, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 35085.955433, mae: 1294.113403, mean_q: 2670.321222, mean_tau: 0.021519\n",
            " 39655/50000: episode: 1700, duration: 0.104s, episode steps:  12, steps per second: 116, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 19252.184448, mae: 1304.582336, mean_q: 2672.305908, mean_tau: 0.021496\n",
            " 39684/50000: episode: 1701, duration: 0.227s, episode steps:  29, steps per second: 127, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 15854.344668, mae: 1275.617423, mean_q: 2635.022890, mean_tau: 0.021455\n",
            " 39701/50000: episode: 1702, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 24598.628418, mae: 1298.965670, mean_q: 2627.059901, mean_tau: 0.021410\n",
            " 39755/50000: episode: 1703, duration: 0.394s, episode steps:  54, steps per second: 137, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 28402.919863, mae: 1267.410909, mean_q: 2620.358878, mean_tau: 0.021340\n",
            " 39772/50000: episode: 1704, duration: 0.143s, episode steps:  17, steps per second: 119, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 34298.566521, mae: 1285.242934, mean_q: 2623.774673, mean_tau: 0.021269\n",
            " 39800/50000: episode: 1705, duration: 0.212s, episode steps:  28, steps per second: 132, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 20907.464600, mae: 1291.526206, mean_q: 2641.975377, mean_tau: 0.021225\n",
            " 39812/50000: episode: 1706, duration: 0.094s, episode steps:  12, steps per second: 128, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 20653.623657, mae: 1262.848887, mean_q: 2610.977437, mean_tau: 0.021185\n",
            " 39825/50000: episode: 1707, duration: 0.098s, episode steps:  13, steps per second: 132, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 14780.661095, mae: 1321.472684, mean_q: 2725.947904, mean_tau: 0.021160\n",
            " 39837/50000: episode: 1708, duration: 0.105s, episode steps:  12, steps per second: 114, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 26828.487467, mae: 1301.267263, mean_q: 2666.631999, mean_tau: 0.021136\n",
            " 39863/50000: episode: 1709, duration: 0.192s, episode steps:  26, steps per second: 135, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 23335.772160, mae: 1318.436054, mean_q: 2709.100558, mean_tau: 0.021098\n",
            " 39880/50000: episode: 1710, duration: 0.139s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 27948.900764, mae: 1277.645996, mean_q: 2618.703786, mean_tau: 0.021055\n",
            " 39900/50000: episode: 1711, duration: 0.157s, episode steps:  20, steps per second: 128, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 14994.775830, mae: 1314.307599, mean_q: 2699.891663, mean_tau: 0.021019\n",
            " 39914/50000: episode: 1712, duration: 0.122s, episode steps:  14, steps per second: 115, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 24898.601842, mae: 1265.943464, mean_q: 2610.256574, mean_tau: 0.020985\n",
            " 39949/50000: episode: 1713, duration: 0.307s, episode steps:  35, steps per second: 114, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 27094.230525, mae: 1292.869468, mean_q: 2674.308650, mean_tau: 0.020937\n",
            " 39993/50000: episode: 1714, duration: 0.487s, episode steps:  44, steps per second:  90, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 20087.163558, mae: 1297.454834, mean_q: 2678.566983, mean_tau: 0.020858\n",
            " 40003/50000: episode: 1715, duration: 0.124s, episode steps:  10, steps per second:  81, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 19550.154102, mae: 1286.687830, mean_q: 2630.265991, mean_tau: 0.020805\n",
            " 40040/50000: episode: 1716, duration: 0.410s, episode steps:  37, steps per second:  90, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 27752.288053, mae: 1303.012220, mean_q: 2679.779244, mean_tau: 0.020758\n",
            " 40054/50000: episode: 1717, duration: 0.159s, episode steps:  14, steps per second:  88, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 22582.233974, mae: 1328.777579, mean_q: 2733.837873, mean_tau: 0.020708\n",
            " 40073/50000: episode: 1718, duration: 0.216s, episode steps:  19, steps per second:  88, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 36193.512143, mae: 1349.482615, mean_q: 2722.750989, mean_tau: 0.020675\n",
            " 40092/50000: episode: 1719, duration: 0.224s, episode steps:  19, steps per second:  85, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 47220.811421, mae: 1270.634110, mean_q: 2597.579526, mean_tau: 0.020638\n",
            " 40102/50000: episode: 1720, duration: 0.127s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 26805.582446, mae: 1296.906775, mean_q: 2679.320508, mean_tau: 0.020609\n",
            " 40122/50000: episode: 1721, duration: 0.225s, episode steps:  20, steps per second:  89, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 38383.276013, mae: 1315.023694, mean_q: 2671.054553, mean_tau: 0.020579\n",
            " 40134/50000: episode: 1722, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 20162.219421, mae: 1276.208171, mean_q: 2609.511393, mean_tau: 0.020548\n",
            " 40169/50000: episode: 1723, duration: 0.417s, episode steps:  35, steps per second:  84, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 19126.238198, mae: 1314.698675, mean_q: 2670.340130, mean_tau: 0.020501\n",
            " 40194/50000: episode: 1724, duration: 0.303s, episode steps:  25, steps per second:  82, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 25476.674902, mae: 1325.316240, mean_q: 2721.620371, mean_tau: 0.020442\n",
            " 40210/50000: episode: 1725, duration: 0.131s, episode steps:  16, steps per second: 122, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 29292.075134, mae: 1360.821358, mean_q: 2771.146576, mean_tau: 0.020401\n",
            " 40241/50000: episode: 1726, duration: 0.236s, episode steps:  31, steps per second: 132, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 21566.050120, mae: 1290.774520, mean_q: 2657.691446, mean_tau: 0.020355\n",
            " 40252/50000: episode: 1727, duration: 0.095s, episode steps:  11, steps per second: 115, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 24551.169611, mae: 1332.540672, mean_q: 2724.274525, mean_tau: 0.020313\n",
            " 40284/50000: episode: 1728, duration: 0.246s, episode steps:  32, steps per second: 130, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 26561.305763, mae: 1318.926067, mean_q: 2696.112869, mean_tau: 0.020270\n",
            " 40303/50000: episode: 1729, duration: 0.162s, episode steps:  19, steps per second: 118, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 22141.857589, mae: 1333.485486, mean_q: 2740.757992, mean_tau: 0.020220\n",
            " 40318/50000: episode: 1730, duration: 0.129s, episode steps:  15, steps per second: 116, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 20121.865820, mae: 1320.219466, mean_q: 2697.010173, mean_tau: 0.020186\n",
            " 40332/50000: episode: 1731, duration: 0.119s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 26242.208391, mae: 1321.075771, mean_q: 2716.025460, mean_tau: 0.020157\n",
            " 40362/50000: episode: 1732, duration: 0.233s, episode steps:  30, steps per second: 129, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 22132.379997, mae: 1317.301632, mean_q: 2699.288761, mean_tau: 0.020114\n",
            " 40384/50000: episode: 1733, duration: 0.169s, episode steps:  22, steps per second: 130, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 22272.833968, mae: 1331.807501, mean_q: 2741.391635, mean_tau: 0.020062\n",
            " 40434/50000: episode: 1734, duration: 0.392s, episode steps:  50, steps per second: 128, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 24862.219678, mae: 1317.770588, mean_q: 2698.959985, mean_tau: 0.019991\n",
            " 40452/50000: episode: 1735, duration: 0.132s, episode steps:  18, steps per second: 136, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 26496.017497, mae: 1315.082520, mean_q: 2693.568563, mean_tau: 0.019924\n",
            " 40481/50000: episode: 1736, duration: 0.223s, episode steps:  29, steps per second: 130, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.621 [0.000, 1.000],  loss: 16100.516172, mae: 1314.146106, mean_q: 2698.136618, mean_tau: 0.019877\n",
            " 40507/50000: episode: 1737, duration: 0.188s, episode steps:  26, steps per second: 139, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13549.138146, mae: 1324.630639, mean_q: 2737.946759, mean_tau: 0.019823\n",
            " 40522/50000: episode: 1738, duration: 0.122s, episode steps:  15, steps per second: 123, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 21157.138607, mae: 1365.227091, mean_q: 2776.044222, mean_tau: 0.019782\n",
            " 40544/50000: episode: 1739, duration: 0.167s, episode steps:  22, steps per second: 132, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 26348.760542, mae: 1312.143821, mean_q: 2702.620295, mean_tau: 0.019746\n",
            " 40592/50000: episode: 1740, duration: 0.364s, episode steps:  48, steps per second: 132, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 22977.534958, mae: 1318.276909, mean_q: 2705.258728, mean_tau: 0.019676\n",
            " 40606/50000: episode: 1741, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 18328.835554, mae: 1335.811105, mean_q: 2726.999285, mean_tau: 0.019615\n",
            " 40618/50000: episode: 1742, duration: 0.101s, episode steps:  12, steps per second: 119, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 15049.232483, mae: 1352.139119, mean_q: 2798.591939, mean_tau: 0.019589\n",
            " 40682/50000: episode: 1743, duration: 0.494s, episode steps:  64, steps per second: 130, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 29576.101601, mae: 1340.125923, mean_q: 2726.904865, mean_tau: 0.019514\n",
            " 40713/50000: episode: 1744, duration: 0.246s, episode steps:  31, steps per second: 126, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 26737.867361, mae: 1361.245369, mean_q: 2774.370432, mean_tau: 0.019420\n",
            " 40722/50000: episode: 1745, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 17319.984701, mae: 1282.414185, mean_q: 2630.556803, mean_tau: 0.019380\n",
            " 40752/50000: episode: 1746, duration: 0.240s, episode steps:  30, steps per second: 125, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 16673.062402, mae: 1364.848523, mean_q: 2799.103019, mean_tau: 0.019342\n",
            " 40766/50000: episode: 1747, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 17555.163470, mae: 1324.023560, mean_q: 2729.487130, mean_tau: 0.019298\n",
            " 40778/50000: episode: 1748, duration: 0.096s, episode steps:  12, steps per second: 125, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 26980.703186, mae: 1292.631795, mean_q: 2631.369649, mean_tau: 0.019272\n",
            " 40806/50000: episode: 1749, duration: 0.227s, episode steps:  28, steps per second: 123, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 38948.131592, mae: 1353.477635, mean_q: 2742.749913, mean_tau: 0.019233\n",
            " 40828/50000: episode: 1750, duration: 0.179s, episode steps:  22, steps per second: 123, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 23299.434626, mae: 1326.852423, mean_q: 2719.291992, mean_tau: 0.019183\n",
            " 40844/50000: episode: 1751, duration: 0.132s, episode steps:  16, steps per second: 121, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 38582.528687, mae: 1351.120430, mean_q: 2769.928268, mean_tau: 0.019146\n",
            " 40864/50000: episode: 1752, duration: 0.159s, episode steps:  20, steps per second: 126, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 35604.254004, mae: 1287.899042, mean_q: 2619.282788, mean_tau: 0.019110\n",
            " 40895/50000: episode: 1753, duration: 0.240s, episode steps:  31, steps per second: 129, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 21471.379686, mae: 1358.251209, mean_q: 2792.555892, mean_tau: 0.019060\n",
            " 40925/50000: episode: 1754, duration: 0.254s, episode steps:  30, steps per second: 118, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24001.894954, mae: 1315.646830, mean_q: 2709.802246, mean_tau: 0.018999\n",
            " 40956/50000: episode: 1755, duration: 0.279s, episode steps:  31, steps per second: 111, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 17723.581913, mae: 1356.712887, mean_q: 2792.661416, mean_tau: 0.018939\n",
            " 40973/50000: episode: 1756, duration: 0.130s, episode steps:  17, steps per second: 131, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 17649.331083, mae: 1354.095308, mean_q: 2800.020623, mean_tau: 0.018891\n",
            " 40994/50000: episode: 1757, duration: 0.164s, episode steps:  21, steps per second: 128, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 26752.205218, mae: 1370.160720, mean_q: 2773.562802, mean_tau: 0.018854\n",
            " 41024/50000: episode: 1758, duration: 0.243s, episode steps:  30, steps per second: 123, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 23336.009578, mae: 1337.755143, mean_q: 2754.830615, mean_tau: 0.018803\n",
            " 41042/50000: episode: 1759, duration: 0.144s, episode steps:  18, steps per second: 125, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24029.558539, mae: 1374.039103, mean_q: 2814.005344, mean_tau: 0.018756\n",
            " 41057/50000: episode: 1760, duration: 0.127s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 23349.416960, mae: 1392.970190, mean_q: 2865.495638, mean_tau: 0.018723\n",
            " 41073/50000: episode: 1761, duration: 0.146s, episode steps:  16, steps per second: 109, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 34320.761490, mae: 1390.466965, mean_q: 2813.134949, mean_tau: 0.018692\n",
            " 41111/50000: episode: 1762, duration: 0.300s, episode steps:  38, steps per second: 127, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 19222.507266, mae: 1397.311331, mean_q: 2886.591810, mean_tau: 0.018639\n",
            " 41132/50000: episode: 1763, duration: 0.160s, episode steps:  21, steps per second: 131, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 18767.239955, mae: 1359.244065, mean_q: 2825.910517, mean_tau: 0.018580\n",
            " 41149/50000: episode: 1764, duration: 0.159s, episode steps:  17, steps per second: 107, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 42210.252441, mae: 1389.603573, mean_q: 2841.251379, mean_tau: 0.018543\n",
            " 41182/50000: episode: 1765, duration: 0.274s, episode steps:  33, steps per second: 120, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 25007.222390, mae: 1367.784172, mean_q: 2815.149606, mean_tau: 0.018493\n",
            " 41219/50000: episode: 1766, duration: 0.310s, episode steps:  37, steps per second: 119, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.405 [0.000, 1.000],  loss: 30806.568089, mae: 1360.256173, mean_q: 2795.966190, mean_tau: 0.018424\n",
            " 41232/50000: episode: 1767, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 35953.083120, mae: 1358.193500, mean_q: 2756.772348, mean_tau: 0.018374\n",
            " 41246/50000: episode: 1768, duration: 0.123s, episode steps:  14, steps per second: 114, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 41486.868129, mae: 1392.458444, mean_q: 2801.679339, mean_tau: 0.018348\n",
            " 41285/50000: episode: 1769, duration: 0.325s, episode steps:  39, steps per second: 120, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 23497.026330, mae: 1382.905630, mean_q: 2845.158335, mean_tau: 0.018295\n",
            " 41299/50000: episode: 1770, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 21658.320731, mae: 1358.551627, mean_q: 2805.903355, mean_tau: 0.018243\n",
            " 41313/50000: episode: 1771, duration: 0.134s, episode steps:  14, steps per second: 105, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 28682.043736, mae: 1390.915989, mean_q: 2828.076381, mean_tau: 0.018215\n",
            " 41328/50000: episode: 1772, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 26641.503613, mae: 1353.902417, mean_q: 2751.722054, mean_tau: 0.018186\n",
            " 41351/50000: episode: 1773, duration: 0.199s, episode steps:  23, steps per second: 116, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 20374.191300, mae: 1378.198502, mean_q: 2826.467996, mean_tau: 0.018149\n",
            " 41364/50000: episode: 1774, duration: 0.106s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 25200.500188, mae: 1399.325975, mean_q: 2850.305082, mean_tau: 0.018113\n",
            " 41378/50000: episode: 1775, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 30944.303816, mae: 1349.999904, mean_q: 2774.994838, mean_tau: 0.018086\n",
            " 41390/50000: episode: 1776, duration: 0.100s, episode steps:  12, steps per second: 120, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 27521.813273, mae: 1352.061462, mean_q: 2784.780396, mean_tau: 0.018061\n",
            " 41409/50000: episode: 1777, duration: 0.170s, episode steps:  19, steps per second: 112, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 23226.061446, mae: 1383.023470, mean_q: 2840.472785, mean_tau: 0.018030\n",
            " 41423/50000: episode: 1778, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 25801.550014, mae: 1413.346296, mean_q: 2900.409703, mean_tau: 0.017997\n",
            " 41438/50000: episode: 1779, duration: 0.197s, episode steps:  15, steps per second:  76, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 28157.009505, mae: 1399.669214, mean_q: 2897.916178, mean_tau: 0.017969\n",
            " 41463/50000: episode: 1780, duration: 0.302s, episode steps:  25, steps per second:  83, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 26543.037637, mae: 1373.170659, mean_q: 2800.839551, mean_tau: 0.017929\n",
            " 41481/50000: episode: 1781, duration: 0.210s, episode steps:  18, steps per second:  86, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 24797.324436, mae: 1372.274339, mean_q: 2808.699042, mean_tau: 0.017886\n",
            " 41515/50000: episode: 1782, duration: 0.398s, episode steps:  34, steps per second:  86, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 21820.639648, mae: 1353.083435, mean_q: 2801.854298, mean_tau: 0.017835\n",
            " 41531/50000: episode: 1783, duration: 0.202s, episode steps:  16, steps per second:  79, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 22128.250305, mae: 1369.892708, mean_q: 2828.663925, mean_tau: 0.017785\n",
            " 41599/50000: episode: 1784, duration: 0.802s, episode steps:  68, steps per second:  85, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 24636.351333, mae: 1393.446504, mean_q: 2860.440344, mean_tau: 0.017702\n",
            " 41609/50000: episode: 1785, duration: 0.132s, episode steps:  10, steps per second:  76, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 52434.307178, mae: 1402.446497, mean_q: 2798.522021, mean_tau: 0.017625\n",
            " 41673/50000: episode: 1786, duration: 0.730s, episode steps:  64, steps per second:  88, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 27905.437836, mae: 1397.140001, mean_q: 2862.941196, mean_tau: 0.017552\n",
            " 41694/50000: episode: 1787, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 40859.723610, mae: 1410.249826, mean_q: 2877.385242, mean_tau: 0.017468\n",
            " 41714/50000: episode: 1788, duration: 0.188s, episode steps:  20, steps per second: 106, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 26741.528613, mae: 1452.144470, mean_q: 2965.513977, mean_tau: 0.017427\n",
            " 41758/50000: episode: 1789, duration: 0.348s, episode steps:  44, steps per second: 126, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 29168.107433, mae: 1418.549863, mean_q: 2911.297447, mean_tau: 0.017364\n",
            " 41773/50000: episode: 1790, duration: 0.130s, episode steps:  15, steps per second: 115, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 26767.742904, mae: 1448.962598, mean_q: 2940.736670, mean_tau: 0.017305\n",
            " 41793/50000: episode: 1791, duration: 0.171s, episode steps:  20, steps per second: 117, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16852.067944, mae: 1418.831683, mean_q: 2925.573804, mean_tau: 0.017271\n",
            " 41817/50000: episode: 1792, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 26885.445536, mae: 1405.786591, mean_q: 2917.010142, mean_tau: 0.017227\n",
            " 41849/50000: episode: 1793, duration: 0.277s, episode steps:  32, steps per second: 115, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.406 [0.000, 1.000],  loss: 32757.154968, mae: 1415.499245, mean_q: 2906.762199, mean_tau: 0.017172\n",
            " 41861/50000: episode: 1794, duration: 0.097s, episode steps:  12, steps per second: 124, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 19613.316020, mae: 1414.638245, mean_q: 2928.040263, mean_tau: 0.017128\n",
            " 41901/50000: episode: 1795, duration: 0.316s, episode steps:  40, steps per second: 127, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23553.626575, mae: 1418.231921, mean_q: 2918.110596, mean_tau: 0.017077\n",
            " 41913/50000: episode: 1796, duration: 0.098s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 20851.513550, mae: 1407.532532, mean_q: 2918.072835, mean_tau: 0.017025\n",
            " 41934/50000: episode: 1797, duration: 0.178s, episode steps:  21, steps per second: 118, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 30878.087054, mae: 1418.248105, mean_q: 2896.176758, mean_tau: 0.016992\n",
            " 41950/50000: episode: 1798, duration: 0.135s, episode steps:  16, steps per second: 119, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 30093.205414, mae: 1447.267807, mean_q: 2960.028107, mean_tau: 0.016956\n",
            " 41964/50000: episode: 1799, duration: 0.128s, episode steps:  14, steps per second: 110, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 36673.636300, mae: 1456.170218, mean_q: 2956.660017, mean_tau: 0.016926\n",
            " 41987/50000: episode: 1800, duration: 0.191s, episode steps:  23, steps per second: 121, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 36620.772610, mae: 1405.851525, mean_q: 2856.294625, mean_tau: 0.016890\n",
            " 41997/50000: episode: 1801, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 19421.066064, mae: 1388.225989, mean_q: 2848.821484, mean_tau: 0.016857\n",
            " 42067/50000: episode: 1802, duration: 0.590s, episode steps:  70, steps per second: 119, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.443 [0.000, 1.000],  loss: 27219.095009, mae: 1405.655943, mean_q: 2897.709724, mean_tau: 0.016778\n",
            " 42081/50000: episode: 1803, duration: 0.126s, episode steps:  14, steps per second: 111, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 18542.724749, mae: 1391.762224, mean_q: 2861.989938, mean_tau: 0.016694\n",
            " 42105/50000: episode: 1804, duration: 0.196s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 16151.309194, mae: 1445.084925, mean_q: 3000.481354, mean_tau: 0.016657\n",
            " 42131/50000: episode: 1805, duration: 0.212s, episode steps:  26, steps per second: 123, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 23349.603722, mae: 1434.124249, mean_q: 2942.165706, mean_tau: 0.016607\n",
            " 42157/50000: episode: 1806, duration: 0.199s, episode steps:  26, steps per second: 130, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 27428.593393, mae: 1453.379254, mean_q: 2983.071392, mean_tau: 0.016556\n",
            " 42166/50000: episode: 1807, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 25143.460938, mae: 1448.846273, mean_q: 3003.554905, mean_tau: 0.016521\n",
            " 42204/50000: episode: 1808, duration: 0.302s, episode steps:  38, steps per second: 126, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24958.703106, mae: 1422.287074, mean_q: 2907.906269, mean_tau: 0.016475\n",
            " 42221/50000: episode: 1809, duration: 0.135s, episode steps:  17, steps per second: 126, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 30908.154326, mae: 1448.327888, mean_q: 2978.945815, mean_tau: 0.016420\n",
            " 42238/50000: episode: 1810, duration: 0.147s, episode steps:  17, steps per second: 116, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 14388.183536, mae: 1444.903191, mean_q: 2985.432373, mean_tau: 0.016387\n",
            " 42256/50000: episode: 1811, duration: 0.138s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 23700.680935, mae: 1378.372131, mean_q: 2837.682468, mean_tau: 0.016352\n",
            " 42273/50000: episode: 1812, duration: 0.146s, episode steps:  17, steps per second: 116, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 19819.613324, mae: 1378.367862, mean_q: 2850.695657, mean_tau: 0.016317\n",
            " 42299/50000: episode: 1813, duration: 0.200s, episode steps:  26, steps per second: 130, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.423 [0.000, 1.000],  loss: 22356.230131, mae: 1460.140001, mean_q: 2992.914128, mean_tau: 0.016275\n",
            " 42312/50000: episode: 1814, duration: 0.102s, episode steps:  13, steps per second: 128, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 38270.708233, mae: 1446.644653, mean_q: 2970.889855, mean_tau: 0.016236\n",
            " 42350/50000: episode: 1815, duration: 0.297s, episode steps:  38, steps per second: 128, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21636.824193, mae: 1407.711850, mean_q: 2922.138852, mean_tau: 0.016186\n",
            " 42383/50000: episode: 1816, duration: 0.254s, episode steps:  33, steps per second: 130, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 21839.119688, mae: 1445.716926, mean_q: 2965.599935, mean_tau: 0.016115\n",
            " 42400/50000: episode: 1817, duration: 0.129s, episode steps:  17, steps per second: 131, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 28702.692124, mae: 1490.631111, mean_q: 3044.029713, mean_tau: 0.016066\n",
            " 42420/50000: episode: 1818, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 20794.435718, mae: 1495.594403, mean_q: 3061.761707, mean_tau: 0.016029\n",
            " 42498/50000: episode: 1819, duration: 0.608s, episode steps:  78, steps per second: 128, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 26521.373798, mae: 1448.088853, mean_q: 2979.592827, mean_tau: 0.015932\n",
            " 42508/50000: episode: 1820, duration: 0.086s, episode steps:  10, steps per second: 117, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 38067.532422, mae: 1437.423877, mean_q: 2911.431226, mean_tau: 0.015845\n",
            " 42522/50000: episode: 1821, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 20186.069859, mae: 1461.189218, mean_q: 3000.091675, mean_tau: 0.015821\n",
            " 42542/50000: episode: 1822, duration: 0.160s, episode steps:  20, steps per second: 125, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 38525.141211, mae: 1456.871869, mean_q: 2969.299060, mean_tau: 0.015788\n",
            " 42590/50000: episode: 1823, duration: 0.407s, episode steps:  48, steps per second: 118, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 33278.450877, mae: 1439.821884, mean_q: 2958.498764, mean_tau: 0.015720\n",
            " 42637/50000: episode: 1824, duration: 0.383s, episode steps:  47, steps per second: 123, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 25351.720563, mae: 1452.435718, mean_q: 2961.896729, mean_tau: 0.015626\n",
            " 42657/50000: episode: 1825, duration: 0.156s, episode steps:  20, steps per second: 129, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 14585.567017, mae: 1502.190173, mean_q: 3102.733850, mean_tau: 0.015560\n",
            " 42670/50000: episode: 1826, duration: 0.117s, episode steps:  13, steps per second: 111, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 35987.576247, mae: 1487.353271, mean_q: 3055.590201, mean_tau: 0.015527\n",
            " 42681/50000: episode: 1827, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 14916.919478, mae: 1446.054488, mean_q: 2998.018288, mean_tau: 0.015504\n",
            " 42693/50000: episode: 1828, duration: 0.093s, episode steps:  12, steps per second: 128, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 23889.760091, mae: 1476.128194, mean_q: 3026.639343, mean_tau: 0.015481\n",
            " 42708/50000: episode: 1829, duration: 0.131s, episode steps:  15, steps per second: 115, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 29701.331543, mae: 1507.160815, mean_q: 3088.148584, mean_tau: 0.015454\n",
            " 42756/50000: episode: 1830, duration: 0.373s, episode steps:  48, steps per second: 129, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 37693.698542, mae: 1504.649882, mean_q: 3068.362249, mean_tau: 0.015392\n",
            " 42774/50000: episode: 1831, duration: 0.147s, episode steps:  18, steps per second: 123, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 15917.897515, mae: 1529.445889, mean_q: 3165.301771, mean_tau: 0.015326\n",
            " 42787/50000: episode: 1832, duration: 0.114s, episode steps:  13, steps per second: 114, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 48549.490648, mae: 1469.690768, mean_q: 3013.342079, mean_tau: 0.015296\n",
            " 42804/50000: episode: 1833, duration: 0.139s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 25860.541820, mae: 1478.324642, mean_q: 3072.590203, mean_tau: 0.015266\n",
            " 42830/50000: episode: 1834, duration: 0.229s, episode steps:  26, steps per second: 114, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 33098.385291, mae: 1507.972314, mean_q: 3085.525841, mean_tau: 0.015223\n",
            " 42855/50000: episode: 1835, duration: 0.195s, episode steps:  25, steps per second: 128, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 21624.031025, mae: 1539.122651, mean_q: 3171.284717, mean_tau: 0.015173\n",
            " 42872/50000: episode: 1836, duration: 0.168s, episode steps:  17, steps per second: 101, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 21787.658375, mae: 1487.757389, mean_q: 3076.728429, mean_tau: 0.015131\n",
            " 42885/50000: episode: 1837, duration: 0.150s, episode steps:  13, steps per second:  86, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 25624.210337, mae: 1416.967557, mean_q: 2935.094426, mean_tau: 0.015102\n",
            " 42903/50000: episode: 1838, duration: 0.212s, episode steps:  18, steps per second:  85, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 22877.342909, mae: 1508.179979, mean_q: 3100.017375, mean_tau: 0.015071\n",
            " 42928/50000: episode: 1839, duration: 0.300s, episode steps:  25, steps per second:  83, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 28392.508438, mae: 1480.495293, mean_q: 3046.086016, mean_tau: 0.015028\n",
            " 42938/50000: episode: 1840, duration: 0.117s, episode steps:  10, steps per second:  85, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 40223.582568, mae: 1468.907617, mean_q: 2999.352002, mean_tau: 0.014994\n",
            " 42957/50000: episode: 1841, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 35865.043020, mae: 1499.065250, mean_q: 3069.525198, mean_tau: 0.014965\n",
            " 42970/50000: episode: 1842, duration: 0.154s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 30908.853403, mae: 1484.275616, mean_q: 3033.211651, mean_tau: 0.014933\n",
            " 42989/50000: episode: 1843, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 23818.000488, mae: 1519.144737, mean_q: 3103.126645, mean_tau: 0.014902\n",
            " 43008/50000: episode: 1844, duration: 0.215s, episode steps:  19, steps per second:  89, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 20834.785233, mae: 1509.448242, mean_q: 3102.995785, mean_tau: 0.014864\n",
            " 43021/50000: episode: 1845, duration: 0.166s, episode steps:  13, steps per second:  78, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 15741.013371, mae: 1497.761108, mean_q: 3106.515155, mean_tau: 0.014832\n",
            " 43047/50000: episode: 1846, duration: 0.310s, episode steps:  26, steps per second:  84, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 25473.307505, mae: 1480.364873, mean_q: 3038.759399, mean_tau: 0.014794\n",
            " 43060/50000: episode: 1847, duration: 0.153s, episode steps:  13, steps per second:  85, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 46843.642541, mae: 1503.443866, mean_q: 3053.764799, mean_tau: 0.014755\n",
            " 43074/50000: episode: 1848, duration: 0.174s, episode steps:  14, steps per second:  80, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 25422.487584, mae: 1472.250837, mean_q: 3057.651018, mean_tau: 0.014728\n",
            " 43086/50000: episode: 1849, duration: 0.147s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 33938.371989, mae: 1484.292308, mean_q: 3046.336731, mean_tau: 0.014703\n",
            " 43098/50000: episode: 1850, duration: 0.159s, episode steps:  12, steps per second:  75, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 27083.122559, mae: 1515.519135, mean_q: 3114.649862, mean_tau: 0.014679\n",
            " 43112/50000: episode: 1851, duration: 0.180s, episode steps:  14, steps per second:  78, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 19468.268520, mae: 1487.311951, mean_q: 3079.945243, mean_tau: 0.014653\n",
            " 43133/50000: episode: 1852, duration: 0.181s, episode steps:  21, steps per second: 116, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 28863.083403, mae: 1508.090245, mean_q: 3083.717460, mean_tau: 0.014618\n",
            " 43172/50000: episode: 1853, duration: 0.317s, episode steps:  39, steps per second: 123, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 25809.820250, mae: 1482.439159, mean_q: 3036.228553, mean_tau: 0.014559\n",
            " 43230/50000: episode: 1854, duration: 0.481s, episode steps:  58, steps per second: 120, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 26610.734215, mae: 1491.016503, mean_q: 3064.995744, mean_tau: 0.014463\n",
            " 43269/50000: episode: 1855, duration: 0.316s, episode steps:  39, steps per second: 123, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 32232.651981, mae: 1496.706991, mean_q: 3066.322065, mean_tau: 0.014367\n",
            " 43287/50000: episode: 1856, duration: 0.137s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 31806.450033, mae: 1523.199361, mean_q: 3095.689602, mean_tau: 0.014311\n",
            " 43316/50000: episode: 1857, duration: 0.233s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 34920.501852, mae: 1513.423496, mean_q: 3107.769683, mean_tau: 0.014264\n",
            " 43345/50000: episode: 1858, duration: 0.239s, episode steps:  29, steps per second: 121, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 26960.585752, mae: 1508.431586, mean_q: 3101.151005, mean_tau: 0.014207\n",
            " 43356/50000: episode: 1859, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 18743.794833, mae: 1508.373657, mean_q: 3107.869296, mean_tau: 0.014167\n",
            " 43387/50000: episode: 1860, duration: 0.242s, episode steps:  31, steps per second: 128, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 25698.817178, mae: 1517.204862, mean_q: 3120.503678, mean_tau: 0.014125\n",
            " 43398/50000: episode: 1861, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 12497.972701, mae: 1472.668812, mean_q: 3107.749112, mean_tau: 0.014084\n",
            " 43410/50000: episode: 1862, duration: 0.095s, episode steps:  12, steps per second: 126, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 22311.291585, mae: 1522.583954, mean_q: 3117.913005, mean_tau: 0.014061\n",
            " 43437/50000: episode: 1863, duration: 0.212s, episode steps:  27, steps per second: 127, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 27655.975387, mae: 1518.699974, mean_q: 3141.525264, mean_tau: 0.014022\n",
            " 43453/50000: episode: 1864, duration: 0.135s, episode steps:  16, steps per second: 118, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 34803.597046, mae: 1506.398346, mean_q: 3079.632111, mean_tau: 0.013980\n",
            " 43468/50000: episode: 1865, duration: 0.139s, episode steps:  15, steps per second: 108, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 31499.596387, mae: 1489.759871, mean_q: 3072.244564, mean_tau: 0.013949\n",
            " 43485/50000: episode: 1866, duration: 0.129s, episode steps:  17, steps per second: 131, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 34245.893670, mae: 1507.233147, mean_q: 3109.216021, mean_tau: 0.013918\n",
            " 43500/50000: episode: 1867, duration: 0.122s, episode steps:  15, steps per second: 123, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 29772.681510, mae: 1527.220247, mean_q: 3161.826953, mean_tau: 0.013886\n",
            " 43537/50000: episode: 1868, duration: 0.287s, episode steps:  37, steps per second: 129, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 23758.909127, mae: 1542.864132, mean_q: 3174.531949, mean_tau: 0.013834\n",
            " 43551/50000: episode: 1869, duration: 0.115s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 15720.468488, mae: 1531.039019, mean_q: 3173.739153, mean_tau: 0.013784\n",
            " 43575/50000: episode: 1870, duration: 0.190s, episode steps:  24, steps per second: 127, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 33480.210246, mae: 1526.653895, mean_q: 3123.674713, mean_tau: 0.013746\n",
            " 43594/50000: episode: 1871, duration: 0.157s, episode steps:  19, steps per second: 121, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 20468.750617, mae: 1515.054707, mean_q: 3142.730096, mean_tau: 0.013704\n",
            " 43639/50000: episode: 1872, duration: 0.347s, episode steps:  45, steps per second: 130, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 27612.824468, mae: 1562.274870, mean_q: 3201.433285, mean_tau: 0.013640\n",
            " 43649/50000: episode: 1873, duration: 0.080s, episode steps:  10, steps per second: 124, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 28456.065723, mae: 1535.268469, mean_q: 3132.808398, mean_tau: 0.013586\n",
            " 43666/50000: episode: 1874, duration: 0.139s, episode steps:  17, steps per second: 123, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 23138.534869, mae: 1528.599746, mean_q: 3134.996482, mean_tau: 0.013559\n",
            " 43678/50000: episode: 1875, duration: 0.098s, episode steps:  12, steps per second: 122, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 29515.474935, mae: 1554.057760, mean_q: 3169.246948, mean_tau: 0.013530\n",
            " 43701/50000: episode: 1876, duration: 0.205s, episode steps:  23, steps per second: 112, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 32174.645847, mae: 1517.054518, mean_q: 3137.134776, mean_tau: 0.013496\n",
            " 43714/50000: episode: 1877, duration: 0.112s, episode steps:  13, steps per second: 116, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 34859.421312, mae: 1556.001352, mean_q: 3195.958177, mean_tau: 0.013460\n",
            " 43727/50000: episode: 1878, duration: 0.121s, episode steps:  13, steps per second: 107, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 16549.928110, mae: 1554.492911, mean_q: 3236.480431, mean_tau: 0.013434\n",
            " 43742/50000: episode: 1879, duration: 0.123s, episode steps:  15, steps per second: 122, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.733 [0.000, 1.000],  loss: 35529.664714, mae: 1498.627490, mean_q: 3098.223470, mean_tau: 0.013407\n",
            " 43770/50000: episode: 1880, duration: 0.217s, episode steps:  28, steps per second: 129, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 23613.195487, mae: 1535.582450, mean_q: 3141.737837, mean_tau: 0.013364\n",
            " 43781/50000: episode: 1881, duration: 0.090s, episode steps:  11, steps per second: 122, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 23460.768466, mae: 1521.322987, mean_q: 3104.546475, mean_tau: 0.013326\n",
            " 43797/50000: episode: 1882, duration: 0.143s, episode steps:  16, steps per second: 112, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 31484.211914, mae: 1574.247437, mean_q: 3244.765579, mean_tau: 0.013299\n",
            " 43820/50000: episode: 1883, duration: 0.176s, episode steps:  23, steps per second: 131, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 30295.554794, mae: 1580.360362, mean_q: 3236.942149, mean_tau: 0.013260\n",
            " 43835/50000: episode: 1884, duration: 0.127s, episode steps:  15, steps per second: 118, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 20482.094222, mae: 1513.980941, mean_q: 3132.384993, mean_tau: 0.013223\n",
            " 43862/50000: episode: 1885, duration: 0.208s, episode steps:  27, steps per second: 130, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 26699.875904, mae: 1564.025897, mean_q: 3226.245397, mean_tau: 0.013181\n",
            " 43881/50000: episode: 1886, duration: 0.152s, episode steps:  19, steps per second: 125, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 29005.804688, mae: 1529.090313, mean_q: 3136.974545, mean_tau: 0.013135\n",
            " 43895/50000: episode: 1887, duration: 0.115s, episode steps:  14, steps per second: 121, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 26489.716169, mae: 1598.525800, mean_q: 3269.708636, mean_tau: 0.013103\n",
            " 43933/50000: episode: 1888, duration: 0.297s, episode steps:  38, steps per second: 128, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 31213.208933, mae: 1513.979929, mean_q: 3122.359844, mean_tau: 0.013051\n",
            " 43948/50000: episode: 1889, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 34207.990951, mae: 1579.081356, mean_q: 3252.229590, mean_tau: 0.012999\n",
            " 43967/50000: episode: 1890, duration: 0.176s, episode steps:  19, steps per second: 108, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 34286.870040, mae: 1602.382061, mean_q: 3266.189337, mean_tau: 0.012965\n",
            " 43986/50000: episode: 1891, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 29752.062449, mae: 1569.961638, mean_q: 3245.522217, mean_tau: 0.012928\n",
            " 44012/50000: episode: 1892, duration: 0.215s, episode steps:  26, steps per second: 121, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23578.113431, mae: 1580.761963, mean_q: 3257.690411, mean_tau: 0.012883\n",
            " 44027/50000: episode: 1893, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 33475.363672, mae: 1566.968009, mean_q: 3210.041423, mean_tau: 0.012842\n",
            " 44042/50000: episode: 1894, duration: 0.128s, episode steps:  15, steps per second: 117, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 37016.327507, mae: 1582.089583, mean_q: 3271.791536, mean_tau: 0.012813\n",
            " 44053/50000: episode: 1895, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 20408.600098, mae: 1538.486483, mean_q: 3186.974543, mean_tau: 0.012787\n",
            " 44071/50000: episode: 1896, duration: 0.147s, episode steps:  18, steps per second: 122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 35229.586833, mae: 1540.374424, mean_q: 3195.068536, mean_tau: 0.012758\n",
            " 44091/50000: episode: 1897, duration: 0.186s, episode steps:  20, steps per second: 108, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 30094.383716, mae: 1591.707526, mean_q: 3236.641650, mean_tau: 0.012721\n",
            " 44103/50000: episode: 1898, duration: 0.102s, episode steps:  12, steps per second: 117, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 27958.468750, mae: 1645.096395, mean_q: 3380.649353, mean_tau: 0.012689\n",
            " 44113/50000: episode: 1899, duration: 0.081s, episode steps:  10, steps per second: 124, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 31167.356348, mae: 1629.842542, mean_q: 3337.671216, mean_tau: 0.012667\n",
            " 44124/50000: episode: 1900, duration: 0.088s, episode steps:  11, steps per second: 125, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 14966.250910, mae: 1580.895186, mean_q: 3306.196378, mean_tau: 0.012646\n",
            " 44144/50000: episode: 1901, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 32122.237793, mae: 1558.186963, mean_q: 3199.828308, mean_tau: 0.012616\n",
            " 44162/50000: episode: 1902, duration: 0.138s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 27774.647976, mae: 1591.853373, mean_q: 3254.749390, mean_tau: 0.012578\n",
            " 44201/50000: episode: 1903, duration: 0.309s, episode steps:  39, steps per second: 126, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 25229.309865, mae: 1595.585568, mean_q: 3278.190079, mean_tau: 0.012522\n",
            " 44238/50000: episode: 1904, duration: 0.298s, episode steps:  37, steps per second: 124, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 29252.338689, mae: 1611.910183, mean_q: 3306.469568, mean_tau: 0.012446\n",
            " 44249/50000: episode: 1905, duration: 0.087s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 41360.661222, mae: 1599.199430, mean_q: 3262.561390, mean_tau: 0.012399\n",
            " 44271/50000: episode: 1906, duration: 0.178s, episode steps:  22, steps per second: 124, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 34116.282981, mae: 1586.170366, mean_q: 3250.996926, mean_tau: 0.012366\n",
            " 44317/50000: episode: 1907, duration: 0.362s, episode steps:  46, steps per second: 127, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 27421.170601, mae: 1564.183522, mean_q: 3240.773135, mean_tau: 0.012299\n",
            " 44330/50000: episode: 1908, duration: 0.110s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 34039.806190, mae: 1548.450524, mean_q: 3156.220328, mean_tau: 0.012240\n",
            " 44349/50000: episode: 1909, duration: 0.233s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 30705.119321, mae: 1598.290225, mean_q: 3301.158447, mean_tau: 0.012209\n",
            " 44360/50000: episode: 1910, duration: 0.130s, episode steps:  11, steps per second:  85, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 23780.608221, mae: 1636.991300, mean_q: 3400.803733, mean_tau: 0.012179\n",
            " 44375/50000: episode: 1911, duration: 0.185s, episode steps:  15, steps per second:  81, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 32987.304102, mae: 1606.063037, mean_q: 3301.331478, mean_tau: 0.012153\n",
            " 44391/50000: episode: 1912, duration: 0.199s, episode steps:  16, steps per second:  80, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 28963.451080, mae: 1601.919853, mean_q: 3284.943848, mean_tau: 0.012123\n",
            " 44410/50000: episode: 1913, duration: 0.227s, episode steps:  19, steps per second:  84, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 28142.067537, mae: 1591.166510, mean_q: 3236.675344, mean_tau: 0.012088\n",
            " 44422/50000: episode: 1914, duration: 0.141s, episode steps:  12, steps per second:  85, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 14339.759766, mae: 1623.226908, mean_q: 3327.707438, mean_tau: 0.012057\n",
            " 44454/50000: episode: 1915, duration: 0.383s, episode steps:  32, steps per second:  84, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 38701.390244, mae: 1582.085075, mean_q: 3253.808258, mean_tau: 0.012014\n",
            " 44466/50000: episode: 1916, duration: 0.158s, episode steps:  12, steps per second:  76, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 32267.058594, mae: 1636.886292, mean_q: 3341.561320, mean_tau: 0.011970\n",
            " 44488/50000: episode: 1917, duration: 0.278s, episode steps:  22, steps per second:  79, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 29383.695668, mae: 1603.659934, mean_q: 3322.198819, mean_tau: 0.011937\n",
            " 44502/50000: episode: 1918, duration: 0.166s, episode steps:  14, steps per second:  84, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24742.085938, mae: 1599.681065, mean_q: 3343.183210, mean_tau: 0.011901\n",
            " 44515/50000: episode: 1919, duration: 0.180s, episode steps:  13, steps per second:  72, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 19520.252667, mae: 1652.952871, mean_q: 3406.333571, mean_tau: 0.011874\n",
            " 44529/50000: episode: 1920, duration: 0.169s, episode steps:  14, steps per second:  83, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 29211.021415, mae: 1629.288984, mean_q: 3361.171143, mean_tau: 0.011847\n",
            " 44554/50000: episode: 1921, duration: 0.306s, episode steps:  25, steps per second:  82, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 29286.096113, mae: 1654.295981, mean_q: 3400.728555, mean_tau: 0.011809\n",
            " 44564/50000: episode: 1922, duration: 0.126s, episode steps:  10, steps per second:  79, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 27843.802588, mae: 1594.119299, mean_q: 3307.494604, mean_tau: 0.011774\n",
            " 44586/50000: episode: 1923, duration: 0.186s, episode steps:  22, steps per second: 118, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 40583.251953, mae: 1612.657310, mean_q: 3266.990390, mean_tau: 0.011742\n",
            " 44630/50000: episode: 1924, duration: 0.355s, episode steps:  44, steps per second: 124, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 25322.012235, mae: 1621.983584, mean_q: 3352.591242, mean_tau: 0.011677\n",
            " 44693/50000: episode: 1925, duration: 0.475s, episode steps:  63, steps per second: 133, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 28882.100268, mae: 1619.598267, mean_q: 3322.950641, mean_tau: 0.011571\n",
            " 44707/50000: episode: 1926, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 25030.647705, mae: 1595.929199, mean_q: 3307.532750, mean_tau: 0.011495\n",
            " 44722/50000: episode: 1927, duration: 0.133s, episode steps:  15, steps per second: 113, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 50909.614062, mae: 1638.116040, mean_q: 3339.026432, mean_tau: 0.011466\n",
            " 44743/50000: episode: 1928, duration: 0.174s, episode steps:  21, steps per second: 121, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 28884.126209, mae: 1629.529018, mean_q: 3347.925595, mean_tau: 0.011431\n",
            " 44764/50000: episode: 1929, duration: 0.170s, episode steps:  21, steps per second: 124, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 27878.717587, mae: 1589.160145, mean_q: 3280.761800, mean_tau: 0.011389\n",
            " 44810/50000: episode: 1930, duration: 0.352s, episode steps:  46, steps per second: 131, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 28471.641352, mae: 1645.026083, mean_q: 3381.250372, mean_tau: 0.011323\n",
            " 44822/50000: episode: 1931, duration: 0.098s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 29042.358765, mae: 1635.604390, mean_q: 3329.732076, mean_tau: 0.011265\n",
            " 44838/50000: episode: 1932, duration: 0.132s, episode steps:  16, steps per second: 122, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 26971.991211, mae: 1605.030769, mean_q: 3307.808075, mean_tau: 0.011238\n",
            " 44849/50000: episode: 1933, duration: 0.088s, episode steps:  11, steps per second: 126, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 23742.051225, mae: 1669.142034, mean_q: 3454.248446, mean_tau: 0.011211\n",
            " 44878/50000: episode: 1934, duration: 0.245s, episode steps:  29, steps per second: 118, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 26570.424114, mae: 1632.077085, mean_q: 3331.197055, mean_tau: 0.011171\n",
            " 44896/50000: episode: 1935, duration: 0.130s, episode steps:  18, steps per second: 138, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39355.625868, mae: 1655.570509, mean_q: 3388.913303, mean_tau: 0.011125\n",
            " 44913/50000: episode: 1936, duration: 0.146s, episode steps:  17, steps per second: 117, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 23836.449018, mae: 1616.829568, mean_q: 3362.988726, mean_tau: 0.011090\n",
            " 44939/50000: episode: 1937, duration: 0.192s, episode steps:  26, steps per second: 135, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 32732.519700, mae: 1643.702923, mean_q: 3372.498225, mean_tau: 0.011048\n",
            " 44950/50000: episode: 1938, duration: 0.106s, episode steps:  11, steps per second: 104, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 37810.767134, mae: 1574.532027, mean_q: 3231.653653, mean_tau: 0.011011\n",
            " 44984/50000: episode: 1939, duration: 0.252s, episode steps:  34, steps per second: 135, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 37660.193503, mae: 1655.023287, mean_q: 3383.589815, mean_tau: 0.010966\n",
            " 45011/50000: episode: 1940, duration: 0.203s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 26283.726562, mae: 1658.549963, mean_q: 3436.883482, mean_tau: 0.010906\n",
            " 45033/50000: episode: 1941, duration: 0.175s, episode steps:  22, steps per second: 126, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 38200.302868, mae: 1611.302257, mean_q: 3276.474410, mean_tau: 0.010857\n",
            " 45056/50000: episode: 1942, duration: 0.182s, episode steps:  23, steps per second: 127, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 35306.912194, mae: 1663.068184, mean_q: 3398.232730, mean_tau: 0.010813\n",
            " 45080/50000: episode: 1943, duration: 0.175s, episode steps:  24, steps per second: 137, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 20075.827464, mae: 1647.936239, mean_q: 3415.493184, mean_tau: 0.010766\n",
            " 45114/50000: episode: 1944, duration: 0.264s, episode steps:  34, steps per second: 129, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 34064.620275, mae: 1642.509460, mean_q: 3373.013298, mean_tau: 0.010709\n",
            " 45136/50000: episode: 1945, duration: 0.176s, episode steps:  22, steps per second: 125, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 26043.643333, mae: 1628.721225, mean_q: 3375.291781, mean_tau: 0.010653\n",
            " 45169/50000: episode: 1946, duration: 0.239s, episode steps:  33, steps per second: 138, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 18757.844778, mae: 1665.868183, mean_q: 3425.915550, mean_tau: 0.010599\n",
            " 45182/50000: episode: 1947, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 25064.301194, mae: 1690.032123, mean_q: 3468.806866, mean_tau: 0.010554\n",
            " 45207/50000: episode: 1948, duration: 0.191s, episode steps:  25, steps per second: 131, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 14731.898242, mae: 1652.669902, mean_q: 3414.051387, mean_tau: 0.010516\n",
            " 45228/50000: episode: 1949, duration: 0.159s, episode steps:  21, steps per second: 132, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 18611.065104, mae: 1676.386271, mean_q: 3470.388928, mean_tau: 0.010470\n",
            " 45254/50000: episode: 1950, duration: 0.196s, episode steps:  26, steps per second: 133, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 33794.876653, mae: 1657.406475, mean_q: 3393.466233, mean_tau: 0.010424\n",
            " 45269/50000: episode: 1951, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 16959.582813, mae: 1710.775195, mean_q: 3555.068587, mean_tau: 0.010383\n",
            " 45284/50000: episode: 1952, duration: 0.119s, episode steps:  15, steps per second: 126, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 32394.372428, mae: 1686.181372, mean_q: 3469.416357, mean_tau: 0.010354\n",
            " 45295/50000: episode: 1953, duration: 0.085s, episode steps:  11, steps per second: 130, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 31216.597301, mae: 1670.205311, mean_q: 3421.285822, mean_tau: 0.010328\n",
            " 45320/50000: episode: 1954, duration: 0.194s, episode steps:  25, steps per second: 129, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 18555.305879, mae: 1689.285396, mean_q: 3457.678516, mean_tau: 0.010292\n",
            " 45387/50000: episode: 1955, duration: 0.509s, episode steps:  67, steps per second: 132, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 30607.383979, mae: 1680.060492, mean_q: 3455.174367, mean_tau: 0.010201\n",
            " 45400/50000: episode: 1956, duration: 0.128s, episode steps:  13, steps per second: 101, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 33675.588717, mae: 1660.266808, mean_q: 3437.738394, mean_tau: 0.010122\n",
            " 45417/50000: episode: 1957, duration: 0.154s, episode steps:  17, steps per second: 110, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 29232.170726, mae: 1629.881017, mean_q: 3379.651784, mean_tau: 0.010092\n",
            " 45440/50000: episode: 1958, duration: 0.192s, episode steps:  23, steps per second: 120, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 53869.751104, mae: 1684.387828, mean_q: 3433.694612, mean_tau: 0.010053\n",
            " 45456/50000: episode: 1959, duration: 0.141s, episode steps:  16, steps per second: 113, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 17414.920258, mae: 1660.888817, mean_q: 3463.184784, mean_tau: 0.010014\n",
            " 45470/50000: episode: 1960, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 33390.426967, mae: 1673.364450, mean_q: 3417.454084, mean_tau: 0.009984\n",
            " 45505/50000: episode: 1961, duration: 0.285s, episode steps:  35, steps per second: 123, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 38805.537653, mae: 1671.800956, mean_q: 3444.049156, mean_tau: 0.009936\n",
            " 45516/50000: episode: 1962, duration: 0.099s, episode steps:  11, steps per second: 111, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 17705.153986, mae: 1684.119296, mean_q: 3474.435258, mean_tau: 0.009890\n",
            " 45529/50000: episode: 1963, duration: 0.109s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 27777.653433, mae: 1675.663105, mean_q: 3450.303730, mean_tau: 0.009866\n",
            " 45561/50000: episode: 1964, duration: 0.265s, episode steps:  32, steps per second: 121, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 34620.570251, mae: 1667.855957, mean_q: 3428.203346, mean_tau: 0.009822\n",
            " 45606/50000: episode: 1965, duration: 0.346s, episode steps:  45, steps per second: 130, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 33420.129861, mae: 1677.206440, mean_q: 3454.148334, mean_tau: 0.009746\n",
            " 45619/50000: episode: 1966, duration: 0.097s, episode steps:  13, steps per second: 134, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 25815.289663, mae: 1746.763775, mean_q: 3579.529147, mean_tau: 0.009688\n",
            " 45652/50000: episode: 1967, duration: 0.259s, episode steps:  33, steps per second: 127, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 26834.755815, mae: 1654.886156, mean_q: 3425.853767, mean_tau: 0.009643\n",
            " 45677/50000: episode: 1968, duration: 0.198s, episode steps:  25, steps per second: 126, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 54909.600664, mae: 1666.265605, mean_q: 3396.912422, mean_tau: 0.009585\n",
            " 45694/50000: episode: 1969, duration: 0.135s, episode steps:  17, steps per second: 126, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 31882.014218, mae: 1660.566981, mean_q: 3436.178539, mean_tau: 0.009544\n",
            " 45743/50000: episode: 1970, duration: 0.374s, episode steps:  49, steps per second: 131, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.408 [0.000, 1.000],  loss: 24315.590252, mae: 1704.675490, mean_q: 3508.895029, mean_tau: 0.009478\n",
            " 45753/50000: episode: 1971, duration: 0.094s, episode steps:  10, steps per second: 106, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 32466.894775, mae: 1731.164587, mean_q: 3509.902808, mean_tau: 0.009420\n",
            " 45771/50000: episode: 1972, duration: 0.167s, episode steps:  18, steps per second: 108, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20527.085395, mae: 1665.049167, mean_q: 3435.385593, mean_tau: 0.009392\n",
            " 45786/50000: episode: 1973, duration: 0.118s, episode steps:  15, steps per second: 128, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 47310.830404, mae: 1764.491081, mean_q: 3587.420687, mean_tau: 0.009360\n",
            " 45803/50000: episode: 1974, duration: 0.145s, episode steps:  17, steps per second: 117, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 28426.130974, mae: 1682.176815, mean_q: 3456.264376, mean_tau: 0.009328\n",
            " 45824/50000: episode: 1975, duration: 0.190s, episode steps:  21, steps per second: 110, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.714 [0.000, 1.000],  loss: 22320.105166, mae: 1739.488903, mean_q: 3561.616350, mean_tau: 0.009290\n",
            " 45865/50000: episode: 1976, duration: 0.468s, episode steps:  41, steps per second:  88, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 38331.259087, mae: 1712.994793, mean_q: 3497.899188, mean_tau: 0.009229\n",
            " 45907/50000: episode: 1977, duration: 0.486s, episode steps:  42, steps per second:  86, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 29472.072324, mae: 1698.995268, mean_q: 3502.512748, mean_tau: 0.009147\n",
            " 45919/50000: episode: 1978, duration: 0.140s, episode steps:  12, steps per second:  86, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 40413.870321, mae: 1775.149292, mean_q: 3631.711019, mean_tau: 0.009093\n",
            " 45949/50000: episode: 1979, duration: 0.348s, episode steps:  30, steps per second:  86, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 41245.412142, mae: 1742.877059, mean_q: 3576.832975, mean_tau: 0.009052\n",
            " 45962/50000: episode: 1980, duration: 0.151s, episode steps:  13, steps per second:  86, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 44390.593863, mae: 1722.047157, mean_q: 3547.535682, mean_tau: 0.009009\n",
            " 45975/50000: episode: 1981, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 55204.525954, mae: 1727.067533, mean_q: 3499.203707, mean_tau: 0.008983\n",
            " 45984/50000: episode: 1982, duration: 0.107s, episode steps:   9, steps per second:  84, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 43278.448296, mae: 1729.846422, mean_q: 3559.908637, mean_tau: 0.008962\n",
            " 46001/50000: episode: 1983, duration: 0.224s, episode steps:  17, steps per second:  76, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 35844.636116, mae: 1741.358456, mean_q: 3569.806167, mean_tau: 0.008936\n",
            " 46016/50000: episode: 1984, duration: 0.177s, episode steps:  15, steps per second:  85, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 25020.464225, mae: 1732.762614, mean_q: 3576.165771, mean_tau: 0.008904\n",
            " 46026/50000: episode: 1985, duration: 0.143s, episode steps:  10, steps per second:  70, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 41131.713281, mae: 1658.428210, mean_q: 3359.492920, mean_tau: 0.008879\n",
            " 46119/50000: episode: 1986, duration: 0.871s, episode steps:  93, steps per second: 107, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 35134.523283, mae: 1742.151747, mean_q: 3581.437883, mean_tau: 0.008777\n",
            " 46134/50000: episode: 1987, duration: 0.120s, episode steps:  15, steps per second: 125, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 55450.317122, mae: 1698.763696, mean_q: 3479.581657, mean_tau: 0.008671\n",
            " 46170/50000: episode: 1988, duration: 0.287s, episode steps:  36, steps per second: 125, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23541.690375, mae: 1760.203739, mean_q: 3625.531121, mean_tau: 0.008620\n",
            " 46187/50000: episode: 1989, duration: 0.128s, episode steps:  17, steps per second: 132, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 30861.927935, mae: 1741.712711, mean_q: 3574.206945, mean_tau: 0.008568\n",
            " 46200/50000: episode: 1990, duration: 0.119s, episode steps:  13, steps per second: 110, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 29570.572679, mae: 1729.264761, mean_q: 3547.177791, mean_tau: 0.008538\n",
            " 46221/50000: episode: 1991, duration: 0.162s, episode steps:  21, steps per second: 129, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 32160.616257, mae: 1714.215727, mean_q: 3527.178385, mean_tau: 0.008504\n",
            " 46245/50000: episode: 1992, duration: 0.194s, episode steps:  24, steps per second: 124, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 34872.821798, mae: 1737.744949, mean_q: 3559.898326, mean_tau: 0.008460\n",
            " 46268/50000: episode: 1993, duration: 0.179s, episode steps:  23, steps per second: 129, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 36410.643809, mae: 1752.845348, mean_q: 3613.430866, mean_tau: 0.008413\n",
            " 46292/50000: episode: 1994, duration: 0.188s, episode steps:  24, steps per second: 128, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 39751.328695, mae: 1754.561366, mean_q: 3570.617940, mean_tau: 0.008367\n",
            " 46335/50000: episode: 1995, duration: 0.337s, episode steps:  43, steps per second: 127, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.558 [0.000, 1.000],  loss: 30307.295240, mae: 1749.985204, mean_q: 3607.123223, mean_tau: 0.008300\n",
            " 46357/50000: episode: 1996, duration: 0.186s, episode steps:  22, steps per second: 118, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 34058.900568, mae: 1711.217846, mean_q: 3542.804110, mean_tau: 0.008236\n",
            " 46368/50000: episode: 1997, duration: 0.096s, episode steps:  11, steps per second: 115, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 18639.830788, mae: 1788.592574, mean_q: 3724.800515, mean_tau: 0.008203\n",
            " 46384/50000: episode: 1998, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 37537.674377, mae: 1757.260277, mean_q: 3633.424393, mean_tau: 0.008177\n",
            " 46398/50000: episode: 1999, duration: 0.118s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 41229.510672, mae: 1769.905055, mean_q: 3620.752459, mean_tau: 0.008147\n",
            " 46414/50000: episode: 2000, duration: 0.119s, episode steps:  16, steps per second: 135, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 21001.099365, mae: 1785.641991, mean_q: 3657.715561, mean_tau: 0.008117\n",
            " 46431/50000: episode: 2001, duration: 0.149s, episode steps:  17, steps per second: 114, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 35177.717170, mae: 1763.239459, mean_q: 3610.437371, mean_tau: 0.008084\n",
            " 46489/50000: episode: 2002, duration: 0.435s, episode steps:  58, steps per second: 133, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.431 [0.000, 1.000],  loss: 32967.638562, mae: 1762.279440, mean_q: 3628.352787, mean_tau: 0.008010\n",
            " 46526/50000: episode: 2003, duration: 0.281s, episode steps:  37, steps per second: 131, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 29792.522672, mae: 1748.458381, mean_q: 3616.469958, mean_tau: 0.007916\n",
            " 46543/50000: episode: 2004, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 30235.236730, mae: 1750.247616, mean_q: 3632.511101, mean_tau: 0.007863\n",
            " 46558/50000: episode: 2005, duration: 0.144s, episode steps:  15, steps per second: 104, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 34532.736068, mae: 1812.226847, mean_q: 3738.989893, mean_tau: 0.007831\n",
            " 46568/50000: episode: 2006, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 32953.742920, mae: 1783.463123, mean_q: 3696.340845, mean_tau: 0.007806\n",
            " 46581/50000: episode: 2007, duration: 0.114s, episode steps:  13, steps per second: 114, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 41857.376653, mae: 1777.498657, mean_q: 3624.060641, mean_tau: 0.007783\n",
            " 46603/50000: episode: 2008, duration: 0.185s, episode steps:  22, steps per second: 119, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36535.391113, mae: 1797.648260, mean_q: 3690.684703, mean_tau: 0.007749\n",
            " 46618/50000: episode: 2009, duration: 0.134s, episode steps:  15, steps per second: 112, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 31731.229948, mae: 1798.902791, mean_q: 3704.116748, mean_tau: 0.007712\n",
            " 46637/50000: episode: 2010, duration: 0.145s, episode steps:  19, steps per second: 131, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 32851.074373, mae: 1771.978888, mean_q: 3633.146407, mean_tau: 0.007679\n",
            " 46653/50000: episode: 2011, duration: 0.136s, episode steps:  16, steps per second: 118, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 21678.097229, mae: 1731.679176, mean_q: 3593.330185, mean_tau: 0.007644\n",
            " 46702/50000: episode: 2012, duration: 0.384s, episode steps:  49, steps per second: 128, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 35709.675104, mae: 1765.952365, mean_q: 3633.532197, mean_tau: 0.007580\n",
            " 46734/50000: episode: 2013, duration: 0.246s, episode steps:  32, steps per second: 130, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 36696.701523, mae: 1737.004620, mean_q: 3561.601173, mean_tau: 0.007499\n",
            " 46758/50000: episode: 2014, duration: 0.188s, episode steps:  24, steps per second: 127, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 33539.153402, mae: 1748.024409, mean_q: 3617.348551, mean_tau: 0.007444\n",
            " 46785/50000: episode: 2015, duration: 0.203s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 30287.798159, mae: 1787.807369, mean_q: 3692.595450, mean_tau: 0.007393\n",
            " 46795/50000: episode: 2016, duration: 0.093s, episode steps:  10, steps per second: 108, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 24572.610254, mae: 1747.316663, mean_q: 3616.653052, mean_tau: 0.007357\n",
            " 46823/50000: episode: 2017, duration: 0.247s, episode steps:  28, steps per second: 114, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 29791.496669, mae: 1788.492746, mean_q: 3688.646580, mean_tau: 0.007319\n",
            " 46852/50000: episode: 2018, duration: 0.237s, episode steps:  29, steps per second: 122, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 36901.465366, mae: 1810.645946, mean_q: 3691.757122, mean_tau: 0.007263\n",
            " 46898/50000: episode: 2019, duration: 0.363s, episode steps:  46, steps per second: 127, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 40953.963719, mae: 1788.019011, mean_q: 3663.492145, mean_tau: 0.007188\n",
            " 46911/50000: episode: 2020, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 26689.447040, mae: 1826.346576, mean_q: 3793.567383, mean_tau: 0.007130\n",
            " 46924/50000: episode: 2021, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 43141.442157, mae: 1750.240028, mean_q: 3627.747803, mean_tau: 0.007104\n",
            " 46982/50000: episode: 2022, duration: 0.448s, episode steps:  58, steps per second: 129, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 29535.694412, mae: 1797.745092, mean_q: 3713.903392, mean_tau: 0.007034\n",
            " 47022/50000: episode: 2023, duration: 0.299s, episode steps:  40, steps per second: 134, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 46317.274609, mae: 1798.764175, mean_q: 3676.547394, mean_tau: 0.006937\n",
            " 47036/50000: episode: 2024, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 41737.752999, mae: 1749.482474, mean_q: 3652.949027, mean_tau: 0.006884\n",
            " 47071/50000: episode: 2025, duration: 0.285s, episode steps:  35, steps per second: 123, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 31826.049093, mae: 1816.286084, mean_q: 3747.619510, mean_tau: 0.006835\n",
            " 47091/50000: episode: 2026, duration: 0.165s, episode steps:  20, steps per second: 122, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36472.631055, mae: 1811.337024, mean_q: 3720.124268, mean_tau: 0.006781\n",
            " 47115/50000: episode: 2027, duration: 0.202s, episode steps:  24, steps per second: 119, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 34922.288595, mae: 1846.910222, mean_q: 3797.940165, mean_tau: 0.006737\n",
            " 47142/50000: episode: 2028, duration: 0.229s, episode steps:  27, steps per second: 118, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 33030.410753, mae: 1818.756949, mean_q: 3739.863869, mean_tau: 0.006687\n",
            " 47181/50000: episode: 2029, duration: 0.298s, episode steps:  39, steps per second: 131, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 39414.130534, mae: 1789.703742, mean_q: 3702.800168, mean_tau: 0.006621\n",
            " 47272/50000: episode: 2030, duration: 0.692s, episode steps:  91, steps per second: 132, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 35338.204874, mae: 1843.580071, mean_q: 3791.391239, mean_tau: 0.006493\n",
            " 47293/50000: episode: 2031, duration: 0.165s, episode steps:  21, steps per second: 127, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 38279.580450, mae: 1831.986293, mean_q: 3757.565127, mean_tau: 0.006382\n",
            " 47309/50000: episode: 2032, duration: 0.169s, episode steps:  16, steps per second:  94, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 37163.784576, mae: 1834.911980, mean_q: 3772.797882, mean_tau: 0.006345\n",
            " 47333/50000: episode: 2033, duration: 0.286s, episode steps:  24, steps per second:  84, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 32710.620341, mae: 1837.787460, mean_q: 3780.858246, mean_tau: 0.006305\n",
            " 47352/50000: episode: 2034, duration: 0.223s, episode steps:  19, steps per second:  85, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 33255.793971, mae: 1833.685425, mean_q: 3785.517912, mean_tau: 0.006263\n",
            " 47380/50000: episode: 2035, duration: 0.323s, episode steps:  28, steps per second:  87, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 33124.122175, mae: 1832.991804, mean_q: 3776.465690, mean_tau: 0.006216\n",
            " 47398/50000: episode: 2036, duration: 0.222s, episode steps:  18, steps per second:  81, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 25026.133762, mae: 1870.751126, mean_q: 3879.343940, mean_tau: 0.006171\n",
            " 47415/50000: episode: 2037, duration: 0.205s, episode steps:  17, steps per second:  83, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 25982.056468, mae: 1834.547342, mean_q: 3810.472599, mean_tau: 0.006136\n",
            " 47430/50000: episode: 2038, duration: 0.176s, episode steps:  15, steps per second:  85, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 52726.069694, mae: 1812.945467, mean_q: 3703.923503, mean_tau: 0.006104\n",
            " 47448/50000: episode: 2039, duration: 0.220s, episode steps:  18, steps per second:  82, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 45461.216471, mae: 1815.655104, mean_q: 3721.645454, mean_tau: 0.006072\n",
            " 47474/50000: episode: 2040, duration: 0.314s, episode steps:  26, steps per second:  83, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 52531.223238, mae: 1840.665908, mean_q: 3789.952721, mean_tau: 0.006028\n",
            " 47491/50000: episode: 2041, duration: 0.217s, episode steps:  17, steps per second:  78, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 46542.868882, mae: 1845.375668, mean_q: 3798.086742, mean_tau: 0.005986\n",
            " 47515/50000: episode: 2042, duration: 0.267s, episode steps:  24, steps per second:  90, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 27598.895101, mae: 1860.290609, mean_q: 3834.935527, mean_tau: 0.005945\n",
            " 47530/50000: episode: 2043, duration: 0.175s, episode steps:  15, steps per second:  86, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 36316.023372, mae: 1871.172420, mean_q: 3867.766032, mean_tau: 0.005906\n",
            " 47555/50000: episode: 2044, duration: 0.288s, episode steps:  25, steps per second:  87, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 36100.309336, mae: 1894.741035, mean_q: 3900.517100, mean_tau: 0.005867\n",
            " 47568/50000: episode: 2045, duration: 0.104s, episode steps:  13, steps per second: 125, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 33462.189491, mae: 1800.649715, mean_q: 3759.080735, mean_tau: 0.005829\n",
            " 47604/50000: episode: 2046, duration: 0.295s, episode steps:  36, steps per second: 122, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 45371.734578, mae: 1858.261987, mean_q: 3801.765171, mean_tau: 0.005781\n",
            " 47626/50000: episode: 2047, duration: 0.165s, episode steps:  22, steps per second: 133, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 41742.461670, mae: 1878.215510, mean_q: 3869.542192, mean_tau: 0.005723\n",
            " 47659/50000: episode: 2048, duration: 0.253s, episode steps:  33, steps per second: 130, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 43001.682055, mae: 1904.959831, mean_q: 3911.518969, mean_tau: 0.005669\n",
            " 47683/50000: episode: 2049, duration: 0.177s, episode steps:  24, steps per second: 136, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 23838.695841, mae: 1900.411794, mean_q: 3937.957418, mean_tau: 0.005612\n",
            " 47694/50000: episode: 2050, duration: 0.100s, episode steps:  11, steps per second: 110, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 38945.264826, mae: 1902.060047, mean_q: 3898.904075, mean_tau: 0.005578\n",
            " 47711/50000: episode: 2051, duration: 0.134s, episode steps:  17, steps per second: 127, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 51210.014304, mae: 1857.015158, mean_q: 3810.862190, mean_tau: 0.005550\n",
            " 47733/50000: episode: 2052, duration: 0.185s, episode steps:  22, steps per second: 119, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 40253.543812, mae: 1890.310625, mean_q: 3863.994196, mean_tau: 0.005511\n",
            " 47758/50000: episode: 2053, duration: 0.213s, episode steps:  25, steps per second: 117, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 40012.985703, mae: 1888.971250, mean_q: 3877.911348, mean_tau: 0.005465\n",
            " 47772/50000: episode: 2054, duration: 0.109s, episode steps:  14, steps per second: 129, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 32274.593471, mae: 1907.582302, mean_q: 3939.692801, mean_tau: 0.005426\n",
            " 47783/50000: episode: 2055, duration: 0.089s, episode steps:  11, steps per second: 124, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 32136.820756, mae: 1830.881969, mean_q: 3808.577925, mean_tau: 0.005402\n",
            " 47807/50000: episode: 2056, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 27062.705037, mae: 1891.391291, mean_q: 3924.091838, mean_tau: 0.005367\n",
            " 47834/50000: episode: 2057, duration: 0.219s, episode steps:  27, steps per second: 123, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 34901.560041, mae: 1905.139752, mean_q: 3927.434136, mean_tau: 0.005316\n",
            " 47847/50000: episode: 2058, duration: 0.108s, episode steps:  13, steps per second: 121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 25884.472806, mae: 1887.454355, mean_q: 3861.317402, mean_tau: 0.005277\n",
            " 47865/50000: episode: 2059, duration: 0.144s, episode steps:  18, steps per second: 125, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 46961.424533, mae: 1914.997097, mean_q: 3933.514418, mean_tau: 0.005246\n",
            " 47889/50000: episode: 2060, duration: 0.185s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 71112.481171, mae: 1952.002803, mean_q: 3981.489634, mean_tau: 0.005205\n",
            " 47900/50000: episode: 2061, duration: 0.096s, episode steps:  11, steps per second: 114, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 39448.996582, mae: 1857.358077, mean_q: 3849.988037, mean_tau: 0.005170\n",
            " 47949/50000: episode: 2062, duration: 0.380s, episode steps:  49, steps per second: 129, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 40028.100397, mae: 1906.169304, mean_q: 3930.911113, mean_tau: 0.005110\n",
            " 47961/50000: episode: 2063, duration: 0.098s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 35717.400350, mae: 1901.663737, mean_q: 3889.796183, mean_tau: 0.005050\n",
            " 47989/50000: episode: 2064, duration: 0.233s, episode steps:  28, steps per second: 120, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39813.226597, mae: 1891.044124, mean_q: 3880.274998, mean_tau: 0.005010\n",
            " 48025/50000: episode: 2065, duration: 0.298s, episode steps:  36, steps per second: 121, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 45608.492676, mae: 1914.069811, mean_q: 3905.506585, mean_tau: 0.004947\n",
            " 48049/50000: episode: 2066, duration: 0.186s, episode steps:  24, steps per second: 129, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 36484.362712, mae: 1969.338272, mean_q: 4037.441457, mean_tau: 0.004888\n",
            " 48080/50000: episode: 2067, duration: 0.260s, episode steps:  31, steps per second: 119, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 43380.766476, mae: 1902.480020, mean_q: 3916.878197, mean_tau: 0.004833\n",
            " 48095/50000: episode: 2068, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 55835.048372, mae: 1875.847355, mean_q: 3854.607650, mean_tau: 0.004788\n",
            " 48130/50000: episode: 2069, duration: 0.279s, episode steps:  35, steps per second: 125, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 40791.275823, mae: 1929.331609, mean_q: 3961.176137, mean_tau: 0.004738\n",
            " 48173/50000: episode: 2070, duration: 0.322s, episode steps:  43, steps per second: 134, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 30587.958871, mae: 1903.717995, mean_q: 3933.039363, mean_tau: 0.004661\n",
            " 48185/50000: episode: 2071, duration: 0.094s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 37483.152384, mae: 1942.457153, mean_q: 3992.428141, mean_tau: 0.004607\n",
            " 48204/50000: episode: 2072, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 32794.232679, mae: 1895.791928, mean_q: 3913.228066, mean_tau: 0.004576\n",
            " 48214/50000: episode: 2073, duration: 0.080s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 40230.214063, mae: 1922.143713, mean_q: 3928.305518, mean_tau: 0.004547\n",
            " 48234/50000: episode: 2074, duration: 0.168s, episode steps:  20, steps per second: 119, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 34351.622241, mae: 1918.162476, mean_q: 3949.639050, mean_tau: 0.004517\n",
            " 48261/50000: episode: 2075, duration: 0.229s, episode steps:  27, steps per second: 118, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 35095.741464, mae: 1938.922273, mean_q: 3991.853027, mean_tau: 0.004471\n",
            " 48287/50000: episode: 2076, duration: 0.232s, episode steps:  26, steps per second: 112, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 57572.727070, mae: 1941.537551, mean_q: 4008.206055, mean_tau: 0.004418\n",
            " 48301/50000: episode: 2077, duration: 0.119s, episode steps:  14, steps per second: 118, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 32431.441825, mae: 1898.979510, mean_q: 3921.965890, mean_tau: 0.004379\n",
            " 48322/50000: episode: 2078, duration: 0.189s, episode steps:  21, steps per second: 111, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 40569.194661, mae: 1980.896763, mean_q: 4078.623175, mean_tau: 0.004344\n",
            " 48351/50000: episode: 2079, duration: 0.249s, episode steps:  29, steps per second: 116, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 41435.800731, mae: 1965.856870, mean_q: 4052.555866, mean_tau: 0.004295\n",
            " 48364/50000: episode: 2080, duration: 0.107s, episode steps:  13, steps per second: 122, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 72654.431115, mae: 1917.363751, mean_q: 3888.750995, mean_tau: 0.004253\n",
            " 48413/50000: episode: 2081, duration: 0.392s, episode steps:  49, steps per second: 125, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 32204.455367, mae: 1967.309102, mean_q: 4046.669678, mean_tau: 0.004192\n",
            " 48433/50000: episode: 2082, duration: 0.153s, episode steps:  20, steps per second: 131, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 29555.042358, mae: 1998.980865, mean_q: 4142.255908, mean_tau: 0.004123\n",
            " 48474/50000: episode: 2083, duration: 0.317s, episode steps:  41, steps per second: 129, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 34626.636659, mae: 1980.241268, mean_q: 4072.842470, mean_tau: 0.004063\n",
            " 48489/50000: episode: 2084, duration: 0.116s, episode steps:  15, steps per second: 129, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 35895.330632, mae: 1977.736833, mean_q: 4114.087288, mean_tau: 0.004008\n",
            " 48512/50000: episode: 2085, duration: 0.175s, episode steps:  23, steps per second: 132, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 29013.399350, mae: 1968.039742, mean_q: 4082.473675, mean_tau: 0.003970\n",
            " 48533/50000: episode: 2086, duration: 0.164s, episode steps:  21, steps per second: 128, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 26340.103655, mae: 1952.728353, mean_q: 4063.408564, mean_tau: 0.003926\n",
            " 48563/50000: episode: 2087, duration: 0.230s, episode steps:  30, steps per second: 130, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39599.008724, mae: 1933.487109, mean_q: 4008.279191, mean_tau: 0.003876\n",
            " 48581/50000: episode: 2088, duration: 0.141s, episode steps:  18, steps per second: 128, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 44416.275336, mae: 1917.687893, mean_q: 3959.142795, mean_tau: 0.003828\n",
            " 48624/50000: episode: 2089, duration: 0.335s, episode steps:  43, steps per second: 128, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 51346.243732, mae: 2009.755942, mean_q: 4127.544382, mean_tau: 0.003768\n",
            " 48638/50000: episode: 2090, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 60353.113560, mae: 2000.321612, mean_q: 4078.003871, mean_tau: 0.003712\n",
            " 48676/50000: episode: 2091, duration: 0.290s, episode steps:  38, steps per second: 131, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 31504.334203, mae: 1981.014012, mean_q: 4088.914878, mean_tau: 0.003660\n",
            " 48710/50000: episode: 2092, duration: 0.260s, episode steps:  34, steps per second: 131, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 43802.627585, mae: 1967.311380, mean_q: 4037.430707, mean_tau: 0.003589\n",
            " 48731/50000: episode: 2093, duration: 0.170s, episode steps:  21, steps per second: 124, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 39773.058594, mae: 1971.959136, mean_q: 4062.428292, mean_tau: 0.003534\n",
            " 48740/50000: episode: 2094, duration: 0.081s, episode steps:   9, steps per second: 112, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 50021.640516, mae: 1968.317017, mean_q: 4005.053494, mean_tau: 0.003505\n",
            " 48779/50000: episode: 2095, duration: 0.297s, episode steps:  39, steps per second: 131, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.641 [0.000, 1.000],  loss: 37100.558694, mae: 2005.233790, mean_q: 4113.801532, mean_tau: 0.003457\n",
            " 48802/50000: episode: 2096, duration: 0.221s, episode steps:  23, steps per second: 104, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 41101.805112, mae: 2004.679921, mean_q: 4152.813540, mean_tau: 0.003396\n",
            " 48824/50000: episode: 2097, duration: 0.253s, episode steps:  22, steps per second:  87, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 47495.512029, mae: 1986.328708, mean_q: 4099.241877, mean_tau: 0.003351\n",
            " 48837/50000: episode: 2098, duration: 0.154s, episode steps:  13, steps per second:  84, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 41076.475999, mae: 1936.452421, mean_q: 3963.751183, mean_tau: 0.003317\n",
            " 48854/50000: episode: 2099, duration: 0.212s, episode steps:  17, steps per second:  80, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 51912.298369, mae: 2063.712668, mean_q: 4215.646456, mean_tau: 0.003287\n",
            " 48867/50000: episode: 2100, duration: 0.146s, episode steps:  13, steps per second:  89, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 36884.755033, mae: 2019.552716, mean_q: 4177.073655, mean_tau: 0.003257\n",
            " 48879/50000: episode: 2101, duration: 0.163s, episode steps:  12, steps per second:  73, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 34748.610270, mae: 2017.962219, mean_q: 4169.871928, mean_tau: 0.003232\n",
            " 48892/50000: episode: 2102, duration: 0.156s, episode steps:  13, steps per second:  83, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 67763.596154, mae: 1928.055514, mean_q: 3975.785776, mean_tau: 0.003208\n",
            " 48911/50000: episode: 2103, duration: 0.242s, episode steps:  19, steps per second:  79, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 67993.346217, mae: 2011.565879, mean_q: 4061.584730, mean_tau: 0.003176\n",
            " 48934/50000: episode: 2104, duration: 0.282s, episode steps:  23, steps per second:  82, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 42399.580949, mae: 1999.574341, mean_q: 4080.682097, mean_tau: 0.003134\n",
            " 48962/50000: episode: 2105, duration: 0.328s, episode steps:  28, steps per second:  85, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 56208.684640, mae: 2042.799107, mean_q: 4155.734671, mean_tau: 0.003084\n",
            " 48975/50000: episode: 2106, duration: 0.168s, episode steps:  13, steps per second:  77, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 35213.547964, mae: 2024.899508, mean_q: 4137.148738, mean_tau: 0.003043\n",
            " 49028/50000: episode: 2107, duration: 0.637s, episode steps:  53, steps per second:  83, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 39330.001290, mae: 2002.687698, mean_q: 4115.428430, mean_tau: 0.002978\n",
            " 49048/50000: episode: 2108, duration: 0.163s, episode steps:  20, steps per second: 123, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 49813.643359, mae: 2015.763043, mean_q: 4156.376648, mean_tau: 0.002906\n",
            " 49066/50000: episode: 2109, duration: 0.157s, episode steps:  18, steps per second: 115, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 48126.434408, mae: 2019.925388, mean_q: 4170.968614, mean_tau: 0.002868\n",
            " 49098/50000: episode: 2110, duration: 0.260s, episode steps:  32, steps per second: 123, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 36724.470062, mae: 1945.640347, mean_q: 4031.079361, mean_tau: 0.002819\n",
            " 49129/50000: episode: 2111, duration: 0.250s, episode steps:  31, steps per second: 124, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 26505.059318, mae: 2003.091903, mean_q: 4140.785928, mean_tau: 0.002756\n",
            " 49204/50000: episode: 2112, duration: 0.580s, episode steps:  75, steps per second: 129, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 41182.202734, mae: 2009.675363, mean_q: 4147.023154, mean_tau: 0.002651\n",
            " 49245/50000: episode: 2113, duration: 0.323s, episode steps:  41, steps per second: 127, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 45010.497499, mae: 1984.951976, mean_q: 4100.363222, mean_tau: 0.002536\n",
            " 49258/50000: episode: 2114, duration: 0.117s, episode steps:  13, steps per second: 111, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 40004.816106, mae: 1963.038236, mean_q: 4060.384127, mean_tau: 0.002483\n",
            " 49271/50000: episode: 2115, duration: 0.107s, episode steps:  13, steps per second: 121, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 37971.980356, mae: 2023.603854, mean_q: 4172.811411, mean_tau: 0.002457\n",
            " 49298/50000: episode: 2116, duration: 0.206s, episode steps:  27, steps per second: 131, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 32671.484954, mae: 2021.666545, mean_q: 4162.143166, mean_tau: 0.002418\n",
            " 49322/50000: episode: 2117, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 37991.675008, mae: 2054.400004, mean_q: 4256.633901, mean_tau: 0.002367\n",
            " 49336/50000: episode: 2118, duration: 0.119s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 50208.967076, mae: 1973.948940, mean_q: 4079.605835, mean_tau: 0.002330\n",
            " 49380/50000: episode: 2119, duration: 0.347s, episode steps:  44, steps per second: 127, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 45679.561313, mae: 2032.960277, mean_q: 4170.132385, mean_tau: 0.002272\n",
            " 49397/50000: episode: 2120, duration: 0.140s, episode steps:  17, steps per second: 122, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 30683.995634, mae: 2020.703628, mean_q: 4154.561452, mean_tau: 0.002212\n",
            " 49423/50000: episode: 2121, duration: 0.210s, episode steps:  26, steps per second: 124, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.654 [0.000, 1.000],  loss: 36013.474234, mae: 2064.703350, mean_q: 4279.101262, mean_tau: 0.002169\n",
            " 49434/50000: episode: 2122, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 51565.745384, mae: 2019.577004, mean_q: 4173.279031, mean_tau: 0.002133\n",
            " 49446/50000: episode: 2123, duration: 0.093s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 50196.124023, mae: 2008.780518, mean_q: 4148.560425, mean_tau: 0.002110\n",
            " 49475/50000: episode: 2124, duration: 0.252s, episode steps:  29, steps per second: 115, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 47295.637123, mae: 2064.605077, mean_q: 4241.130893, mean_tau: 0.002069\n",
            " 49493/50000: episode: 2125, duration: 0.145s, episode steps:  18, steps per second: 124, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 40400.657444, mae: 2089.866869, mean_q: 4322.939426, mean_tau: 0.002023\n",
            " 49505/50000: episode: 2126, duration: 0.111s, episode steps:  12, steps per second: 108, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 44554.367920, mae: 2025.799978, mean_q: 4147.970825, mean_tau: 0.001993\n",
            " 49519/50000: episode: 2127, duration: 0.113s, episode steps:  14, steps per second: 124, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 43877.426409, mae: 2059.373361, mean_q: 4244.786080, mean_tau: 0.001967\n",
            " 49546/50000: episode: 2128, duration: 0.200s, episode steps:  27, steps per second: 135, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 56945.266131, mae: 2065.578658, mean_q: 4255.962375, mean_tau: 0.001927\n",
            " 49560/50000: episode: 2129, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 45187.972517, mae: 2094.389483, mean_q: 4254.477574, mean_tau: 0.001886\n",
            " 49578/50000: episode: 2130, duration: 0.148s, episode steps:  18, steps per second: 122, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 36520.100993, mae: 2079.269891, mean_q: 4267.952949, mean_tau: 0.001854\n",
            " 49596/50000: episode: 2131, duration: 0.136s, episode steps:  18, steps per second: 132, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 43366.273546, mae: 2053.090875, mean_q: 4199.416951, mean_tau: 0.001819\n",
            " 49611/50000: episode: 2132, duration: 0.122s, episode steps:  15, steps per second: 123, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 32461.307324, mae: 2104.326408, mean_q: 4343.584049, mean_tau: 0.001786\n",
            " 49623/50000: episode: 2133, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 35480.023926, mae: 2099.708598, mean_q: 4292.026693, mean_tau: 0.001759\n",
            " 49662/50000: episode: 2134, duration: 0.309s, episode steps:  39, steps per second: 126, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  loss: 34627.929637, mae: 2067.032900, mean_q: 4266.989402, mean_tau: 0.001709\n",
            " 49675/50000: episode: 2135, duration: 0.097s, episode steps:  13, steps per second: 134, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 42957.207707, mae: 2048.448167, mean_q: 4223.697209, mean_tau: 0.001657\n",
            " 49705/50000: episode: 2136, duration: 0.250s, episode steps:  30, steps per second: 120, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 36964.427653, mae: 2024.684517, mean_q: 4183.076090, mean_tau: 0.001615\n",
            " 49722/50000: episode: 2137, duration: 0.137s, episode steps:  17, steps per second: 124, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.294 [0.000, 1.000],  loss: 35108.172507, mae: 2018.387264, mean_q: 4158.107120, mean_tau: 0.001568\n",
            " 49736/50000: episode: 2138, duration: 0.117s, episode steps:  14, steps per second: 120, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 33470.910261, mae: 2011.486738, mean_q: 4183.370187, mean_tau: 0.001538\n",
            " 49775/50000: episode: 2139, duration: 0.291s, episode steps:  39, steps per second: 134, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 46504.057367, mae: 2060.851963, mean_q: 4239.251333, mean_tau: 0.001485\n",
            " 49798/50000: episode: 2140, duration: 0.173s, episode steps:  23, steps per second: 133, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 35544.808233, mae: 2064.593028, mean_q: 4302.144170, mean_tau: 0.001424\n",
            " 49825/50000: episode: 2141, duration: 0.209s, episode steps:  27, steps per second: 129, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 43710.353552, mae: 2038.079468, mean_q: 4206.758807, mean_tau: 0.001374\n",
            " 49859/50000: episode: 2142, duration: 0.250s, episode steps:  34, steps per second: 136, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.618 [0.000, 1.000],  loss: 45504.186581, mae: 2092.575285, mean_q: 4325.884981, mean_tau: 0.001314\n",
            " 49885/50000: episode: 2143, duration: 0.191s, episode steps:  26, steps per second: 136, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 33131.188063, mae: 2134.723994, mean_q: 4416.738385, mean_tau: 0.001254\n",
            " 49905/50000: episode: 2144, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 44781.095898, mae: 2065.813611, mean_q: 4310.265759, mean_tau: 0.001209\n",
            " 49943/50000: episode: 2145, duration: 0.291s, episode steps:  38, steps per second: 131, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 45771.266537, mae: 2108.177497, mean_q: 4380.717735, mean_tau: 0.001151\n",
            " 49989/50000: episode: 2146, duration: 0.349s, episode steps:  46, steps per second: 132, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 41492.294869, mae: 2082.682657, mean_q: 4305.848601, mean_tau: 0.001068\n",
            "done, took 436.197 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 183.000, steps: 183\n",
            "Episode 3: reward: 151.000, steps: 151\n",
            "Episode 4: reward: 195.000, steps: 195\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 175.000, steps: 175\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 188.000, steps: 188\n",
            "Episode 16: reward: 173.000, steps: 173\n",
            "Episode 17: reward: 163.000, steps: 163\n",
            "Episode 18: reward: 181.000, steps: 181\n",
            "Episode 19: reward: 153.000, steps: 153\n",
            "Episode 20: reward: 191.000, steps: 191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f403c1e9910>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_outer =  LinearAnnealedPolicy(inner_policy=policy_inner, \n",
        "                               attr='tau',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.001, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=50000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy_outer) \n",
        "\n",
        "dqn.compile(Adam(lr=0.01), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)\n",
        "\n",
        "plt.imshow(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sB-_KPwpWx5A",
        "outputId": "00541e88-2e74-4c0d-afe9-dc1e49f23b60"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   178/50000: episode: 1, duration: 2.933s, episode steps: 178, steps per second:  61, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.348449, mae: 35.520777, mean_q: 71.735194, mean_tau: 0.998122\n",
            "   374/50000: episode: 2, duration: 3.340s, episode steps: 196, steps per second:  59, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.809555, mae: 35.879948, mean_q: 72.462145, mean_tau: 0.994496\n",
            "   537/50000: episode: 3, duration: 1.312s, episode steps: 163, steps per second: 124, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.938013, mae: 35.728814, mean_q: 72.218768, mean_tau: 0.990909\n",
            "   692/50000: episode: 4, duration: 1.181s, episode steps: 155, steps per second: 131, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.205044, mae: 36.230269, mean_q: 72.889314, mean_tau: 0.987732\n",
            "   887/50000: episode: 5, duration: 1.822s, episode steps: 195, steps per second: 107, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 7.492442, mae: 36.208019, mean_q: 72.836251, mean_tau: 0.984236\n",
            "  1043/50000: episode: 6, duration: 1.736s, episode steps: 156, steps per second:  90, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 4.715065, mae: 36.147383, mean_q: 72.863305, mean_tau: 0.980729\n",
            "  1237/50000: episode: 7, duration: 1.478s, episode steps: 194, steps per second: 131, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.838565, mae: 36.613618, mean_q: 73.772927, mean_tau: 0.977233\n",
            "  1425/50000: episode: 8, duration: 1.456s, episode steps: 188, steps per second: 129, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 8.019822, mae: 36.644445, mean_q: 73.641918, mean_tau: 0.973417\n",
            "  1578/50000: episode: 9, duration: 1.193s, episode steps: 153, steps per second: 128, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4.061236, mae: 36.733748, mean_q: 73.959586, mean_tau: 0.970010\n",
            "  1776/50000: episode: 10, duration: 1.560s, episode steps: 198, steps per second: 127, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 8.469668, mae: 37.355377, mean_q: 74.993116, mean_tau: 0.966504\n",
            "  1934/50000: episode: 11, duration: 1.229s, episode steps: 158, steps per second: 129, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.641254, mae: 37.172077, mean_q: 74.746622, mean_tau: 0.962947\n",
            "  2121/50000: episode: 12, duration: 1.436s, episode steps: 187, steps per second: 130, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.614361, mae: 37.353164, mean_q: 75.251183, mean_tau: 0.959501\n",
            "  2282/50000: episode: 13, duration: 1.276s, episode steps: 161, steps per second: 126, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 8.252888, mae: 37.751971, mean_q: 75.966172, mean_tau: 0.956024\n",
            "  2442/50000: episode: 14, duration: 1.668s, episode steps: 160, steps per second:  96, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 8.439287, mae: 37.712556, mean_q: 76.013926, mean_tau: 0.952817\n",
            "  2605/50000: episode: 15, duration: 1.697s, episode steps: 163, steps per second:  96, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 7.739078, mae: 37.997113, mean_q: 76.687308, mean_tau: 0.949590\n",
            "  2749/50000: episode: 16, duration: 1.107s, episode steps: 144, steps per second: 130, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.049416, mae: 38.410146, mean_q: 77.383425, mean_tau: 0.946524\n",
            "  2915/50000: episode: 17, duration: 1.336s, episode steps: 166, steps per second: 124, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 10.751067, mae: 38.497101, mean_q: 77.747589, mean_tau: 0.943427\n",
            "  3079/50000: episode: 18, duration: 1.301s, episode steps: 164, steps per second: 126, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 14.991812, mae: 38.744297, mean_q: 78.439163, mean_tau: 0.940130\n",
            "  3253/50000: episode: 19, duration: 1.350s, episode steps: 174, steps per second: 129, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 12.858359, mae: 39.277972, mean_q: 79.756873, mean_tau: 0.936753\n",
            "  3453/50000: episode: 20, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 8.401619, mae: 39.303812, mean_q: 79.707177, mean_tau: 0.933017\n",
            "  3653/50000: episode: 21, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 13.738701, mae: 39.768857, mean_q: 80.594692, mean_tau: 0.929021\n",
            "  3853/50000: episode: 22, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 15.676165, mae: 40.529949, mean_q: 81.933358, mean_tau: 0.925025\n",
            "  4053/50000: episode: 23, duration: 2.265s, episode steps: 200, steps per second:  88, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 18.775183, mae: 41.294301, mean_q: 83.609594, mean_tau: 0.921029\n",
            "  4253/50000: episode: 24, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 12.601160, mae: 42.031630, mean_q: 85.157492, mean_tau: 0.917033\n",
            "  4453/50000: episode: 25, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 15.113272, mae: 43.334159, mean_q: 87.874273, mean_tau: 0.913037\n",
            "  4653/50000: episode: 26, duration: 1.629s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 24.044734, mae: 43.884319, mean_q: 88.660753, mean_tau: 0.909041\n",
            "  4853/50000: episode: 27, duration: 1.747s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 73.675942, mae: 44.944296, mean_q: 89.989941, mean_tau: 0.905045\n",
            "  5053/50000: episode: 28, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 16.467505, mae: 44.798287, mean_q: 90.387448, mean_tau: 0.901049\n",
            "  5253/50000: episode: 29, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.434311, mae: 45.296749, mean_q: 91.418223, mean_tau: 0.897053\n",
            "  5453/50000: episode: 30, duration: 2.064s, episode steps: 200, steps per second:  97, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 34.003597, mae: 46.334014, mean_q: 93.738383, mean_tau: 0.893057\n",
            "  5653/50000: episode: 31, duration: 2.099s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 42.952886, mae: 46.920467, mean_q: 94.798010, mean_tau: 0.889061\n",
            "  5853/50000: episode: 32, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 56.407520, mae: 48.332936, mean_q: 96.915974, mean_tau: 0.885065\n",
            "  6053/50000: episode: 33, duration: 1.587s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27.281784, mae: 48.669308, mean_q: 98.049776, mean_tau: 0.881069\n",
            "  6253/50000: episode: 34, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 41.737017, mae: 49.890882, mean_q: 100.503179, mean_tau: 0.877073\n",
            "  6453/50000: episode: 35, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 33.872182, mae: 50.989023, mean_q: 102.765262, mean_tau: 0.873077\n",
            "  6653/50000: episode: 36, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 41.535885, mae: 51.746386, mean_q: 104.250528, mean_tau: 0.869081\n",
            "  6853/50000: episode: 37, duration: 2.299s, episode steps: 200, steps per second:  87, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 31.311715, mae: 52.562072, mean_q: 106.347075, mean_tau: 0.865085\n",
            "  7053/50000: episode: 38, duration: 2.045s, episode steps: 200, steps per second:  98, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 48.414915, mae: 54.102785, mean_q: 109.019459, mean_tau: 0.861089\n",
            "  7253/50000: episode: 39, duration: 1.811s, episode steps: 200, steps per second: 110, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 67.692380, mae: 55.634217, mean_q: 111.812188, mean_tau: 0.857093\n",
            "  7453/50000: episode: 40, duration: 1.758s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 75.345079, mae: 56.006788, mean_q: 112.413498, mean_tau: 0.853097\n",
            "  7653/50000: episode: 41, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 48.601101, mae: 57.331604, mean_q: 115.549190, mean_tau: 0.849101\n",
            "  7853/50000: episode: 42, duration: 1.781s, episode steps: 200, steps per second: 112, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 81.606429, mae: 58.476032, mean_q: 117.540015, mean_tau: 0.845105\n",
            "  8053/50000: episode: 43, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 68.765467, mae: 59.793090, mean_q: 120.077410, mean_tau: 0.841109\n",
            "  8253/50000: episode: 44, duration: 2.608s, episode steps: 200, steps per second:  77, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 80.117834, mae: 60.499855, mean_q: 121.965154, mean_tau: 0.837113\n",
            "  8453/50000: episode: 45, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 69.794461, mae: 61.703312, mean_q: 124.239995, mean_tau: 0.833117\n",
            "  8653/50000: episode: 46, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 82.484056, mae: 63.371881, mean_q: 127.437407, mean_tau: 0.829121\n",
            "  8853/50000: episode: 47, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 111.458453, mae: 64.663893, mean_q: 129.550020, mean_tau: 0.825125\n",
            "  9053/50000: episode: 48, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 76.728152, mae: 65.682174, mean_q: 132.428386, mean_tau: 0.821129\n",
            "  9253/50000: episode: 49, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 108.384413, mae: 67.377061, mean_q: 135.065456, mean_tau: 0.817133\n",
            "  9453/50000: episode: 50, duration: 1.547s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 103.269027, mae: 68.257640, mean_q: 136.715850, mean_tau: 0.813137\n",
            "  9653/50000: episode: 51, duration: 2.190s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 54.116416, mae: 68.495500, mean_q: 137.355842, mean_tau: 0.809141\n",
            "  9853/50000: episode: 52, duration: 2.006s, episode steps: 200, steps per second: 100, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 70.886470, mae: 68.193599, mean_q: 136.550108, mean_tau: 0.805145\n",
            " 10053/50000: episode: 53, duration: 1.591s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 52.311123, mae: 67.461694, mean_q: 134.987693, mean_tau: 0.801149\n",
            " 10253/50000: episode: 54, duration: 1.614s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 43.802857, mae: 67.903810, mean_q: 136.119044, mean_tau: 0.797153\n",
            " 10453/50000: episode: 55, duration: 1.973s, episode steps: 200, steps per second: 101, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 41.799086, mae: 68.775437, mean_q: 137.790401, mean_tau: 0.793157\n",
            " 10653/50000: episode: 56, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 33.191490, mae: 69.180697, mean_q: 139.483879, mean_tau: 0.789161\n",
            " 10853/50000: episode: 57, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.241717, mae: 69.999466, mean_q: 140.766460, mean_tau: 0.785165\n",
            " 11053/50000: episode: 58, duration: 2.116s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 50.649278, mae: 71.138648, mean_q: 142.760049, mean_tau: 0.781169\n",
            " 11253/50000: episode: 59, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 39.993190, mae: 71.670186, mean_q: 144.097506, mean_tau: 0.777173\n",
            " 11453/50000: episode: 60, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 51.846416, mae: 72.600766, mean_q: 145.650304, mean_tau: 0.773177\n",
            " 11653/50000: episode: 61, duration: 1.540s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 52.965687, mae: 73.338800, mean_q: 146.911245, mean_tau: 0.769181\n",
            " 11853/50000: episode: 62, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 59.366677, mae: 74.147694, mean_q: 148.388341, mean_tau: 0.765185\n",
            " 12053/50000: episode: 63, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 62.434702, mae: 74.887978, mean_q: 149.554289, mean_tau: 0.761189\n",
            " 12253/50000: episode: 64, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 58.068813, mae: 74.786009, mean_q: 149.676469, mean_tau: 0.757193\n",
            " 12453/50000: episode: 65, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 54.132999, mae: 75.557785, mean_q: 151.199509, mean_tau: 0.753197\n",
            " 12653/50000: episode: 66, duration: 2.215s, episode steps: 200, steps per second:  90, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 83.610442, mae: 77.039011, mean_q: 153.907854, mean_tau: 0.749201\n",
            " 12812/50000: episode: 67, duration: 1.234s, episode steps: 159, steps per second: 129, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 40.747234, mae: 77.540578, mean_q: 155.984718, mean_tau: 0.745615\n",
            " 12963/50000: episode: 68, duration: 1.100s, episode steps: 151, steps per second: 137, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 64.672868, mae: 78.988640, mean_q: 158.304716, mean_tau: 0.742518\n",
            " 13114/50000: episode: 69, duration: 1.104s, episode steps: 151, steps per second: 137, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 58.049846, mae: 80.599997, mean_q: 162.027292, mean_tau: 0.739501\n",
            " 13249/50000: episode: 70, duration: 0.986s, episode steps: 135, steps per second: 137, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 104.645722, mae: 81.693934, mean_q: 163.835010, mean_tau: 0.736644\n",
            " 13388/50000: episode: 71, duration: 1.079s, episode steps: 139, steps per second: 129, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 116.813040, mae: 82.739409, mean_q: 165.163174, mean_tau: 0.733906\n",
            " 13588/50000: episode: 72, duration: 1.772s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 142.027029, mae: 83.648750, mean_q: 166.177992, mean_tau: 0.730520\n",
            " 13675/50000: episode: 73, duration: 0.827s, episode steps:  87, steps per second: 105, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 115.112073, mae: 83.058360, mean_q: 164.982668, mean_tau: 0.727653\n",
            " 13776/50000: episode: 74, duration: 0.988s, episode steps: 101, steps per second: 102, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 111.347048, mae: 83.008602, mean_q: 165.026277, mean_tau: 0.725774\n",
            " 13853/50000: episode: 75, duration: 0.739s, episode steps:  77, steps per second: 104, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 116.497554, mae: 82.356077, mean_q: 163.716696, mean_tau: 0.723996\n",
            " 13950/50000: episode: 76, duration: 1.176s, episode steps:  97, steps per second:  82, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 113.970420, mae: 82.996861, mean_q: 165.099139, mean_tau: 0.722258\n",
            " 14025/50000: episode: 77, duration: 0.984s, episode steps:  75, steps per second:  76, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 124.405665, mae: 81.695140, mean_q: 162.098969, mean_tau: 0.720540\n",
            " 14122/50000: episode: 78, duration: 1.183s, episode steps:  97, steps per second:  82, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.577 [0.000, 1.000],  loss: 95.760281, mae: 82.069811, mean_q: 163.307060, mean_tau: 0.718821\n",
            " 14322/50000: episode: 79, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 72.838609, mae: 80.795677, mean_q: 161.687914, mean_tau: 0.715854\n",
            " 14522/50000: episode: 80, duration: 1.610s, episode steps: 200, steps per second: 124, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 79.728179, mae: 79.890581, mean_q: 159.835316, mean_tau: 0.711858\n",
            " 14722/50000: episode: 81, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 69.963088, mae: 78.881847, mean_q: 158.062577, mean_tau: 0.707862\n",
            " 14922/50000: episode: 82, duration: 1.604s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 82.065204, mae: 79.199807, mean_q: 158.351970, mean_tau: 0.703866\n",
            " 15122/50000: episode: 83, duration: 1.565s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 76.021843, mae: 79.122537, mean_q: 158.474939, mean_tau: 0.699870\n",
            " 15322/50000: episode: 84, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 71.321151, mae: 78.898240, mean_q: 158.136245, mean_tau: 0.695874\n",
            " 15522/50000: episode: 85, duration: 2.234s, episode steps: 200, steps per second:  90, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 71.624287, mae: 79.063680, mean_q: 159.207963, mean_tau: 0.691878\n",
            " 15722/50000: episode: 86, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 76.233261, mae: 79.611655, mean_q: 159.975489, mean_tau: 0.687882\n",
            " 15922/50000: episode: 87, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 48.468696, mae: 79.489688, mean_q: 160.301038, mean_tau: 0.683886\n",
            " 16122/50000: episode: 88, duration: 1.588s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 103.300105, mae: 80.171397, mean_q: 160.711273, mean_tau: 0.679890\n",
            " 16322/50000: episode: 89, duration: 1.566s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 62.271998, mae: 79.658745, mean_q: 160.793621, mean_tau: 0.675894\n",
            " 16522/50000: episode: 90, duration: 1.593s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 89.060505, mae: 80.270848, mean_q: 161.301705, mean_tau: 0.671898\n",
            " 16722/50000: episode: 91, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 80.250739, mae: 79.960113, mean_q: 160.884365, mean_tau: 0.667902\n",
            " 16922/50000: episode: 92, duration: 1.868s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 45.193510, mae: 79.823434, mean_q: 161.328843, mean_tau: 0.663906\n",
            " 17122/50000: episode: 93, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 57.234412, mae: 80.074463, mean_q: 161.888642, mean_tau: 0.659910\n",
            " 17322/50000: episode: 94, duration: 1.590s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 77.370854, mae: 80.020608, mean_q: 161.212571, mean_tau: 0.655914\n",
            " 17522/50000: episode: 95, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 66.546506, mae: 80.087615, mean_q: 161.253305, mean_tau: 0.651918\n",
            " 17722/50000: episode: 96, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 53.533874, mae: 79.303130, mean_q: 159.990861, mean_tau: 0.647922\n",
            " 17922/50000: episode: 97, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 43.810491, mae: 79.347927, mean_q: 160.419743, mean_tau: 0.643926\n",
            " 18122/50000: episode: 98, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 61.121167, mae: 78.909276, mean_q: 159.098250, mean_tau: 0.639930\n",
            " 18322/50000: episode: 99, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 41.276345, mae: 78.633932, mean_q: 159.011830, mean_tau: 0.635934\n",
            " 18521/50000: episode: 100, duration: 1.907s, episode steps: 199, steps per second: 104, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 54.933908, mae: 78.602922, mean_q: 158.152957, mean_tau: 0.631948\n",
            " 18699/50000: episode: 101, duration: 1.744s, episode steps: 178, steps per second: 102, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 38.726607, mae: 78.591706, mean_q: 158.078125, mean_tau: 0.628182\n",
            " 18899/50000: episode: 102, duration: 1.427s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 51.331429, mae: 78.545154, mean_q: 157.985801, mean_tau: 0.624406\n",
            " 19099/50000: episode: 103, duration: 1.601s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 31.877727, mae: 77.977045, mean_q: 156.737296, mean_tau: 0.620410\n",
            " 19299/50000: episode: 104, duration: 1.687s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 30.717760, mae: 77.931720, mean_q: 156.749983, mean_tau: 0.616414\n",
            " 19494/50000: episode: 105, duration: 1.666s, episode steps: 195, steps per second: 117, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 35.865772, mae: 78.012931, mean_q: 156.366546, mean_tau: 0.612468\n",
            " 19677/50000: episode: 106, duration: 1.694s, episode steps: 183, steps per second: 108, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 48.672375, mae: 77.577188, mean_q: 154.812509, mean_tau: 0.608692\n",
            " 19877/50000: episode: 107, duration: 2.147s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 31.887559, mae: 76.892515, mean_q: 153.051853, mean_tau: 0.604866\n",
            " 20077/50000: episode: 108, duration: 3.061s, episode steps: 200, steps per second:  65, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 52.098595, mae: 77.321716, mean_q: 153.092487, mean_tau: 0.600870\n",
            " 20253/50000: episode: 109, duration: 1.359s, episode steps: 176, steps per second: 130, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 46.842054, mae: 76.781536, mean_q: 152.608937, mean_tau: 0.597113\n",
            " 20421/50000: episode: 110, duration: 1.240s, episode steps: 168, steps per second: 135, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 30.376920, mae: 76.599487, mean_q: 151.751393, mean_tau: 0.593677\n",
            " 20596/50000: episode: 111, duration: 1.297s, episode steps: 175, steps per second: 135, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 26.572787, mae: 76.038864, mean_q: 150.191289, mean_tau: 0.590250\n",
            " 20758/50000: episode: 112, duration: 1.192s, episode steps: 162, steps per second: 136, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 40.160326, mae: 76.433159, mean_q: 150.926863, mean_tau: 0.586884\n",
            " 20907/50000: episode: 113, duration: 1.076s, episode steps: 149, steps per second: 138, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 46.911544, mae: 76.159770, mean_q: 150.720848, mean_tau: 0.583777\n",
            " 21069/50000: episode: 114, duration: 1.218s, episode steps: 162, steps per second: 133, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 36.355431, mae: 75.706254, mean_q: 149.633956, mean_tau: 0.580670\n",
            " 21237/50000: episode: 115, duration: 1.252s, episode steps: 168, steps per second: 134, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 34.630856, mae: 75.341226, mean_q: 149.100715, mean_tau: 0.577373\n",
            " 21414/50000: episode: 116, duration: 1.308s, episode steps: 177, steps per second: 135, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 34.574487, mae: 74.735196, mean_q: 147.013029, mean_tau: 0.573927\n",
            " 21549/50000: episode: 117, duration: 1.419s, episode steps: 135, steps per second:  95, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 37.430470, mae: 75.376544, mean_q: 147.547102, mean_tau: 0.570810\n",
            " 21749/50000: episode: 118, duration: 1.981s, episode steps: 200, steps per second: 101, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 26.542396, mae: 74.151816, mean_q: 146.172212, mean_tau: 0.567463\n",
            " 21876/50000: episode: 119, duration: 0.975s, episode steps: 127, steps per second: 130, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 27.631001, mae: 74.614409, mean_q: 146.765914, mean_tau: 0.564196\n",
            " 22015/50000: episode: 120, duration: 1.048s, episode steps: 139, steps per second: 133, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 26.790537, mae: 73.303648, mean_q: 143.844818, mean_tau: 0.561539\n",
            " 22215/50000: episode: 121, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 35.037857, mae: 72.138561, mean_q: 140.899915, mean_tau: 0.558152\n",
            " 22354/50000: episode: 122, duration: 1.025s, episode steps: 139, steps per second: 136, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 22.272501, mae: 71.694930, mean_q: 140.648729, mean_tau: 0.554766\n",
            " 22488/50000: episode: 123, duration: 0.985s, episode steps: 134, steps per second: 136, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 26.903656, mae: 71.958993, mean_q: 140.689147, mean_tau: 0.552038\n",
            " 22688/50000: episode: 124, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 22.711847, mae: 72.007431, mean_q: 141.761697, mean_tau: 0.548702\n",
            " 22861/50000: episode: 125, duration: 1.284s, episode steps: 173, steps per second: 135, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 24.558285, mae: 70.451773, mean_q: 137.704014, mean_tau: 0.544975\n",
            " 23029/50000: episode: 126, duration: 1.230s, episode steps: 168, steps per second: 137, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 26.056285, mae: 70.332803, mean_q: 136.515701, mean_tau: 0.541569\n",
            " 23229/50000: episode: 127, duration: 2.106s, episode steps: 200, steps per second:  95, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.100719, mae: 69.828019, mean_q: 136.261067, mean_tau: 0.537893\n",
            " 23413/50000: episode: 128, duration: 1.597s, episode steps: 184, steps per second: 115, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 22.869221, mae: 69.536253, mean_q: 135.452790, mean_tau: 0.534056\n",
            " 23552/50000: episode: 129, duration: 0.986s, episode steps: 139, steps per second: 141, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 28.233982, mae: 68.751437, mean_q: 132.595860, mean_tau: 0.530830\n",
            " 23648/50000: episode: 130, duration: 0.685s, episode steps:  96, steps per second: 140, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 13.421680, mae: 67.460960, mean_q: 130.899178, mean_tau: 0.528482\n",
            " 23848/50000: episode: 131, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 27.571260, mae: 68.287969, mean_q: 132.507148, mean_tau: 0.525525\n",
            " 23942/50000: episode: 132, duration: 0.665s, episode steps:  94, steps per second: 141, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 29.810839, mae: 68.384542, mean_q: 131.745236, mean_tau: 0.522588\n",
            " 24114/50000: episode: 133, duration: 1.244s, episode steps: 172, steps per second: 138, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 28.479939, mae: 67.004880, mean_q: 129.430126, mean_tau: 0.519931\n",
            " 24243/50000: episode: 134, duration: 0.942s, episode steps: 129, steps per second: 137, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 17.668687, mae: 66.949419, mean_q: 130.260539, mean_tau: 0.516924\n",
            " 24390/50000: episode: 135, duration: 1.041s, episode steps: 147, steps per second: 141, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 24.931578, mae: 66.878634, mean_q: 129.318135, mean_tau: 0.514166\n",
            " 24515/50000: episode: 136, duration: 0.877s, episode steps: 125, steps per second: 142, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 27.570755, mae: 66.979570, mean_q: 129.704381, mean_tau: 0.511449\n",
            " 24574/50000: episode: 137, duration: 0.446s, episode steps:  59, steps per second: 132, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 25.209816, mae: 66.554931, mean_q: 129.006249, mean_tau: 0.509611\n",
            " 24630/50000: episode: 138, duration: 0.439s, episode steps:  56, steps per second: 127, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 18.962204, mae: 66.563997, mean_q: 129.211052, mean_tau: 0.508462\n",
            " 24693/50000: episode: 139, duration: 0.440s, episode steps:  63, steps per second: 143, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 28.258456, mae: 65.788957, mean_q: 127.208716, mean_tau: 0.507273\n",
            " 24757/50000: episode: 140, duration: 0.671s, episode steps:  64, steps per second:  95, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 22.134270, mae: 65.634727, mean_q: 126.594622, mean_tau: 0.506004\n",
            " 24881/50000: episode: 141, duration: 1.332s, episode steps: 124, steps per second:  93, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 27.146436, mae: 66.027594, mean_q: 127.770042, mean_tau: 0.504126\n",
            " 24977/50000: episode: 142, duration: 1.040s, episode steps:  96, steps per second:  92, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 24.398392, mae: 65.151620, mean_q: 126.920565, mean_tau: 0.501929\n",
            " 25103/50000: episode: 143, duration: 0.962s, episode steps: 126, steps per second: 131, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 25.910128, mae: 65.459240, mean_q: 127.260262, mean_tau: 0.499711\n",
            " 25257/50000: episode: 144, duration: 1.142s, episode steps: 154, steps per second: 135, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 19.541653, mae: 64.895125, mean_q: 126.865534, mean_tau: 0.496914\n",
            " 25377/50000: episode: 145, duration: 0.921s, episode steps: 120, steps per second: 130, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 26.009372, mae: 64.691112, mean_q: 125.733453, mean_tau: 0.494176\n",
            " 25575/50000: episode: 146, duration: 1.499s, episode steps: 198, steps per second: 132, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 18.705901, mae: 64.532319, mean_q: 125.414181, mean_tau: 0.491000\n",
            " 25642/50000: episode: 147, duration: 0.532s, episode steps:  67, steps per second: 126, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 15.893305, mae: 63.684086, mean_q: 123.324034, mean_tau: 0.488352\n",
            " 25705/50000: episode: 148, duration: 0.463s, episode steps:  63, steps per second: 136, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 36.681188, mae: 63.782033, mean_q: 123.200694, mean_tau: 0.487053\n",
            " 25905/50000: episode: 149, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.327747, mae: 62.896931, mean_q: 120.964512, mean_tau: 0.484426\n",
            " 26036/50000: episode: 150, duration: 0.961s, episode steps: 131, steps per second: 136, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 18.629113, mae: 62.081105, mean_q: 119.931224, mean_tau: 0.481119\n",
            " 26236/50000: episode: 151, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 22.248577, mae: 61.329161, mean_q: 118.670988, mean_tau: 0.477813\n",
            " 26436/50000: episode: 152, duration: 2.002s, episode steps: 200, steps per second: 100, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 16.480486, mae: 60.653447, mean_q: 117.261306, mean_tau: 0.473817\n",
            " 26589/50000: episode: 153, duration: 1.537s, episode steps: 153, steps per second: 100, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 25.797096, mae: 59.737653, mean_q: 115.287957, mean_tau: 0.470290\n",
            " 26789/50000: episode: 154, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 11.951888, mae: 59.479723, mean_q: 115.529140, mean_tau: 0.466764\n",
            " 26989/50000: episode: 155, duration: 1.440s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 17.921651, mae: 58.315581, mean_q: 112.452813, mean_tau: 0.462768\n",
            " 27117/50000: episode: 156, duration: 0.969s, episode steps: 128, steps per second: 132, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 19.798739, mae: 57.398690, mean_q: 110.241554, mean_tau: 0.459491\n",
            " 27220/50000: episode: 157, duration: 0.743s, episode steps: 103, steps per second: 139, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 30.093195, mae: 56.287711, mean_q: 107.742611, mean_tau: 0.457183\n",
            " 27355/50000: episode: 158, duration: 1.009s, episode steps: 135, steps per second: 134, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 10.956477, mae: 56.423126, mean_q: 109.626409, mean_tau: 0.454806\n",
            " 27524/50000: episode: 159, duration: 1.246s, episode steps: 169, steps per second: 136, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 12.707978, mae: 56.213900, mean_q: 108.354619, mean_tau: 0.451769\n",
            " 27655/50000: episode: 160, duration: 1.008s, episode steps: 131, steps per second: 130, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 17.816468, mae: 55.617620, mean_q: 107.434125, mean_tau: 0.448772\n",
            " 27778/50000: episode: 161, duration: 0.924s, episode steps: 123, steps per second: 133, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 12.577493, mae: 54.669632, mean_q: 105.769593, mean_tau: 0.446234\n",
            " 27921/50000: episode: 162, duration: 1.272s, episode steps: 143, steps per second: 112, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 13.953875, mae: 54.663890, mean_q: 105.616131, mean_tau: 0.443577\n",
            " 28121/50000: episode: 163, duration: 2.192s, episode steps: 200, steps per second:  91, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16.458515, mae: 53.910008, mean_q: 103.947523, mean_tau: 0.440150\n",
            " 28230/50000: episode: 164, duration: 0.835s, episode steps: 109, steps per second: 131, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 18.936086, mae: 52.987215, mean_q: 101.709021, mean_tau: 0.437064\n",
            " 28314/50000: episode: 165, duration: 0.656s, episode steps:  84, steps per second: 128, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 16.660494, mae: 53.144415, mean_q: 102.222072, mean_tau: 0.435135\n",
            " 28446/50000: episode: 166, duration: 0.970s, episode steps: 132, steps per second: 136, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 8.827829, mae: 53.539403, mean_q: 103.828558, mean_tau: 0.432978\n",
            " 28618/50000: episode: 167, duration: 1.292s, episode steps: 172, steps per second: 133, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 19.348855, mae: 52.730777, mean_q: 101.819595, mean_tau: 0.429941\n",
            " 28734/50000: episode: 168, duration: 0.852s, episode steps: 116, steps per second: 136, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 11.841071, mae: 52.890639, mean_q: 102.845738, mean_tau: 0.427064\n",
            " 28866/50000: episode: 169, duration: 0.968s, episode steps: 132, steps per second: 136, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 11.460852, mae: 52.791240, mean_q: 102.738795, mean_tau: 0.424586\n",
            " 29023/50000: episode: 170, duration: 1.213s, episode steps: 157, steps per second: 129, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 12.587406, mae: 52.739349, mean_q: 103.287326, mean_tau: 0.421699\n",
            " 29223/50000: episode: 171, duration: 1.487s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 15.540413, mae: 51.791643, mean_q: 100.645600, mean_tau: 0.418132\n",
            " 29423/50000: episode: 172, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 12.889729, mae: 52.107398, mean_q: 102.244361, mean_tau: 0.414136\n",
            " 29623/50000: episode: 173, duration: 2.146s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 11.344651, mae: 51.853510, mean_q: 101.865286, mean_tau: 0.410140\n",
            " 29750/50000: episode: 174, duration: 1.305s, episode steps: 127, steps per second:  97, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 9.668190, mae: 52.212614, mean_q: 102.999763, mean_tau: 0.406874\n",
            " 29950/50000: episode: 175, duration: 1.495s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.874311, mae: 51.043615, mean_q: 100.537427, mean_tau: 0.403607\n",
            " 30150/50000: episode: 176, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 19.677611, mae: 51.585397, mean_q: 101.929280, mean_tau: 0.399611\n",
            " 30297/50000: episode: 177, duration: 1.120s, episode steps: 147, steps per second: 131, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 15.375817, mae: 51.016008, mean_q: 100.538122, mean_tau: 0.396144\n",
            " 30426/50000: episode: 178, duration: 0.987s, episode steps: 129, steps per second: 131, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 8.858178, mae: 51.072783, mean_q: 101.311829, mean_tau: 0.393387\n",
            " 30626/50000: episode: 179, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 14.893959, mae: 50.689627, mean_q: 100.307699, mean_tau: 0.390101\n",
            " 30772/50000: episode: 180, duration: 1.104s, episode steps: 146, steps per second: 132, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 12.420522, mae: 50.963557, mean_q: 101.028947, mean_tau: 0.386644\n",
            " 30904/50000: episode: 181, duration: 0.950s, episode steps: 132, steps per second: 139, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 11.090168, mae: 50.794282, mean_q: 100.980329, mean_tau: 0.383867\n",
            " 31104/50000: episode: 182, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 13.918397, mae: 50.191169, mean_q: 99.512994, mean_tau: 0.380550\n",
            " 31264/50000: episode: 183, duration: 1.767s, episode steps: 160, steps per second:  91, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 17.567741, mae: 49.916681, mean_q: 99.067908, mean_tau: 0.376954\n",
            " 31464/50000: episode: 184, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 10.720623, mae: 50.346618, mean_q: 100.074485, mean_tau: 0.373357\n",
            " 31635/50000: episode: 185, duration: 1.231s, episode steps: 171, steps per second: 139, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 12.622068, mae: 50.361583, mean_q: 100.190797, mean_tau: 0.369651\n",
            " 31835/50000: episode: 186, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 11.309306, mae: 50.671939, mean_q: 101.188448, mean_tau: 0.365945\n",
            " 32035/50000: episode: 187, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 16.922236, mae: 50.612542, mean_q: 100.573167, mean_tau: 0.361949\n",
            " 32216/50000: episode: 188, duration: 1.353s, episode steps: 181, steps per second: 134, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 10.180120, mae: 50.204934, mean_q: 99.984251, mean_tau: 0.358143\n",
            " 32416/50000: episode: 189, duration: 1.544s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.289931, mae: 50.410450, mean_q: 100.786919, mean_tau: 0.354336\n",
            " 32616/50000: episode: 190, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.766177, mae: 50.564967, mean_q: 100.916363, mean_tau: 0.350340\n",
            " 32816/50000: episode: 191, duration: 2.128s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 19.021372, mae: 51.073775, mean_q: 101.803249, mean_tau: 0.346344\n",
            " 33016/50000: episode: 192, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.934852, mae: 51.161520, mean_q: 101.701734, mean_tau: 0.342348\n",
            " 33216/50000: episode: 193, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 12.733093, mae: 51.709839, mean_q: 103.275710, mean_tau: 0.338352\n",
            " 33334/50000: episode: 194, duration: 0.871s, episode steps: 118, steps per second: 135, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.734757, mae: 51.339476, mean_q: 102.457527, mean_tau: 0.335175\n",
            " 33534/50000: episode: 195, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 12.620605, mae: 51.664299, mean_q: 103.136389, mean_tau: 0.331999\n",
            " 33677/50000: episode: 196, duration: 1.060s, episode steps: 143, steps per second: 135, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 14.604057, mae: 51.409952, mean_q: 102.437587, mean_tau: 0.328572\n",
            " 33832/50000: episode: 197, duration: 1.177s, episode steps: 155, steps per second: 132, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 14.064295, mae: 51.122843, mean_q: 101.949206, mean_tau: 0.325595\n",
            " 34032/50000: episode: 198, duration: 1.492s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 18.288177, mae: 51.370725, mean_q: 102.395080, mean_tau: 0.322049\n",
            " 34232/50000: episode: 199, duration: 1.464s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.344590, mae: 50.523992, mean_q: 100.807008, mean_tau: 0.318053\n",
            " 34432/50000: episode: 200, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 9.995970, mae: 50.383374, mean_q: 100.677027, mean_tau: 0.314057\n",
            " 34632/50000: episode: 201, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 12.067839, mae: 50.397206, mean_q: 100.475562, mean_tau: 0.310061\n",
            " 34832/50000: episode: 202, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.856680, mae: 50.342220, mean_q: 100.478602, mean_tau: 0.306065\n",
            " 34989/50000: episode: 203, duration: 1.150s, episode steps: 157, steps per second: 137, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 12.941941, mae: 50.017880, mean_q: 99.720752, mean_tau: 0.302498\n",
            " 35189/50000: episode: 204, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 13.017165, mae: 49.354668, mean_q: 98.571149, mean_tau: 0.298932\n",
            " 35389/50000: episode: 205, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 11.175931, mae: 49.135863, mean_q: 98.233076, mean_tau: 0.294936\n",
            " 35589/50000: episode: 206, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.293789, mae: 49.706165, mean_q: 99.514534, mean_tau: 0.290940\n",
            " 35789/50000: episode: 207, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 15.715498, mae: 49.236482, mean_q: 98.210709, mean_tau: 0.286944\n",
            " 35989/50000: episode: 208, duration: 1.936s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 11.440808, mae: 49.615180, mean_q: 98.972614, mean_tau: 0.282948\n",
            " 36189/50000: episode: 209, duration: 1.907s, episode steps: 200, steps per second: 105, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 13.358136, mae: 48.892817, mean_q: 97.601686, mean_tau: 0.278952\n",
            " 36389/50000: episode: 210, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.027840, mae: 48.118072, mean_q: 96.012903, mean_tau: 0.274956\n",
            " 36550/50000: episode: 211, duration: 1.205s, episode steps: 161, steps per second: 134, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 9.236365, mae: 48.008823, mean_q: 96.051238, mean_tau: 0.271349\n",
            " 36750/50000: episode: 212, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 18.660973, mae: 47.614165, mean_q: 95.016111, mean_tau: 0.267743\n",
            " 36911/50000: episode: 213, duration: 1.382s, episode steps: 161, steps per second: 116, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 9.386020, mae: 47.481171, mean_q: 94.928092, mean_tau: 0.264137\n",
            " 37111/50000: episode: 214, duration: 1.723s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 12.605055, mae: 47.852328, mean_q: 95.863949, mean_tau: 0.260530\n",
            " 37311/50000: episode: 215, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 12.868204, mae: 47.069953, mean_q: 94.187788, mean_tau: 0.256534\n",
            " 37501/50000: episode: 216, duration: 2.143s, episode steps: 190, steps per second:  89, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 15.331928, mae: 47.834290, mean_q: 95.696089, mean_tau: 0.252638\n",
            " 37701/50000: episode: 217, duration: 2.026s, episode steps: 200, steps per second:  99, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.320138, mae: 47.498106, mean_q: 95.257927, mean_tau: 0.248742\n",
            " 37781/50000: episode: 218, duration: 0.739s, episode steps:  80, steps per second: 108, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.433526, mae: 47.960071, mean_q: 96.114691, mean_tau: 0.245945\n",
            " 37981/50000: episode: 219, duration: 1.878s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 9.924126, mae: 48.097030, mean_q: 96.335034, mean_tau: 0.243148\n",
            " 38144/50000: episode: 220, duration: 1.502s, episode steps: 163, steps per second: 109, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 6.868539, mae: 47.736464, mean_q: 95.631498, mean_tau: 0.239521\n",
            " 38175/50000: episode: 221, duration: 0.274s, episode steps:  31, steps per second: 113, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 5.463682, mae: 48.049547, mean_q: 96.463102, mean_tau: 0.237583\n",
            " 38198/50000: episode: 222, duration: 0.246s, episode steps:  23, steps per second:  93, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 8.237335, mae: 47.067750, mean_q: 94.126396, mean_tau: 0.237044\n",
            " 38346/50000: episode: 223, duration: 1.316s, episode steps: 148, steps per second: 112, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 14.124536, mae: 47.305959, mean_q: 94.480182, mean_tau: 0.235335\n",
            " 38371/50000: episode: 224, duration: 0.217s, episode steps:  25, steps per second: 115, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.838035, mae: 47.050178, mean_q: 94.291231, mean_tau: 0.233607\n",
            " 38571/50000: episode: 225, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 11.083428, mae: 47.671874, mean_q: 95.402923, mean_tau: 0.231359\n",
            " 38591/50000: episode: 226, duration: 0.172s, episode steps:  20, steps per second: 116, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.774881, mae: 47.772083, mean_q: 95.777946, mean_tau: 0.229162\n",
            " 38668/50000: episode: 227, duration: 0.685s, episode steps:  77, steps per second: 112, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.416 [0.000, 1.000],  loss: 21.474962, mae: 47.876756, mean_q: 95.245769, mean_tau: 0.228193\n",
            " 38821/50000: episode: 228, duration: 1.883s, episode steps: 153, steps per second:  81, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 12.424738, mae: 47.867781, mean_q: 95.610014, mean_tau: 0.225895\n",
            " 38879/50000: episode: 229, duration: 0.734s, episode steps:  58, steps per second:  79, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 15.266158, mae: 47.949782, mean_q: 95.480605, mean_tau: 0.223787\n",
            " 39012/50000: episode: 230, duration: 1.070s, episode steps: 133, steps per second: 124, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 11.404973, mae: 47.340219, mean_q: 94.717188, mean_tau: 0.221879\n",
            " 39032/50000: episode: 231, duration: 0.157s, episode steps:  20, steps per second: 128, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16.594407, mae: 47.686206, mean_q: 94.983121, mean_tau: 0.220350\n",
            " 39132/50000: episode: 232, duration: 0.714s, episode steps: 100, steps per second: 140, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.410 [0.000, 1.000],  loss: 31.349364, mae: 47.515368, mean_q: 94.176189, mean_tau: 0.219152\n",
            " 39332/50000: episode: 233, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.333784, mae: 47.412059, mean_q: 94.515578, mean_tau: 0.216155\n",
            " 39459/50000: episode: 234, duration: 0.971s, episode steps: 127, steps per second: 131, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 8.927507, mae: 46.844717, mean_q: 93.686589, mean_tau: 0.212888\n",
            " 39659/50000: episode: 235, duration: 1.489s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.069379, mae: 46.227742, mean_q: 92.490186, mean_tau: 0.209621\n",
            " 39798/50000: episode: 236, duration: 1.194s, episode steps: 139, steps per second: 116, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.453 [0.000, 1.000],  loss: 17.559362, mae: 46.026891, mean_q: 91.932063, mean_tau: 0.206235\n",
            " 39885/50000: episode: 237, duration: 0.788s, episode steps:  87, steps per second: 110, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.437 [0.000, 1.000],  loss: 10.608120, mae: 46.024107, mean_q: 91.915864, mean_tau: 0.203977\n",
            " 40035/50000: episode: 238, duration: 1.336s, episode steps: 150, steps per second: 112, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 14.686214, mae: 45.439283, mean_q: 90.634002, mean_tau: 0.201609\n",
            " 40159/50000: episode: 239, duration: 1.134s, episode steps: 124, steps per second: 109, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 8.869144, mae: 44.997343, mean_q: 90.159261, mean_tau: 0.198872\n",
            " 40359/50000: episode: 240, duration: 2.588s, episode steps: 200, steps per second:  77, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.953004, mae: 45.224336, mean_q: 90.230389, mean_tau: 0.195635\n",
            " 40385/50000: episode: 241, duration: 0.291s, episode steps:  26, steps per second:  89, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.147974, mae: 44.931191, mean_q: 90.042941, mean_tau: 0.193377\n",
            " 40585/50000: episode: 242, duration: 1.977s, episode steps: 200, steps per second: 101, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 15.650615, mae: 44.618814, mean_q: 89.049730, mean_tau: 0.191120\n",
            " 40606/50000: episode: 243, duration: 0.212s, episode steps:  21, steps per second:  99, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 12.984889, mae: 46.134781, mean_q: 92.151466, mean_tau: 0.188912\n",
            " 40626/50000: episode: 244, duration: 0.189s, episode steps:  20, steps per second: 106, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 20.979308, mae: 44.101537, mean_q: 87.804519, mean_tau: 0.188502\n",
            " 40657/50000: episode: 245, duration: 0.276s, episode steps:  31, steps per second: 112, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 6.179575, mae: 44.955907, mean_q: 89.847501, mean_tau: 0.187993\n",
            " 40671/50000: episode: 246, duration: 0.143s, episode steps:  14, steps per second:  98, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.640822, mae: 44.515619, mean_q: 88.593558, mean_tau: 0.187543\n",
            " 40689/50000: episode: 247, duration: 0.174s, episode steps:  18, steps per second: 103, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 20.690749, mae: 43.818236, mean_q: 87.263669, mean_tau: 0.187224\n",
            " 40703/50000: episode: 248, duration: 0.130s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.863362, mae: 45.327618, mean_q: 90.356027, mean_tau: 0.186904\n",
            " 40740/50000: episode: 249, duration: 0.355s, episode steps:  37, steps per second: 104, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.340728, mae: 44.935120, mean_q: 89.816877, mean_tau: 0.186394\n",
            " 40889/50000: episode: 250, duration: 1.262s, episode steps: 149, steps per second: 118, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 10.539537, mae: 44.563249, mean_q: 89.161088, mean_tau: 0.184536\n",
            " 41007/50000: episode: 251, duration: 0.983s, episode steps: 118, steps per second: 120, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 16.094084, mae: 44.449611, mean_q: 88.683075, mean_tau: 0.181869\n",
            " 41149/50000: episode: 252, duration: 1.176s, episode steps: 142, steps per second: 121, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 18.113016, mae: 44.308033, mean_q: 88.412502, mean_tau: 0.179272\n",
            " 41172/50000: episode: 253, duration: 0.196s, episode steps:  23, steps per second: 117, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 6.834412, mae: 44.066257, mean_q: 88.188514, mean_tau: 0.177623\n",
            " 41302/50000: episode: 254, duration: 1.048s, episode steps: 130, steps per second: 124, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 16.658873, mae: 43.796821, mean_q: 87.802028, mean_tau: 0.176095\n",
            " 41502/50000: episode: 255, duration: 1.519s, episode steps: 200, steps per second: 132, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 15.088821, mae: 43.843059, mean_q: 87.870605, mean_tau: 0.172798\n",
            " 41630/50000: episode: 256, duration: 1.533s, episode steps: 128, steps per second:  84, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 17.716682, mae: 43.495253, mean_q: 87.151685, mean_tau: 0.169521\n",
            " 41652/50000: episode: 257, duration: 0.296s, episode steps:  22, steps per second:  74, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 1.925671, mae: 44.039721, mean_q: 88.831385, mean_tau: 0.168023\n",
            " 41676/50000: episode: 258, duration: 0.301s, episode steps:  24, steps per second:  80, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.675144, mae: 43.467407, mean_q: 87.233180, mean_tau: 0.167563\n",
            " 41690/50000: episode: 259, duration: 0.189s, episode steps:  14, steps per second:  74, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 18.296178, mae: 44.185216, mean_q: 88.621230, mean_tau: 0.167184\n",
            " 41709/50000: episode: 260, duration: 0.268s, episode steps:  19, steps per second:  71, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.500276, mae: 43.910381, mean_q: 88.087014, mean_tau: 0.166854\n",
            " 41725/50000: episode: 261, duration: 0.218s, episode steps:  16, steps per second:  73, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 34.930137, mae: 45.251209, mean_q: 89.954792, mean_tau: 0.166504\n",
            " 41745/50000: episode: 262, duration: 0.230s, episode steps:  20, steps per second:  87, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 19.951344, mae: 43.163990, mean_q: 86.403489, mean_tau: 0.166145\n",
            " 41777/50000: episode: 263, duration: 0.297s, episode steps:  32, steps per second: 108, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.322489, mae: 43.792329, mean_q: 87.673633, mean_tau: 0.165625\n",
            " 41806/50000: episode: 264, duration: 0.272s, episode steps:  29, steps per second: 107, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 24.193184, mae: 42.591895, mean_q: 85.358517, mean_tau: 0.165016\n",
            " 41820/50000: episode: 265, duration: 0.120s, episode steps:  14, steps per second: 117, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.954515, mae: 44.597254, mean_q: 88.969807, mean_tau: 0.164586\n",
            " 41837/50000: episode: 266, duration: 0.157s, episode steps:  17, steps per second: 109, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 57.977229, mae: 43.384632, mean_q: 85.866641, mean_tau: 0.164277\n",
            " 41848/50000: episode: 267, duration: 0.110s, episode steps:  11, steps per second: 100, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 22.236252, mae: 42.183407, mean_q: 84.081354, mean_tau: 0.163997\n",
            " 41867/50000: episode: 268, duration: 0.193s, episode steps:  19, steps per second:  98, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 17.582938, mae: 43.066174, mean_q: 86.657755, mean_tau: 0.163697\n",
            " 41883/50000: episode: 269, duration: 0.154s, episode steps:  16, steps per second: 104, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22.693031, mae: 44.109360, mean_q: 88.213077, mean_tau: 0.163347\n",
            " 41901/50000: episode: 270, duration: 0.182s, episode steps:  18, steps per second:  99, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 14.714613, mae: 43.400481, mean_q: 86.611406, mean_tau: 0.163008\n",
            " 41914/50000: episode: 271, duration: 0.123s, episode steps:  13, steps per second: 105, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.118968, mae: 43.538691, mean_q: 87.427220, mean_tau: 0.162698\n",
            " 41931/50000: episode: 272, duration: 0.162s, episode steps:  17, steps per second: 105, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 15.250747, mae: 43.242043, mean_q: 86.645094, mean_tau: 0.162398\n",
            " 41966/50000: episode: 273, duration: 0.307s, episode steps:  35, steps per second: 114, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 36.936327, mae: 43.284297, mean_q: 86.332187, mean_tau: 0.161879\n",
            " 41987/50000: episode: 274, duration: 0.201s, episode steps:  21, steps per second: 104, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 19.949130, mae: 43.818437, mean_q: 87.083064, mean_tau: 0.161320\n",
            " 42005/50000: episode: 275, duration: 0.198s, episode steps:  18, steps per second:  91, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 30.085181, mae: 43.755293, mean_q: 86.963462, mean_tau: 0.160930\n",
            " 42020/50000: episode: 276, duration: 0.172s, episode steps:  15, steps per second:  87, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 30.766639, mae: 43.145365, mean_q: 85.999673, mean_tau: 0.160600\n",
            " 42044/50000: episode: 277, duration: 0.235s, episode steps:  24, steps per second: 102, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 45.928797, mae: 43.157357, mean_q: 85.512012, mean_tau: 0.160211\n",
            " 42063/50000: episode: 278, duration: 0.185s, episode steps:  19, steps per second: 103, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 53.567986, mae: 43.147336, mean_q: 85.140917, mean_tau: 0.159781\n",
            " 42075/50000: episode: 279, duration: 0.117s, episode steps:  12, steps per second: 102, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 15.439352, mae: 44.189590, mean_q: 88.407591, mean_tau: 0.159471\n",
            " 42086/50000: episode: 280, duration: 0.109s, episode steps:  11, steps per second: 101, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 18.003215, mae: 43.950002, mean_q: 87.728892, mean_tau: 0.159242\n",
            " 42121/50000: episode: 281, duration: 0.315s, episode steps:  35, steps per second: 111, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 21.690436, mae: 42.593396, mean_q: 84.834682, mean_tau: 0.158782\n",
            " 42321/50000: episode: 282, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 16.518821, mae: 43.242510, mean_q: 86.784497, mean_tau: 0.156434\n",
            " 42335/50000: episode: 283, duration: 0.129s, episode steps:  14, steps per second: 108, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16.033826, mae: 43.387291, mean_q: 87.496880, mean_tau: 0.154297\n",
            " 42352/50000: episode: 284, duration: 0.158s, episode steps:  17, steps per second: 108, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 26.885469, mae: 42.863710, mean_q: 86.145837, mean_tau: 0.153987\n",
            " 42496/50000: episode: 285, duration: 1.259s, episode steps: 144, steps per second: 114, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 18.103586, mae: 42.280871, mean_q: 85.038886, mean_tau: 0.152378\n",
            " 42517/50000: episode: 286, duration: 0.184s, episode steps:  21, steps per second: 114, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 27.219157, mae: 41.863849, mean_q: 83.804786, mean_tau: 0.150730\n",
            " 42543/50000: episode: 287, duration: 0.243s, episode steps:  26, steps per second: 107, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.844862, mae: 43.991614, mean_q: 88.461501, mean_tau: 0.150261\n",
            " 42594/50000: episode: 288, duration: 0.435s, episode steps:  51, steps per second: 117, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.451 [0.000, 1.000],  loss: 23.474358, mae: 43.006485, mean_q: 86.385726, mean_tau: 0.149491\n",
            " 42613/50000: episode: 289, duration: 0.170s, episode steps:  19, steps per second: 112, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 19.389504, mae: 43.748356, mean_q: 87.850834, mean_tau: 0.148792\n",
            " 42625/50000: episode: 290, duration: 0.107s, episode steps:  12, steps per second: 112, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.915747, mae: 42.593104, mean_q: 85.976749, mean_tau: 0.148482\n",
            " 42781/50000: episode: 291, duration: 1.312s, episode steps: 156, steps per second: 119, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 20.924804, mae: 42.455706, mean_q: 85.319547, mean_tau: 0.146804\n",
            " 42800/50000: episode: 292, duration: 0.165s, episode steps:  19, steps per second: 115, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 24.162243, mae: 41.930400, mean_q: 84.342505, mean_tau: 0.145056\n",
            " 42817/50000: episode: 293, duration: 0.167s, episode steps:  17, steps per second: 102, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 23.313117, mae: 42.100745, mean_q: 84.661030, mean_tau: 0.144696\n",
            " 42833/50000: episode: 294, duration: 0.145s, episode steps:  16, steps per second: 110, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.074806, mae: 42.257006, mean_q: 85.297115, mean_tau: 0.144366\n",
            " 42885/50000: episode: 295, duration: 0.663s, episode steps:  52, steps per second:  78, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 12.065108, mae: 42.198874, mean_q: 85.448486, mean_tau: 0.143687\n",
            " 42937/50000: episode: 296, duration: 0.667s, episode steps:  52, steps per second:  78, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 30.977528, mae: 42.702901, mean_q: 85.509478, mean_tau: 0.142648\n",
            " 42949/50000: episode: 297, duration: 0.163s, episode steps:  12, steps per second:  74, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 19.524747, mae: 40.121356, mean_q: 80.597904, mean_tau: 0.142009\n",
            " 43086/50000: episode: 298, duration: 1.593s, episode steps: 137, steps per second:  86, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 19.397907, mae: 42.081081, mean_q: 84.848012, mean_tau: 0.140520\n",
            " 43254/50000: episode: 299, duration: 1.500s, episode steps: 168, steps per second: 112, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 24.467167, mae: 41.707778, mean_q: 83.763380, mean_tau: 0.137473\n",
            " 43280/50000: episode: 300, duration: 0.250s, episode steps:  26, steps per second: 104, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 28.144288, mae: 41.132315, mean_q: 82.545456, mean_tau: 0.135535\n",
            " 43432/50000: episode: 301, duration: 1.300s, episode steps: 152, steps per second: 117, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 18.727586, mae: 41.386784, mean_q: 83.665884, mean_tau: 0.133757\n",
            " 43456/50000: episode: 302, duration: 0.230s, episode steps:  24, steps per second: 104, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 20.685937, mae: 41.611919, mean_q: 83.719448, mean_tau: 0.131999\n",
            " 43656/50000: episode: 303, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 15.843428, mae: 41.484160, mean_q: 83.814693, mean_tau: 0.129761\n",
            " 43677/50000: episode: 304, duration: 0.190s, episode steps:  21, steps per second: 111, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 19.349108, mae: 41.071299, mean_q: 83.060676, mean_tau: 0.127553\n",
            " 43706/50000: episode: 305, duration: 0.252s, episode steps:  29, steps per second: 115, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 29.524694, mae: 41.546214, mean_q: 83.946959, mean_tau: 0.127054\n",
            " 43722/50000: episode: 306, duration: 0.129s, episode steps:  16, steps per second: 124, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27.398668, mae: 42.215272, mean_q: 85.181931, mean_tau: 0.126604\n",
            " 43737/50000: episode: 307, duration: 0.125s, episode steps:  15, steps per second: 120, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 15.710489, mae: 41.991598, mean_q: 85.093646, mean_tau: 0.126295\n",
            " 43758/50000: episode: 308, duration: 0.182s, episode steps:  21, steps per second: 116, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 23.648577, mae: 40.265608, mean_q: 81.855251, mean_tau: 0.125935\n",
            " 43777/50000: episode: 309, duration: 0.171s, episode steps:  19, steps per second: 111, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 24.230331, mae: 42.113495, mean_q: 84.439859, mean_tau: 0.125535\n",
            " 43803/50000: episode: 310, duration: 0.214s, episode steps:  26, steps per second: 122, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 18.696876, mae: 41.600301, mean_q: 84.571967, mean_tau: 0.125086\n",
            " 43918/50000: episode: 311, duration: 0.987s, episode steps: 115, steps per second: 117, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 20.992564, mae: 42.251883, mean_q: 85.389296, mean_tau: 0.123677\n",
            " 43947/50000: episode: 312, duration: 0.272s, episode steps:  29, steps per second: 107, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 15.863352, mae: 42.093292, mean_q: 85.754903, mean_tau: 0.122239\n",
            " 44046/50000: episode: 313, duration: 0.842s, episode steps:  99, steps per second: 118, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 23.333631, mae: 41.460360, mean_q: 83.809824, mean_tau: 0.120960\n",
            " 44138/50000: episode: 314, duration: 0.822s, episode steps:  92, steps per second: 112, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 30.906542, mae: 41.724282, mean_q: 84.567392, mean_tau: 0.119052\n",
            " 44153/50000: episode: 315, duration: 0.132s, episode steps:  15, steps per second: 113, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 9.131205, mae: 42.110334, mean_q: 85.992557, mean_tau: 0.117983\n",
            " 44201/50000: episode: 316, duration: 0.503s, episode steps:  48, steps per second:  95, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 19.297205, mae: 42.596172, mean_q: 86.353380, mean_tau: 0.117354\n",
            " 44218/50000: episode: 317, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 44.865854, mae: 42.079633, mean_q: 84.993374, mean_tau: 0.116704\n",
            " 44237/50000: episode: 318, duration: 0.253s, episode steps:  19, steps per second:  75, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 11.281627, mae: 42.804017, mean_q: 87.144084, mean_tau: 0.116345\n",
            " 44271/50000: episode: 319, duration: 0.434s, episode steps:  34, steps per second:  78, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 39.778528, mae: 41.514345, mean_q: 83.718536, mean_tau: 0.115815\n",
            " 44289/50000: episode: 320, duration: 0.246s, episode steps:  18, steps per second:  73, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.830717, mae: 43.362427, mean_q: 87.386071, mean_tau: 0.115296\n",
            " 44305/50000: episode: 321, duration: 0.241s, episode steps:  16, steps per second:  66, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.777619, mae: 41.888741, mean_q: 85.444832, mean_tau: 0.114956\n",
            " 44402/50000: episode: 322, duration: 1.211s, episode steps:  97, steps per second:  80, episode reward: 97.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 22.840024, mae: 42.147427, mean_q: 85.420811, mean_tau: 0.113827\n",
            " 44433/50000: episode: 323, duration: 0.278s, episode steps:  31, steps per second: 111, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 19.941668, mae: 43.285670, mean_q: 88.436198, mean_tau: 0.112548\n",
            " 44457/50000: episode: 324, duration: 0.214s, episode steps:  24, steps per second: 112, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 14.629676, mae: 43.070238, mean_q: 88.344175, mean_tau: 0.111999\n",
            " 44503/50000: episode: 325, duration: 0.428s, episode steps:  46, steps per second: 107, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 19.589640, mae: 42.937781, mean_q: 87.842414, mean_tau: 0.111300\n",
            " 44539/50000: episode: 326, duration: 0.304s, episode steps:  36, steps per second: 118, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.469306, mae: 42.276367, mean_q: 86.399866, mean_tau: 0.110480\n",
            " 44567/50000: episode: 327, duration: 0.232s, episode steps:  28, steps per second: 121, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.133183, mae: 43.251729, mean_q: 87.811456, mean_tau: 0.109841\n",
            " 44594/50000: episode: 328, duration: 0.244s, episode steps:  27, steps per second: 111, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 27.393709, mae: 43.210557, mean_q: 87.943110, mean_tau: 0.109292\n",
            " 44608/50000: episode: 329, duration: 0.128s, episode steps:  14, steps per second: 109, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 43.723332, mae: 43.178902, mean_q: 87.727621, mean_tau: 0.108882\n",
            " 44621/50000: episode: 330, duration: 0.120s, episode steps:  13, steps per second: 108, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 32.655775, mae: 43.097537, mean_q: 88.142720, mean_tau: 0.108612\n",
            " 44653/50000: episode: 331, duration: 0.277s, episode steps:  32, steps per second: 116, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 21.495764, mae: 42.573514, mean_q: 87.060257, mean_tau: 0.108163\n",
            " 44670/50000: episode: 332, duration: 0.137s, episode steps:  17, steps per second: 124, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 21.901498, mae: 41.887322, mean_q: 85.778316, mean_tau: 0.107673\n",
            " 44686/50000: episode: 333, duration: 0.136s, episode steps:  16, steps per second: 117, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 24.873498, mae: 43.711950, mean_q: 89.760739, mean_tau: 0.107344\n",
            " 44705/50000: episode: 334, duration: 0.153s, episode steps:  19, steps per second: 124, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 27.483683, mae: 44.400501, mean_q: 90.743375, mean_tau: 0.106994\n",
            " 44727/50000: episode: 335, duration: 0.201s, episode steps:  22, steps per second: 109, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.459270, mae: 43.255329, mean_q: 88.591754, mean_tau: 0.106584\n",
            " 44742/50000: episode: 336, duration: 0.122s, episode steps:  15, steps per second: 122, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 35.631467, mae: 43.743210, mean_q: 89.420225, mean_tau: 0.106215\n",
            " 44754/50000: episode: 337, duration: 0.116s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 38.156909, mae: 43.172272, mean_q: 87.190479, mean_tau: 0.105945\n",
            " 44790/50000: episode: 338, duration: 0.283s, episode steps:  36, steps per second: 127, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 34.392289, mae: 43.790838, mean_q: 89.157494, mean_tau: 0.105465\n",
            " 44804/50000: episode: 339, duration: 0.124s, episode steps:  14, steps per second: 113, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.777450, mae: 44.104291, mean_q: 90.410265, mean_tau: 0.104966\n",
            " 44820/50000: episode: 340, duration: 0.139s, episode steps:  16, steps per second: 115, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 20.892135, mae: 44.435763, mean_q: 91.090012, mean_tau: 0.104666\n",
            " 44847/50000: episode: 341, duration: 0.234s, episode steps:  27, steps per second: 116, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 28.249605, mae: 42.251804, mean_q: 85.925500, mean_tau: 0.104237\n",
            " 44862/50000: episode: 342, duration: 0.126s, episode steps:  15, steps per second: 119, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 19.639151, mae: 44.842961, mean_q: 91.458123, mean_tau: 0.103817\n",
            " 44894/50000: episode: 343, duration: 0.272s, episode steps:  32, steps per second: 117, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 34.374193, mae: 44.231208, mean_q: 90.193434, mean_tau: 0.103348\n",
            " 44907/50000: episode: 344, duration: 0.115s, episode steps:  13, steps per second: 113, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 16.958846, mae: 45.189076, mean_q: 92.776316, mean_tau: 0.102898\n",
            " 44926/50000: episode: 345, duration: 0.165s, episode steps:  19, steps per second: 115, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 10.006322, mae: 44.456670, mean_q: 91.319982, mean_tau: 0.102578\n",
            " 44971/50000: episode: 346, duration: 0.389s, episode steps:  45, steps per second: 116, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 30.233355, mae: 44.139811, mean_q: 90.260429, mean_tau: 0.101939\n",
            " 44990/50000: episode: 347, duration: 0.158s, episode steps:  19, steps per second: 120, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 45.190842, mae: 44.860141, mean_q: 91.170386, mean_tau: 0.101300\n",
            " 45010/50000: episode: 348, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 18.392040, mae: 45.125507, mean_q: 92.126950, mean_tau: 0.100910\n",
            " 45027/50000: episode: 349, duration: 0.142s, episode steps:  17, steps per second: 119, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 27.577259, mae: 45.236341, mean_q: 92.693945, mean_tau: 0.100540\n",
            " 45053/50000: episode: 350, duration: 0.238s, episode steps:  26, steps per second: 109, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 47.597011, mae: 44.867752, mean_q: 90.719598, mean_tau: 0.100111\n",
            " 45071/50000: episode: 351, duration: 0.159s, episode steps:  18, steps per second: 113, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 45.736500, mae: 43.115844, mean_q: 87.055105, mean_tau: 0.099671\n",
            " 45093/50000: episode: 352, duration: 0.181s, episode steps:  22, steps per second: 122, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 25.883820, mae: 45.576708, mean_q: 93.005414, mean_tau: 0.099272\n",
            " 45158/50000: episode: 353, duration: 0.531s, episode steps:  65, steps per second: 122, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 36.472648, mae: 44.774708, mean_q: 90.875283, mean_tau: 0.098403\n",
            " 45186/50000: episode: 354, duration: 0.250s, episode steps:  28, steps per second: 112, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 48.993488, mae: 44.670000, mean_q: 90.681305, mean_tau: 0.097473\n",
            " 45198/50000: episode: 355, duration: 0.115s, episode steps:  12, steps per second: 104, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16.254433, mae: 44.916831, mean_q: 91.893906, mean_tau: 0.097074\n",
            " 45214/50000: episode: 356, duration: 0.142s, episode steps:  16, steps per second: 112, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27.013282, mae: 43.386701, mean_q: 88.935156, mean_tau: 0.096794\n",
            " 45231/50000: episode: 357, duration: 0.142s, episode steps:  17, steps per second: 120, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 35.980780, mae: 44.951764, mean_q: 91.472313, mean_tau: 0.096464\n",
            " 45251/50000: episode: 358, duration: 0.165s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 20.352268, mae: 45.654449, mean_q: 93.528288, mean_tau: 0.096095\n",
            " 45265/50000: episode: 359, duration: 0.118s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 32.545779, mae: 44.893327, mean_q: 92.599594, mean_tau: 0.095755\n",
            " 45285/50000: episode: 360, duration: 0.161s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 37.248224, mae: 44.483796, mean_q: 90.937600, mean_tau: 0.095415\n",
            " 45340/50000: episode: 361, duration: 0.434s, episode steps:  55, steps per second: 127, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 28.468040, mae: 45.385164, mean_q: 92.923341, mean_tau: 0.094666\n",
            " 45366/50000: episode: 362, duration: 0.213s, episode steps:  26, steps per second: 122, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 36.531082, mae: 45.178127, mean_q: 92.412287, mean_tau: 0.093857\n",
            " 45380/50000: episode: 363, duration: 0.121s, episode steps:  14, steps per second: 115, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 56.200031, mae: 47.675143, mean_q: 96.304307, mean_tau: 0.093457\n",
            " 45425/50000: episode: 364, duration: 0.364s, episode steps:  45, steps per second: 124, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 26.767003, mae: 46.445939, mean_q: 95.386129, mean_tau: 0.092868\n",
            " 45457/50000: episode: 365, duration: 0.259s, episode steps:  32, steps per second: 124, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 27.439799, mae: 45.780887, mean_q: 94.178479, mean_tau: 0.092099\n",
            " 45497/50000: episode: 366, duration: 0.317s, episode steps:  40, steps per second: 126, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 36.752021, mae: 46.403688, mean_q: 94.628541, mean_tau: 0.091380\n",
            " 45526/50000: episode: 367, duration: 0.240s, episode steps:  29, steps per second: 121, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 37.080821, mae: 46.350351, mean_q: 94.976430, mean_tau: 0.090690\n",
            " 45537/50000: episode: 368, duration: 0.090s, episode steps:  11, steps per second: 122, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 31.961015, mae: 46.316839, mean_q: 94.112468, mean_tau: 0.090291\n",
            " 45555/50000: episode: 369, duration: 0.141s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 30.015391, mae: 46.212143, mean_q: 94.213613, mean_tau: 0.090001\n",
            " 45575/50000: episode: 370, duration: 0.175s, episode steps:  20, steps per second: 115, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 38.549517, mae: 47.044597, mean_q: 96.426464, mean_tau: 0.089621\n",
            " 45593/50000: episode: 371, duration: 0.206s, episode steps:  18, steps per second:  87, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 22.623320, mae: 45.386941, mean_q: 93.143903, mean_tau: 0.089242\n",
            " 45607/50000: episode: 372, duration: 0.164s, episode steps:  14, steps per second:  86, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 37.762988, mae: 46.496993, mean_q: 96.195916, mean_tau: 0.088922\n",
            " 45662/50000: episode: 373, duration: 0.623s, episode steps:  55, steps per second:  88, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 28.183431, mae: 46.318594, mean_q: 95.327946, mean_tau: 0.088233\n",
            " 45688/50000: episode: 374, duration: 0.291s, episode steps:  26, steps per second:  89, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 44.546885, mae: 45.796340, mean_q: 93.614346, mean_tau: 0.087423\n",
            " 45708/50000: episode: 375, duration: 0.253s, episode steps:  20, steps per second:  79, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 30.248571, mae: 46.731682, mean_q: 95.931077, mean_tau: 0.086964\n",
            " 45731/50000: episode: 376, duration: 0.283s, episode steps:  23, steps per second:  81, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 19.348550, mae: 46.686031, mean_q: 96.965540, mean_tau: 0.086534\n",
            " 45752/50000: episode: 377, duration: 0.267s, episode steps:  21, steps per second:  79, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 29.907244, mae: 47.082319, mean_q: 96.834941, mean_tau: 0.086095\n",
            " 45769/50000: episode: 378, duration: 0.199s, episode steps:  17, steps per second:  85, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 37.672429, mae: 48.089813, mean_q: 99.096481, mean_tau: 0.085715\n",
            " 45789/50000: episode: 379, duration: 0.234s, episode steps:  20, steps per second:  85, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.997267, mae: 47.525186, mean_q: 97.480082, mean_tau: 0.085346\n",
            " 45819/50000: episode: 380, duration: 0.307s, episode steps:  30, steps per second:  98, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 36.861391, mae: 47.402909, mean_q: 97.118924, mean_tau: 0.084846\n",
            " 45848/50000: episode: 381, duration: 0.233s, episode steps:  29, steps per second: 124, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 42.488577, mae: 48.183557, mean_q: 98.693864, mean_tau: 0.084257\n",
            " 45873/50000: episode: 382, duration: 0.191s, episode steps:  25, steps per second: 131, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 41.255268, mae: 48.529431, mean_q: 99.320185, mean_tau: 0.083717\n",
            " 45896/50000: episode: 383, duration: 0.185s, episode steps:  23, steps per second: 124, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 46.007440, mae: 47.910001, mean_q: 97.575976, mean_tau: 0.083238\n",
            " 45924/50000: episode: 384, duration: 0.223s, episode steps:  28, steps per second: 125, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 37.941189, mae: 47.805935, mean_q: 98.114353, mean_tau: 0.082728\n",
            " 45944/50000: episode: 385, duration: 0.157s, episode steps:  20, steps per second: 127, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 37.626655, mae: 49.867071, mean_q: 101.881886, mean_tau: 0.082249\n",
            " 45955/50000: episode: 386, duration: 0.094s, episode steps:  11, steps per second: 118, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 62.230158, mae: 51.191771, mean_q: 102.961054, mean_tau: 0.081939\n",
            " 45991/50000: episode: 387, duration: 0.279s, episode steps:  36, steps per second: 129, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 31.757007, mae: 47.838728, mean_q: 98.165039, mean_tau: 0.081469\n",
            " 46033/50000: episode: 388, duration: 0.315s, episode steps:  42, steps per second: 133, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 20.043790, mae: 48.305646, mean_q: 100.092105, mean_tau: 0.080690\n",
            " 46048/50000: episode: 389, duration: 0.122s, episode steps:  15, steps per second: 122, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 21.324993, mae: 49.354845, mean_q: 101.967653, mean_tau: 0.080121\n",
            " 46064/50000: episode: 390, duration: 0.138s, episode steps:  16, steps per second: 116, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 43.414391, mae: 48.325434, mean_q: 99.911773, mean_tau: 0.079811\n",
            " 46091/50000: episode: 391, duration: 0.210s, episode steps:  27, steps per second: 128, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 47.510838, mae: 48.038064, mean_q: 98.620488, mean_tau: 0.079382\n",
            " 46111/50000: episode: 392, duration: 0.162s, episode steps:  20, steps per second: 124, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 38.384441, mae: 48.908219, mean_q: 100.197817, mean_tau: 0.078912\n",
            " 46129/50000: episode: 393, duration: 0.141s, episode steps:  18, steps per second: 127, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.605354, mae: 49.036762, mean_q: 101.442475, mean_tau: 0.078532\n",
            " 46161/50000: episode: 394, duration: 0.239s, episode steps:  32, steps per second: 134, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 34.983441, mae: 49.519767, mean_q: 102.859636, mean_tau: 0.078033\n",
            " 46193/50000: episode: 395, duration: 0.242s, episode steps:  32, steps per second: 132, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 31.833555, mae: 51.196334, mean_q: 106.068055, mean_tau: 0.077394\n",
            " 46214/50000: episode: 396, duration: 0.162s, episode steps:  21, steps per second: 130, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 51.389727, mae: 49.788543, mean_q: 103.499386, mean_tau: 0.076864\n",
            " 46225/50000: episode: 397, duration: 0.090s, episode steps:  11, steps per second: 122, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 47.952853, mae: 49.518905, mean_q: 101.222356, mean_tau: 0.076544\n",
            " 46245/50000: episode: 398, duration: 0.166s, episode steps:  20, steps per second: 121, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 49.242223, mae: 50.295540, mean_q: 103.126561, mean_tau: 0.076235\n",
            " 46291/50000: episode: 399, duration: 0.349s, episode steps:  46, steps per second: 132, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 48.885046, mae: 50.273868, mean_q: 103.445547, mean_tau: 0.075575\n",
            " 46316/50000: episode: 400, duration: 0.195s, episode steps:  25, steps per second: 128, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 50.510993, mae: 51.140934, mean_q: 105.327547, mean_tau: 0.074866\n",
            " 46353/50000: episode: 401, duration: 0.308s, episode steps:  37, steps per second: 120, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 42.200205, mae: 51.047011, mean_q: 105.197210, mean_tau: 0.074247\n",
            " 46370/50000: episode: 402, duration: 0.148s, episode steps:  17, steps per second: 115, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 68.464555, mae: 51.030460, mean_q: 105.274665, mean_tau: 0.073707\n",
            " 46388/50000: episode: 403, duration: 0.144s, episode steps:  18, steps per second: 125, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 45.104033, mae: 51.043525, mean_q: 105.572072, mean_tau: 0.073358\n",
            " 46458/50000: episode: 404, duration: 0.554s, episode steps:  70, steps per second: 126, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 46.819478, mae: 50.249760, mean_q: 103.548192, mean_tau: 0.072478\n",
            " 46471/50000: episode: 405, duration: 0.111s, episode steps:  13, steps per second: 117, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 47.839531, mae: 50.598614, mean_q: 104.555323, mean_tau: 0.071649\n",
            " 46498/50000: episode: 406, duration: 0.225s, episode steps:  27, steps per second: 120, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 52.721774, mae: 50.147333, mean_q: 104.461958, mean_tau: 0.071250\n",
            " 46520/50000: episode: 407, duration: 0.174s, episode steps:  22, steps per second: 127, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 42.874864, mae: 51.431762, mean_q: 106.541924, mean_tau: 0.070760\n",
            " 46611/50000: episode: 408, duration: 0.711s, episode steps:  91, steps per second: 128, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 46.464970, mae: 51.466491, mean_q: 106.645280, mean_tau: 0.069631\n",
            " 46629/50000: episode: 409, duration: 0.152s, episode steps:  18, steps per second: 118, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 48.358793, mae: 48.895147, mean_q: 100.670837, mean_tau: 0.068542\n",
            " 46647/50000: episode: 410, duration: 0.156s, episode steps:  18, steps per second: 116, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 32.832503, mae: 51.616799, mean_q: 106.733011, mean_tau: 0.068183\n",
            " 46658/50000: episode: 411, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 67.521069, mae: 51.080471, mean_q: 104.514759, mean_tau: 0.067893\n",
            " 46669/50000: episode: 412, duration: 0.089s, episode steps:  11, steps per second: 123, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 79.989603, mae: 50.080468, mean_q: 103.503091, mean_tau: 0.067673\n",
            " 46687/50000: episode: 413, duration: 0.153s, episode steps:  18, steps per second: 118, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 49.453847, mae: 51.845308, mean_q: 107.368995, mean_tau: 0.067384\n",
            " 46699/50000: episode: 414, duration: 0.098s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 35.673041, mae: 50.854008, mean_q: 105.244682, mean_tau: 0.067084\n",
            " 46711/50000: episode: 415, duration: 0.104s, episode steps:  12, steps per second: 115, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 29.462915, mae: 49.563735, mean_q: 102.586629, mean_tau: 0.066844\n",
            " 46726/50000: episode: 416, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 48.165155, mae: 54.318783, mean_q: 111.649335, mean_tau: 0.066574\n",
            " 46748/50000: episode: 417, duration: 0.184s, episode steps:  22, steps per second: 119, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 52.450665, mae: 51.646733, mean_q: 106.606950, mean_tau: 0.066205\n",
            " 46779/50000: episode: 418, duration: 0.237s, episode steps:  31, steps per second: 131, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 43.279718, mae: 51.513828, mean_q: 106.645663, mean_tau: 0.065675\n",
            " 46801/50000: episode: 419, duration: 0.177s, episode steps:  22, steps per second: 124, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 37.483277, mae: 51.283661, mean_q: 106.896998, mean_tau: 0.065146\n",
            " 46819/50000: episode: 420, duration: 0.154s, episode steps:  18, steps per second: 117, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 53.551910, mae: 52.477964, mean_q: 108.794699, mean_tau: 0.064746\n",
            " 46837/50000: episode: 421, duration: 0.143s, episode steps:  18, steps per second: 126, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 58.884997, mae: 51.600225, mean_q: 106.658657, mean_tau: 0.064387\n",
            " 46863/50000: episode: 422, duration: 0.219s, episode steps:  26, steps per second: 119, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 43.860670, mae: 52.040923, mean_q: 108.436498, mean_tau: 0.063947\n",
            " 46881/50000: episode: 423, duration: 0.149s, episode steps:  18, steps per second: 121, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 53.135568, mae: 53.311158, mean_q: 109.883504, mean_tau: 0.063507\n",
            " 46932/50000: episode: 424, duration: 0.421s, episode steps:  51, steps per second: 121, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 64.617717, mae: 51.623674, mean_q: 106.577488, mean_tau: 0.062818\n",
            " 46948/50000: episode: 425, duration: 0.141s, episode steps:  16, steps per second: 114, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 38.855278, mae: 52.658042, mean_q: 109.633576, mean_tau: 0.062149\n",
            " 46959/50000: episode: 426, duration: 0.099s, episode steps:  11, steps per second: 111, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 47.545203, mae: 50.741725, mean_q: 107.934832, mean_tau: 0.061879\n",
            " 46983/50000: episode: 427, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 53.664282, mae: 51.160876, mean_q: 106.521644, mean_tau: 0.061529\n",
            " 47021/50000: episode: 428, duration: 0.299s, episode steps:  38, steps per second: 127, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 37.379312, mae: 53.440529, mean_q: 111.623644, mean_tau: 0.060910\n",
            " 47052/50000: episode: 429, duration: 0.301s, episode steps:  31, steps per second: 103, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 52.364757, mae: 51.819245, mean_q: 109.004350, mean_tau: 0.060221\n",
            " 47072/50000: episode: 430, duration: 0.246s, episode steps:  20, steps per second:  81, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 57.335233, mae: 52.796577, mean_q: 111.127528, mean_tau: 0.059711\n",
            " 47091/50000: episode: 431, duration: 0.244s, episode steps:  19, steps per second:  78, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 52.552225, mae: 52.506097, mean_q: 109.760019, mean_tau: 0.059322\n",
            " 47123/50000: episode: 432, duration: 0.370s, episode steps:  32, steps per second:  87, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 45.301978, mae: 53.042870, mean_q: 110.064892, mean_tau: 0.058812\n",
            " 47143/50000: episode: 433, duration: 0.249s, episode steps:  20, steps per second:  80, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 63.495058, mae: 52.255819, mean_q: 109.357096, mean_tau: 0.058293\n",
            " 47169/50000: episode: 434, duration: 0.323s, episode steps:  26, steps per second:  81, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.414734, mae: 53.451154, mean_q: 111.749735, mean_tau: 0.057833\n",
            " 47188/50000: episode: 435, duration: 0.231s, episode steps:  19, steps per second:  82, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 45.399574, mae: 53.432002, mean_q: 112.507262, mean_tau: 0.057384\n",
            " 47209/50000: episode: 436, duration: 0.256s, episode steps:  21, steps per second:  82, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 45.028006, mae: 52.138653, mean_q: 109.522355, mean_tau: 0.056984\n",
            " 47236/50000: episode: 437, duration: 0.359s, episode steps:  27, steps per second:  75, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 50.190999, mae: 54.776716, mean_q: 114.579394, mean_tau: 0.056504\n",
            " 47254/50000: episode: 438, duration: 0.223s, episode steps:  18, steps per second:  81, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 47.056246, mae: 53.840696, mean_q: 112.568288, mean_tau: 0.056055\n",
            " 47274/50000: episode: 439, duration: 0.235s, episode steps:  20, steps per second:  85, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 59.371952, mae: 54.447444, mean_q: 114.183391, mean_tau: 0.055675\n",
            " 47294/50000: episode: 440, duration: 0.151s, episode steps:  20, steps per second: 132, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 44.105240, mae: 55.579011, mean_q: 116.441291, mean_tau: 0.055276\n",
            " 47310/50000: episode: 441, duration: 0.133s, episode steps:  16, steps per second: 120, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 71.072281, mae: 55.947179, mean_q: 114.885681, mean_tau: 0.054916\n",
            " 47334/50000: episode: 442, duration: 0.179s, episode steps:  24, steps per second: 134, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 60.808046, mae: 55.177406, mean_q: 115.290091, mean_tau: 0.054516\n",
            " 47362/50000: episode: 443, duration: 0.210s, episode steps:  28, steps per second: 133, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 50.236928, mae: 55.182995, mean_q: 115.362489, mean_tau: 0.053997\n",
            " 47384/50000: episode: 444, duration: 0.171s, episode steps:  22, steps per second: 128, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 53.894399, mae: 53.991839, mean_q: 113.407069, mean_tau: 0.053497\n",
            " 47401/50000: episode: 445, duration: 0.152s, episode steps:  17, steps per second: 112, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 38.352669, mae: 55.861364, mean_q: 117.359026, mean_tau: 0.053108\n",
            " 47415/50000: episode: 446, duration: 0.104s, episode steps:  14, steps per second: 135, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 82.838529, mae: 52.784806, mean_q: 109.385601, mean_tau: 0.052798\n",
            " 47481/50000: episode: 447, duration: 0.500s, episode steps:  66, steps per second: 132, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 51.387082, mae: 56.348129, mean_q: 118.126547, mean_tau: 0.051999\n",
            " 47494/50000: episode: 448, duration: 0.096s, episode steps:  13, steps per second: 135, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 71.712188, mae: 55.976293, mean_q: 116.554788, mean_tau: 0.051210\n",
            " 47534/50000: episode: 449, duration: 0.303s, episode steps:  40, steps per second: 132, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 68.787086, mae: 56.375713, mean_q: 117.423118, mean_tau: 0.050680\n",
            " 47547/50000: episode: 450, duration: 0.102s, episode steps:  13, steps per second: 127, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 82.804046, mae: 58.761752, mean_q: 121.162483, mean_tau: 0.050151\n",
            " 47571/50000: episode: 451, duration: 0.184s, episode steps:  24, steps per second: 131, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 54.542120, mae: 57.445824, mean_q: 119.556313, mean_tau: 0.049781\n",
            " 47593/50000: episode: 452, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 36.675344, mae: 58.445238, mean_q: 121.523758, mean_tau: 0.049322\n",
            " 47613/50000: episode: 453, duration: 0.153s, episode steps:  20, steps per second: 131, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 37.480977, mae: 57.267812, mean_q: 120.918314, mean_tau: 0.048902\n",
            " 47632/50000: episode: 454, duration: 0.146s, episode steps:  19, steps per second: 130, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 57.518901, mae: 54.082152, mean_q: 114.958862, mean_tau: 0.048512\n",
            " 47650/50000: episode: 455, duration: 0.138s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 53.700997, mae: 57.077766, mean_q: 120.194295, mean_tau: 0.048143\n",
            " 47669/50000: episode: 456, duration: 0.139s, episode steps:  19, steps per second: 137, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 57.079314, mae: 57.486472, mean_q: 121.500563, mean_tau: 0.047773\n",
            " 47692/50000: episode: 457, duration: 0.174s, episode steps:  23, steps per second: 132, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 62.016438, mae: 57.716298, mean_q: 120.617059, mean_tau: 0.047354\n",
            " 47722/50000: episode: 458, duration: 0.229s, episode steps:  30, steps per second: 131, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 63.572829, mae: 58.889856, mean_q: 123.543150, mean_tau: 0.046824\n",
            " 47742/50000: episode: 459, duration: 0.150s, episode steps:  20, steps per second: 133, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 44.270773, mae: 59.562500, mean_q: 124.390023, mean_tau: 0.046325\n",
            " 47769/50000: episode: 460, duration: 0.203s, episode steps:  27, steps per second: 133, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 74.915234, mae: 59.383121, mean_q: 124.044693, mean_tau: 0.045855\n",
            " 47790/50000: episode: 461, duration: 0.168s, episode steps:  21, steps per second: 125, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 78.551657, mae: 57.638592, mean_q: 120.646067, mean_tau: 0.045376\n",
            " 47821/50000: episode: 462, duration: 0.218s, episode steps:  31, steps per second: 142, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 52.203440, mae: 58.385978, mean_q: 122.114896, mean_tau: 0.044856\n",
            " 47843/50000: episode: 463, duration: 0.163s, episode steps:  22, steps per second: 135, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 52.377199, mae: 58.627531, mean_q: 121.943399, mean_tau: 0.044327\n",
            " 47865/50000: episode: 464, duration: 0.164s, episode steps:  22, steps per second: 134, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 63.166760, mae: 58.080579, mean_q: 122.044600, mean_tau: 0.043887\n",
            " 47883/50000: episode: 465, duration: 0.131s, episode steps:  18, steps per second: 138, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 28.350632, mae: 59.969637, mean_q: 127.218260, mean_tau: 0.043487\n",
            " 47893/50000: episode: 466, duration: 0.074s, episode steps:  10, steps per second: 135, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 110.205812, mae: 62.055376, mean_q: 129.762954, mean_tau: 0.043208\n",
            " 47907/50000: episode: 467, duration: 0.103s, episode steps:  14, steps per second: 135, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 56.952276, mae: 57.976041, mean_q: 120.763064, mean_tau: 0.042968\n",
            " 47917/50000: episode: 468, duration: 0.085s, episode steps:  10, steps per second: 117, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 43.522583, mae: 58.077524, mean_q: 123.769287, mean_tau: 0.042728\n",
            " 47965/50000: episode: 469, duration: 0.354s, episode steps:  48, steps per second: 135, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 83.092425, mae: 59.888669, mean_q: 125.628413, mean_tau: 0.042149\n",
            " 47981/50000: episode: 470, duration: 0.136s, episode steps:  16, steps per second: 118, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 33.938194, mae: 58.242891, mean_q: 124.266944, mean_tau: 0.041509\n",
            " 47993/50000: episode: 471, duration: 0.093s, episode steps:  12, steps per second: 129, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 83.728349, mae: 61.174954, mean_q: 127.971888, mean_tau: 0.041230\n",
            " 48019/50000: episode: 472, duration: 0.193s, episode steps:  26, steps per second: 135, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 59.462560, mae: 60.223816, mean_q: 126.109222, mean_tau: 0.040850\n",
            " 48061/50000: episode: 473, duration: 0.325s, episode steps:  42, steps per second: 129, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 73.607825, mae: 60.441671, mean_q: 126.374192, mean_tau: 0.040171\n",
            " 48086/50000: episode: 474, duration: 0.178s, episode steps:  25, steps per second: 140, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 46.021932, mae: 58.907633, mean_q: 125.785048, mean_tau: 0.039501\n",
            " 48119/50000: episode: 475, duration: 0.252s, episode steps:  33, steps per second: 131, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 64.297433, mae: 61.116757, mean_q: 127.381134, mean_tau: 0.038922\n",
            " 48137/50000: episode: 476, duration: 0.135s, episode steps:  18, steps per second: 133, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 57.454745, mae: 62.009917, mean_q: 130.579991, mean_tau: 0.038413\n",
            " 48148/50000: episode: 477, duration: 0.092s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 43.567736, mae: 60.913661, mean_q: 129.043565, mean_tau: 0.038123\n",
            " 48170/50000: episode: 478, duration: 0.168s, episode steps:  22, steps per second: 131, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 72.648300, mae: 60.143229, mean_q: 126.416730, mean_tau: 0.037793\n",
            " 48219/50000: episode: 479, duration: 0.379s, episode steps:  49, steps per second: 129, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 81.872153, mae: 61.803158, mean_q: 129.145201, mean_tau: 0.037084\n",
            " 48237/50000: episode: 480, duration: 0.144s, episode steps:  18, steps per second: 125, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 60.604399, mae: 62.617069, mean_q: 130.624382, mean_tau: 0.036415\n",
            " 48259/50000: episode: 481, duration: 0.176s, episode steps:  22, steps per second: 125, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 40.927545, mae: 63.686609, mean_q: 133.889282, mean_tau: 0.036015\n",
            " 48269/50000: episode: 482, duration: 0.083s, episode steps:  10, steps per second: 120, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 41.983531, mae: 65.832163, mean_q: 137.719554, mean_tau: 0.035695\n",
            " 48294/50000: episode: 483, duration: 0.195s, episode steps:  25, steps per second: 128, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 67.683499, mae: 65.064370, mean_q: 136.618628, mean_tau: 0.035346\n",
            " 48323/50000: episode: 484, duration: 0.243s, episode steps:  29, steps per second: 119, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 62.051383, mae: 63.037815, mean_q: 132.314926, mean_tau: 0.034806\n",
            " 48340/50000: episode: 485, duration: 0.132s, episode steps:  17, steps per second: 129, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 59.229229, mae: 64.373273, mean_q: 134.509551, mean_tau: 0.034347\n",
            " 48355/50000: episode: 486, duration: 0.129s, episode steps:  15, steps per second: 116, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 78.393737, mae: 63.722782, mean_q: 133.184169, mean_tau: 0.034027\n",
            " 48382/50000: episode: 487, duration: 0.215s, episode steps:  27, steps per second: 126, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 67.455447, mae: 62.765229, mean_q: 132.988786, mean_tau: 0.033607\n",
            " 48423/50000: episode: 488, duration: 0.331s, episode steps:  41, steps per second: 124, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 61.194682, mae: 64.205289, mean_q: 135.260561, mean_tau: 0.032928\n",
            " 48456/50000: episode: 489, duration: 0.281s, episode steps:  33, steps per second: 117, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 64.105234, mae: 64.843319, mean_q: 136.289569, mean_tau: 0.032189\n",
            " 48480/50000: episode: 490, duration: 0.197s, episode steps:  24, steps per second: 122, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 70.427435, mae: 62.780626, mean_q: 132.435591, mean_tau: 0.031619\n",
            " 48543/50000: episode: 491, duration: 0.538s, episode steps:  63, steps per second: 117, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 72.688390, mae: 65.706236, mean_q: 137.214815, mean_tau: 0.030750\n",
            " 48556/50000: episode: 492, duration: 0.160s, episode steps:  13, steps per second:  81, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 74.036271, mae: 64.418510, mean_q: 136.680182, mean_tau: 0.029991\n",
            " 48591/50000: episode: 493, duration: 0.436s, episode steps:  35, steps per second:  80, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 62.120753, mae: 66.882417, mean_q: 141.153637, mean_tau: 0.029511\n",
            " 48603/50000: episode: 494, duration: 0.152s, episode steps:  12, steps per second:  79, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 100.610859, mae: 67.106608, mean_q: 140.538941, mean_tau: 0.029042\n",
            " 48619/50000: episode: 495, duration: 0.206s, episode steps:  16, steps per second:  78, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 93.039405, mae: 64.376000, mean_q: 135.111671, mean_tau: 0.028762\n",
            " 48636/50000: episode: 496, duration: 0.216s, episode steps:  17, steps per second:  79, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 66.214045, mae: 66.427337, mean_q: 139.347891, mean_tau: 0.028433\n",
            " 48648/50000: episode: 497, duration: 0.149s, episode steps:  12, steps per second:  81, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 55.373228, mae: 64.037374, mean_q: 135.267666, mean_tau: 0.028143\n",
            " 48661/50000: episode: 498, duration: 0.179s, episode steps:  13, steps per second:  73, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 104.590009, mae: 66.580482, mean_q: 139.288035, mean_tau: 0.027893\n",
            " 48682/50000: episode: 499, duration: 0.268s, episode steps:  21, steps per second:  78, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 70.682877, mae: 65.382866, mean_q: 138.045585, mean_tau: 0.027553\n",
            " 48698/50000: episode: 500, duration: 0.184s, episode steps:  16, steps per second:  87, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 63.597148, mae: 67.434330, mean_q: 142.200778, mean_tau: 0.027184\n",
            " 48717/50000: episode: 501, duration: 0.257s, episode steps:  19, steps per second:  74, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 61.196759, mae: 66.112616, mean_q: 139.054886, mean_tau: 0.026834\n",
            " 48730/50000: episode: 502, duration: 0.171s, episode steps:  13, steps per second:  76, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 49.637039, mae: 64.497981, mean_q: 136.232246, mean_tau: 0.026514\n",
            " 48753/50000: episode: 503, duration: 0.275s, episode steps:  23, steps per second:  84, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 74.940112, mae: 66.969105, mean_q: 142.335055, mean_tau: 0.026155\n",
            " 48769/50000: episode: 504, duration: 0.194s, episode steps:  16, steps per second:  82, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 59.692940, mae: 66.458907, mean_q: 141.626536, mean_tau: 0.025765\n",
            " 48784/50000: episode: 505, duration: 0.120s, episode steps:  15, steps per second: 125, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 70.917503, mae: 67.441996, mean_q: 142.132306, mean_tau: 0.025456\n",
            " 48804/50000: episode: 506, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 54.704114, mae: 67.577803, mean_q: 143.654755, mean_tau: 0.025106\n",
            " 48822/50000: episode: 507, duration: 0.153s, episode steps:  18, steps per second: 118, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 97.956741, mae: 66.662428, mean_q: 139.882011, mean_tau: 0.024726\n",
            " 48846/50000: episode: 508, duration: 0.181s, episode steps:  24, steps per second: 132, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 82.266114, mae: 67.105462, mean_q: 140.389270, mean_tau: 0.024307\n",
            " 48859/50000: episode: 509, duration: 0.106s, episode steps:  13, steps per second: 123, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 77.824979, mae: 67.687725, mean_q: 141.728468, mean_tau: 0.023937\n",
            " 48876/50000: episode: 510, duration: 0.131s, episode steps:  17, steps per second: 130, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 74.185000, mae: 66.763521, mean_q: 140.211931, mean_tau: 0.023637\n",
            " 48930/50000: episode: 511, duration: 0.394s, episode steps:  54, steps per second: 137, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 91.857007, mae: 67.076502, mean_q: 141.012922, mean_tau: 0.022928\n",
            " 48946/50000: episode: 512, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 68.367083, mae: 69.695737, mean_q: 146.959538, mean_tau: 0.022229\n",
            " 48956/50000: episode: 513, duration: 0.088s, episode steps:  10, steps per second: 113, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 66.740170, mae: 71.481022, mean_q: 150.306741, mean_tau: 0.021969\n",
            " 48973/50000: episode: 514, duration: 0.134s, episode steps:  17, steps per second: 126, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 56.020426, mae: 71.174390, mean_q: 148.597586, mean_tau: 0.021699\n",
            " 48987/50000: episode: 515, duration: 0.116s, episode steps:  14, steps per second: 120, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 65.796448, mae: 66.677598, mean_q: 141.703745, mean_tau: 0.021390\n",
            " 49018/50000: episode: 516, duration: 0.229s, episode steps:  31, steps per second: 135, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 86.010373, mae: 68.596566, mean_q: 144.507612, mean_tau: 0.020940\n",
            " 49034/50000: episode: 517, duration: 0.113s, episode steps:  16, steps per second: 141, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 89.856547, mae: 69.625223, mean_q: 147.278957, mean_tau: 0.020471\n",
            " 49056/50000: episode: 518, duration: 0.162s, episode steps:  22, steps per second: 136, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 81.461114, mae: 68.481280, mean_q: 145.044653, mean_tau: 0.020091\n",
            " 49076/50000: episode: 519, duration: 0.145s, episode steps:  20, steps per second: 138, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 61.063165, mae: 70.818818, mean_q: 149.397232, mean_tau: 0.019671\n",
            " 49126/50000: episode: 520, duration: 0.362s, episode steps:  50, steps per second: 138, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.580 [0.000, 1.000],  loss: 80.452629, mae: 70.403856, mean_q: 148.416207, mean_tau: 0.018972\n",
            " 49140/50000: episode: 521, duration: 0.109s, episode steps:  14, steps per second: 129, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 88.871133, mae: 69.067374, mean_q: 146.438249, mean_tau: 0.018333\n",
            " 49183/50000: episode: 522, duration: 0.317s, episode steps:  43, steps per second: 136, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 67.698411, mae: 69.524204, mean_q: 146.812914, mean_tau: 0.017763\n",
            " 49205/50000: episode: 523, duration: 0.159s, episode steps:  22, steps per second: 138, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 88.555255, mae: 69.160748, mean_q: 145.994284, mean_tau: 0.017114\n",
            " 49229/50000: episode: 524, duration: 0.190s, episode steps:  24, steps per second: 126, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 100.241606, mae: 70.532811, mean_q: 149.379296, mean_tau: 0.016654\n",
            " 49245/50000: episode: 525, duration: 0.118s, episode steps:  16, steps per second: 136, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 61.999673, mae: 71.082997, mean_q: 151.074952, mean_tau: 0.016255\n",
            " 49260/50000: episode: 526, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 89.974126, mae: 70.631818, mean_q: 150.910650, mean_tau: 0.015945\n",
            " 49291/50000: episode: 527, duration: 0.221s, episode steps:  31, steps per second: 140, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 103.697630, mae: 70.465803, mean_q: 149.851151, mean_tau: 0.015486\n",
            " 49352/50000: episode: 528, duration: 0.448s, episode steps:  61, steps per second: 136, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 94.457828, mae: 72.046710, mean_q: 152.392731, mean_tau: 0.014566\n",
            " 49376/50000: episode: 529, duration: 0.191s, episode steps:  24, steps per second: 126, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 77.301980, mae: 73.688703, mean_q: 155.630286, mean_tau: 0.013717\n",
            " 49425/50000: episode: 530, duration: 0.369s, episode steps:  49, steps per second: 133, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.449 [0.000, 1.000],  loss: 78.485236, mae: 72.757710, mean_q: 154.114316, mean_tau: 0.012988\n",
            " 49459/50000: episode: 531, duration: 0.260s, episode steps:  34, steps per second: 131, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.559 [0.000, 1.000],  loss: 82.207947, mae: 69.474244, mean_q: 148.793266, mean_tau: 0.012159\n",
            " 49471/50000: episode: 532, duration: 0.094s, episode steps:  12, steps per second: 127, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 46.424815, mae: 72.861244, mean_q: 154.798369, mean_tau: 0.011699\n",
            " 49494/50000: episode: 533, duration: 0.193s, episode steps:  23, steps per second: 119, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 93.176843, mae: 76.203477, mean_q: 161.415591, mean_tau: 0.011350\n",
            " 49506/50000: episode: 534, duration: 0.091s, episode steps:  12, steps per second: 132, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 73.002043, mae: 73.757519, mean_q: 155.855256, mean_tau: 0.011000\n",
            " 49519/50000: episode: 535, duration: 0.098s, episode steps:  13, steps per second: 133, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 49.749659, mae: 77.594231, mean_q: 163.708419, mean_tau: 0.010750\n",
            " 49543/50000: episode: 536, duration: 0.182s, episode steps:  24, steps per second: 132, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 71.305750, mae: 75.085721, mean_q: 159.614190, mean_tau: 0.010381\n",
            " 49558/50000: episode: 537, duration: 0.111s, episode steps:  15, steps per second: 136, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 89.337020, mae: 75.449252, mean_q: 159.989591, mean_tau: 0.009991\n",
            " 49578/50000: episode: 538, duration: 0.159s, episode steps:  20, steps per second: 126, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 94.188706, mae: 75.208793, mean_q: 160.355676, mean_tau: 0.009641\n",
            " 49592/50000: episode: 539, duration: 0.108s, episode steps:  14, steps per second: 130, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 92.115445, mae: 74.321930, mean_q: 158.108846, mean_tau: 0.009302\n",
            " 49627/50000: episode: 540, duration: 0.277s, episode steps:  35, steps per second: 126, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 72.049257, mae: 76.481252, mean_q: 162.165448, mean_tau: 0.008812\n",
            " 49641/50000: episode: 541, duration: 0.107s, episode steps:  14, steps per second: 131, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 95.156334, mae: 73.026179, mean_q: 155.761591, mean_tau: 0.008323\n",
            " 49678/50000: episode: 542, duration: 0.292s, episode steps:  37, steps per second: 127, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 89.415152, mae: 77.754945, mean_q: 164.113626, mean_tau: 0.007813\n",
            " 49692/50000: episode: 543, duration: 0.109s, episode steps:  14, steps per second: 128, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 83.491478, mae: 78.399282, mean_q: 163.439114, mean_tau: 0.007304\n",
            " 49708/50000: episode: 544, duration: 0.139s, episode steps:  16, steps per second: 115, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 88.722353, mae: 75.844042, mean_q: 160.249802, mean_tau: 0.007004\n",
            " 49748/50000: episode: 545, duration: 0.297s, episode steps:  40, steps per second: 135, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 106.609986, mae: 77.722120, mean_q: 164.579406, mean_tau: 0.006445\n",
            " 49766/50000: episode: 546, duration: 0.138s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 93.960786, mae: 78.393858, mean_q: 165.704714, mean_tau: 0.005865\n",
            " 49780/50000: episode: 547, duration: 0.110s, episode steps:  14, steps per second: 127, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 56.736002, mae: 78.700939, mean_q: 166.526008, mean_tau: 0.005545\n",
            " 49797/50000: episode: 548, duration: 0.127s, episode steps:  17, steps per second: 134, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 149.468180, mae: 75.226488, mean_q: 160.095424, mean_tau: 0.005236\n",
            " 49823/50000: episode: 549, duration: 0.203s, episode steps:  26, steps per second: 128, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 84.357430, mae: 79.360750, mean_q: 167.960939, mean_tau: 0.004806\n",
            " 49843/50000: episode: 550, duration: 0.162s, episode steps:  20, steps per second: 123, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 119.295953, mae: 77.472691, mean_q: 163.457204, mean_tau: 0.004347\n",
            " 49856/50000: episode: 551, duration: 0.117s, episode steps:  13, steps per second: 112, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 61.843963, mae: 79.397267, mean_q: 167.770014, mean_tau: 0.004017\n",
            " 49878/50000: episode: 552, duration: 0.175s, episode steps:  22, steps per second: 126, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 74.896459, mae: 79.457667, mean_q: 168.663832, mean_tau: 0.003667\n",
            " 49902/50000: episode: 553, duration: 0.202s, episode steps:  24, steps per second: 119, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 99.702268, mae: 81.115935, mean_q: 170.631549, mean_tau: 0.003208\n",
            " 49915/50000: episode: 554, duration: 0.101s, episode steps:  13, steps per second: 129, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 58.782185, mae: 79.459892, mean_q: 168.898210, mean_tau: 0.002838\n",
            " 49950/50000: episode: 555, duration: 0.265s, episode steps:  35, steps per second: 132, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 89.049568, mae: 80.131248, mean_q: 168.596942, mean_tau: 0.002359\n",
            " 49992/50000: episode: 556, duration: 0.327s, episode steps:  42, steps per second: 129, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 104.852648, mae: 80.251251, mean_q: 170.011327, mean_tau: 0.001589\n",
            "done, took 431.502 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f403c3ad1c0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_outer =  LinearAnnealedPolicy(inner_policy=policy_inner, \n",
        "                               attr='tau',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.1, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy_outer) \n",
        "\n",
        "dqn.compile(Adam(lr=0.01), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Py5-P-PU9Gx",
        "outputId": "0b1af56f-fb91-4dcc-c71f-d30a21d998e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  129/10000: episode: 1, duration: 1.627s, episode steps: 129, steps per second:  79, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 5.076256, mae: 30.031866, mean_q: 60.792605, mean_tau: 0.993745\n",
            "  271/10000: episode: 2, duration: 1.411s, episode steps: 142, steps per second: 101, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 5.797374, mae: 30.207560, mean_q: 61.055602, mean_tau: 0.982045\n",
            "  453/10000: episode: 3, duration: 1.854s, episode steps: 182, steps per second:  98, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 7.056298, mae: 30.472780, mean_q: 61.289083, mean_tau: 0.967465\n",
            "  589/10000: episode: 4, duration: 0.957s, episode steps: 136, steps per second: 142, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 4.369361, mae: 30.699295, mean_q: 62.073603, mean_tau: 0.953155\n",
            "  719/10000: episode: 5, duration: 0.948s, episode steps: 130, steps per second: 137, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.490920, mae: 30.602236, mean_q: 61.954518, mean_tau: 0.941185\n",
            "  847/10000: episode: 6, duration: 0.979s, episode steps: 128, steps per second: 131, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.156820, mae: 31.125871, mean_q: 62.948476, mean_tau: 0.929575\n",
            " 1006/10000: episode: 7, duration: 1.283s, episode steps: 159, steps per second: 124, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.812215, mae: 31.180221, mean_q: 62.848227, mean_tau: 0.916660\n",
            " 1173/10000: episode: 8, duration: 1.325s, episode steps: 167, steps per second: 126, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.216116, mae: 31.381875, mean_q: 63.240049, mean_tau: 0.901990\n",
            " 1321/10000: episode: 9, duration: 1.176s, episode steps: 148, steps per second: 126, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.702734, mae: 31.414537, mean_q: 63.333280, mean_tau: 0.887815\n",
            " 1471/10000: episode: 10, duration: 1.497s, episode steps: 150, steps per second: 100, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 5.215825, mae: 31.485189, mean_q: 63.479359, mean_tau: 0.874405\n",
            " 1606/10000: episode: 11, duration: 1.159s, episode steps: 135, steps per second: 116, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 4.813255, mae: 31.362903, mean_q: 63.110637, mean_tau: 0.861580\n",
            " 1757/10000: episode: 12, duration: 1.424s, episode steps: 151, steps per second: 106, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 3.561769, mae: 31.533803, mean_q: 63.514735, mean_tau: 0.848710\n",
            " 1917/10000: episode: 13, duration: 1.736s, episode steps: 160, steps per second:  92, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.290344, mae: 31.917394, mean_q: 64.377259, mean_tau: 0.834715\n",
            " 2079/10000: episode: 14, duration: 1.216s, episode steps: 162, steps per second: 133, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.951796, mae: 32.036441, mean_q: 64.613481, mean_tau: 0.820225\n",
            " 2238/10000: episode: 15, duration: 1.162s, episode steps: 159, steps per second: 137, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.389780, mae: 32.113274, mean_q: 64.838773, mean_tau: 0.805780\n",
            " 2387/10000: episode: 16, duration: 1.070s, episode steps: 149, steps per second: 139, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.787595, mae: 32.261456, mean_q: 65.033987, mean_tau: 0.791920\n",
            " 2552/10000: episode: 17, duration: 1.221s, episode steps: 165, steps per second: 135, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.745770, mae: 32.276230, mean_q: 64.985076, mean_tau: 0.777790\n",
            " 2698/10000: episode: 18, duration: 1.266s, episode steps: 146, steps per second: 115, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.620217, mae: 32.458342, mean_q: 65.394835, mean_tau: 0.763795\n",
            " 2856/10000: episode: 19, duration: 1.554s, episode steps: 158, steps per second: 102, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.640848, mae: 32.364510, mean_q: 65.019129, mean_tau: 0.750115\n",
            " 3001/10000: episode: 20, duration: 1.272s, episode steps: 145, steps per second: 114, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.816564, mae: 32.247800, mean_q: 64.925173, mean_tau: 0.736480\n",
            " 3143/10000: episode: 21, duration: 1.244s, episode steps: 142, steps per second: 114, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 6.443985, mae: 32.303857, mean_q: 64.817600, mean_tau: 0.723565\n",
            " 3286/10000: episode: 22, duration: 2.444s, episode steps: 143, steps per second:  59, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.291334, mae: 32.329920, mean_q: 65.132207, mean_tau: 0.710740\n",
            " 3443/10000: episode: 23, duration: 2.205s, episode steps: 157, steps per second:  71, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 4.192467, mae: 32.404437, mean_q: 65.269817, mean_tau: 0.697240\n",
            " 3610/10000: episode: 24, duration: 1.243s, episode steps: 167, steps per second: 134, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.080603, mae: 32.450845, mean_q: 65.364229, mean_tau: 0.682660\n",
            " 3777/10000: episode: 25, duration: 1.228s, episode steps: 167, steps per second: 136, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.098258, mae: 32.455312, mean_q: 65.384690, mean_tau: 0.667630\n",
            " 3947/10000: episode: 26, duration: 1.311s, episode steps: 170, steps per second: 130, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.830426, mae: 32.486121, mean_q: 65.551791, mean_tau: 0.652465\n",
            " 4124/10000: episode: 27, duration: 1.318s, episode steps: 177, steps per second: 134, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.074129, mae: 32.533260, mean_q: 65.649035, mean_tau: 0.636850\n",
            " 4259/10000: episode: 28, duration: 1.002s, episode steps: 135, steps per second: 135, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.437498, mae: 32.484981, mean_q: 65.612366, mean_tau: 0.622810\n",
            " 4433/10000: episode: 29, duration: 1.361s, episode steps: 174, steps per second: 128, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.549366, mae: 33.046563, mean_q: 66.570228, mean_tau: 0.608905\n",
            " 4595/10000: episode: 30, duration: 1.241s, episode steps: 162, steps per second: 131, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.517343, mae: 33.025006, mean_q: 66.330419, mean_tau: 0.593785\n",
            " 4739/10000: episode: 31, duration: 1.133s, episode steps: 144, steps per second: 127, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 6.164636, mae: 32.849622, mean_q: 65.988232, mean_tau: 0.580015\n",
            " 4908/10000: episode: 32, duration: 1.769s, episode steps: 169, steps per second:  96, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.436075, mae: 33.251509, mean_q: 66.741938, mean_tau: 0.565930\n",
            " 5075/10000: episode: 33, duration: 1.737s, episode steps: 167, steps per second:  96, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.436575, mae: 33.403074, mean_q: 67.273705, mean_tau: 0.550810\n",
            " 5236/10000: episode: 34, duration: 1.200s, episode steps: 161, steps per second: 134, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.480350, mae: 32.905603, mean_q: 66.239977, mean_tau: 0.536050\n",
            " 5392/10000: episode: 35, duration: 1.221s, episode steps: 156, steps per second: 128, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.605783, mae: 33.061582, mean_q: 66.806150, mean_tau: 0.521785\n",
            " 5567/10000: episode: 36, duration: 1.353s, episode steps: 175, steps per second: 129, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 3.128955, mae: 33.170224, mean_q: 66.944014, mean_tau: 0.506890\n",
            " 5741/10000: episode: 37, duration: 1.923s, episode steps: 174, steps per second:  90, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 5.788386, mae: 33.662370, mean_q: 67.731444, mean_tau: 0.491185\n",
            " 5904/10000: episode: 38, duration: 1.179s, episode steps: 163, steps per second: 138, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.648441, mae: 33.664301, mean_q: 67.770859, mean_tau: 0.476020\n",
            " 6055/10000: episode: 39, duration: 1.192s, episode steps: 151, steps per second: 127, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.318564, mae: 33.279454, mean_q: 67.061541, mean_tau: 0.461890\n",
            " 6255/10000: episode: 40, duration: 1.543s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.164654, mae: 33.587201, mean_q: 67.676166, mean_tau: 0.446095\n",
            " 6429/10000: episode: 41, duration: 1.930s, episode steps: 174, steps per second:  90, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.998236, mae: 33.565464, mean_q: 67.648297, mean_tau: 0.429265\n",
            " 6591/10000: episode: 42, duration: 1.573s, episode steps: 162, steps per second: 103, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.370056, mae: 33.482722, mean_q: 67.387848, mean_tau: 0.414145\n",
            " 6755/10000: episode: 43, duration: 1.329s, episode steps: 164, steps per second: 123, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 6.191704, mae: 33.727612, mean_q: 67.644801, mean_tau: 0.399475\n",
            " 6945/10000: episode: 44, duration: 1.439s, episode steps: 190, steps per second: 132, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.681975, mae: 33.553880, mean_q: 67.630708, mean_tau: 0.383545\n",
            " 7133/10000: episode: 45, duration: 1.396s, episode steps: 188, steps per second: 135, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.596501, mae: 33.570222, mean_q: 67.666211, mean_tau: 0.366535\n",
            " 7322/10000: episode: 46, duration: 1.420s, episode steps: 189, steps per second: 133, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 5.006740, mae: 33.770169, mean_q: 68.098742, mean_tau: 0.349570\n",
            " 7482/10000: episode: 47, duration: 1.234s, episode steps: 160, steps per second: 130, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.804333, mae: 33.669240, mean_q: 67.919723, mean_tau: 0.333865\n",
            " 7636/10000: episode: 48, duration: 1.169s, episode steps: 154, steps per second: 132, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.505005, mae: 33.903633, mean_q: 68.455145, mean_tau: 0.319735\n",
            " 7821/10000: episode: 49, duration: 1.354s, episode steps: 185, steps per second: 137, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.479495, mae: 33.810117, mean_q: 68.275547, mean_tau: 0.304480\n",
            " 7998/10000: episode: 50, duration: 1.875s, episode steps: 177, steps per second:  94, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.932601, mae: 33.682827, mean_q: 68.018086, mean_tau: 0.288190\n",
            " 8186/10000: episode: 51, duration: 1.677s, episode steps: 188, steps per second: 112, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.683176, mae: 33.654060, mean_q: 67.825284, mean_tau: 0.271765\n",
            " 8349/10000: episode: 52, duration: 1.248s, episode steps: 163, steps per second: 131, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 3.230080, mae: 33.495408, mean_q: 67.644974, mean_tau: 0.255970\n",
            " 8506/10000: episode: 53, duration: 1.221s, episode steps: 157, steps per second: 129, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.001825, mae: 33.665000, mean_q: 67.993168, mean_tau: 0.241570\n",
            " 8672/10000: episode: 54, duration: 1.309s, episode steps: 166, steps per second: 127, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.233853, mae: 33.882298, mean_q: 68.332924, mean_tau: 0.227035\n",
            " 8854/10000: episode: 55, duration: 1.398s, episode steps: 182, steps per second: 130, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.372078, mae: 33.827161, mean_q: 68.360103, mean_tau: 0.211375\n",
            " 9033/10000: episode: 56, duration: 1.382s, episode steps: 179, steps per second: 130, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.353228, mae: 34.060409, mean_q: 68.901866, mean_tau: 0.195130\n",
            " 9233/10000: episode: 57, duration: 1.561s, episode steps: 200, steps per second: 128, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 4.343312, mae: 34.298668, mean_q: 69.315160, mean_tau: 0.178075\n",
            " 9398/10000: episode: 58, duration: 1.216s, episode steps: 165, steps per second: 136, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.819879, mae: 34.330372, mean_q: 69.274732, mean_tau: 0.161650\n",
            " 9531/10000: episode: 59, duration: 1.442s, episode steps: 133, steps per second:  92, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.261924, mae: 34.351503, mean_q: 69.460493, mean_tau: 0.148240\n",
            " 9569/10000: episode: 60, duration: 0.417s, episode steps:  38, steps per second:  91, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 3.097454, mae: 34.643735, mean_q: 70.144250, mean_tau: 0.140545\n",
            " 9624/10000: episode: 61, duration: 0.605s, episode steps:  55, steps per second:  91, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.351543, mae: 34.901998, mean_q: 70.464644, mean_tau: 0.136360\n",
            " 9655/10000: episode: 62, duration: 0.296s, episode steps:  31, steps per second: 105, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 3.285295, mae: 34.278466, mean_q: 69.229080, mean_tau: 0.132490\n",
            " 9787/10000: episode: 63, duration: 0.949s, episode steps: 132, steps per second: 139, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.475070, mae: 34.856936, mean_q: 70.380520, mean_tau: 0.125155\n",
            " 9832/10000: episode: 64, duration: 0.329s, episode steps:  45, steps per second: 137, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.222523, mae: 34.384267, mean_q: 69.505751, mean_tau: 0.117190\n",
            " 9866/10000: episode: 65, duration: 0.244s, episode steps:  34, steps per second: 139, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 5.045628, mae: 35.051802, mean_q: 70.686560, mean_tau: 0.113635\n",
            " 9895/10000: episode: 66, duration: 0.217s, episode steps:  29, steps per second: 134, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 2.843111, mae: 34.713506, mean_q: 70.006524, mean_tau: 0.110800\n",
            " 9926/10000: episode: 67, duration: 0.241s, episode steps:  31, steps per second: 129, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 6.797496, mae: 34.623434, mean_q: 69.665695, mean_tau: 0.108100\n",
            " 9949/10000: episode: 68, duration: 0.177s, episode steps:  23, steps per second: 130, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.473678, mae: 34.916128, mean_q: 70.556062, mean_tau: 0.105670\n",
            " 9987/10000: episode: 69, duration: 0.295s, episode steps:  38, steps per second: 129, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.080168, mae: 35.226841, mean_q: 71.314070, mean_tau: 0.102925\n",
            "done, took 86.149 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 168.000, steps: 168\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 164.000, steps: 164\n",
            "Episode 4: reward: 182.000, steps: 182\n",
            "Episode 5: reward: 161.000, steps: 161\n",
            "Episode 6: reward: 180.000, steps: 180\n",
            "Episode 7: reward: 172.000, steps: 172\n",
            "Episode 8: reward: 166.000, steps: 166\n",
            "Episode 9: reward: 171.000, steps: 171\n",
            "Episode 10: reward: 171.000, steps: 171\n",
            "Episode 11: reward: 168.000, steps: 168\n",
            "Episode 12: reward: 178.000, steps: 178\n",
            "Episode 13: reward: 169.000, steps: 169\n",
            "Episode 14: reward: 166.000, steps: 166\n",
            "Episode 15: reward: 168.000, steps: 168\n",
            "Episode 16: reward: 159.000, steps: 159\n",
            "Episode 17: reward: 177.000, steps: 177\n",
            "Episode 18: reward: 156.000, steps: 156\n",
            "Episode 19: reward: 172.000, steps: 172\n",
            "Episode 20: reward: 175.000, steps: 175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f403c5e1970>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "mSMONS5SU7hy",
        "outputId": "1b7db9a8-f541-4e98-dd0d-a2531222911a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f403c7205e0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "351a1687-a82b-4056-b2d3-e17a8e06d080"
      },
      "source": [
        "policy_outer =  LinearAnnealedPolicy(inner_policy=policy_inner, \n",
        "                               attr='tau',            \n",
        "                               value_max=1.0,\n",
        "                               value_min=0.01, \n",
        "                               value_test=.05,\n",
        "                               nb_steps=10000)\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=env.action_space.n,\n",
        "               memory=memory,\n",
        "               nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, \n",
        "               policy=policy_outer) \n",
        "\n",
        "dqn.compile(Adam(lr=0.01), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   17/10000: episode: 1, duration: 0.827s, episode steps:  17, steps per second:  21, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.235 [0.000, 1.000],  loss: 0.371548, mae: 0.517421, mean_q: 0.053184, mean_tau: 0.998664\n",
            "   33/10000: episode: 2, duration: 0.125s, episode steps:  16, steps per second: 128, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.108541, mae: 0.405230, mean_q: 0.614360, mean_tau: 0.997575\n",
            "   82/10000: episode: 3, duration: 0.330s, episode steps:  49, steps per second: 148, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.011081, mae: 0.609301, mean_q: 1.242141, mean_tau: 0.994357\n",
            "  135/10000: episode: 4, duration: 0.358s, episode steps:  53, steps per second: 148, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 0.007794, mae: 0.825500, mean_q: 1.676276, mean_tau: 0.989308\n",
            "  167/10000: episode: 5, duration: 0.230s, episode steps:  32, steps per second: 139, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 0.016765, mae: 1.047011, mean_q: 2.115447, mean_tau: 0.985101\n",
            "  186/10000: episode: 6, duration: 0.134s, episode steps:  19, steps per second: 141, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.035299, mae: 1.148367, mean_q: 2.323359, mean_tau: 0.982576\n",
            "  209/10000: episode: 7, duration: 0.176s, episode steps:  23, steps per second: 131, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.026926, mae: 1.218820, mean_q: 2.412194, mean_tau: 0.980497\n",
            "  224/10000: episode: 8, duration: 0.112s, episode steps:  15, steps per second: 133, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.064691, mae: 1.293386, mean_q: 2.528385, mean_tau: 0.978616\n",
            "  242/10000: episode: 9, duration: 0.125s, episode steps:  18, steps per second: 144, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.058155, mae: 1.373452, mean_q: 2.662802, mean_tau: 0.976982\n",
            "  262/10000: episode: 10, duration: 0.140s, episode steps:  20, steps per second: 142, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.067640, mae: 1.464536, mean_q: 2.831617, mean_tau: 0.975101\n",
            "  276/10000: episode: 11, duration: 0.114s, episode steps:  14, steps per second: 123, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.076683, mae: 1.551891, mean_q: 2.973675, mean_tau: 0.973418\n",
            "  315/10000: episode: 12, duration: 0.263s, episode steps:  39, steps per second: 149, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.091284, mae: 1.659971, mean_q: 3.169561, mean_tau: 0.970795\n",
            "  341/10000: episode: 13, duration: 0.361s, episode steps:  26, steps per second:  72, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.089187, mae: 1.791782, mean_q: 3.466957, mean_tau: 0.967578\n",
            "  355/10000: episode: 14, duration: 0.093s, episode steps:  14, steps per second: 150, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.096076, mae: 1.890282, mean_q: 3.658687, mean_tau: 0.965598\n",
            "  368/10000: episode: 15, duration: 0.086s, episode steps:  13, steps per second: 150, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.132027, mae: 1.957920, mean_q: 3.765069, mean_tau: 0.964261\n",
            "  422/10000: episode: 16, duration: 0.794s, episode steps:  54, steps per second:  68, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.111009, mae: 2.086187, mean_q: 4.028591, mean_tau: 0.960944\n",
            "  479/10000: episode: 17, duration: 0.875s, episode steps:  57, steps per second:  65, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.439 [0.000, 1.000],  loss: 0.127877, mae: 2.322077, mean_q: 4.504736, mean_tau: 0.955450\n",
            "  492/10000: episode: 18, duration: 0.137s, episode steps:  13, steps per second:  95, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.132120, mae: 2.447232, mean_q: 4.717429, mean_tau: 0.951985\n",
            "  512/10000: episode: 19, duration: 0.195s, episode steps:  20, steps per second: 103, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.128641, mae: 2.555912, mean_q: 4.979732, mean_tau: 0.950352\n",
            "  538/10000: episode: 20, duration: 0.259s, episode steps:  26, steps per second: 100, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.158647, mae: 2.639211, mean_q: 5.123924, mean_tau: 0.948075\n",
            "  560/10000: episode: 21, duration: 0.244s, episode steps:  22, steps per second:  90, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.167433, mae: 2.737380, mean_q: 5.291423, mean_tau: 0.945698\n",
            "  575/10000: episode: 22, duration: 0.150s, episode steps:  15, steps per second: 100, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.170535, mae: 2.817794, mean_q: 5.468618, mean_tau: 0.943867\n",
            "  584/10000: episode: 23, duration: 0.101s, episode steps:   9, steps per second:  89, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.277826, mae: 2.859253, mean_q: 5.485140, mean_tau: 0.942679\n",
            "  609/10000: episode: 24, duration: 0.257s, episode steps:  25, steps per second:  97, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.237580, mae: 2.918236, mean_q: 5.653837, mean_tau: 0.940996\n",
            "  649/10000: episode: 25, duration: 0.298s, episode steps:  40, steps per second: 134, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.236138, mae: 3.054609, mean_q: 5.933053, mean_tau: 0.937778\n",
            "  682/10000: episode: 26, duration: 0.241s, episode steps:  33, steps per second: 137, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.424 [0.000, 1.000],  loss: 0.225203, mae: 3.203279, mean_q: 6.228182, mean_tau: 0.934165\n",
            "  696/10000: episode: 27, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.149961, mae: 3.249524, mean_q: 6.459739, mean_tau: 0.931838\n",
            "  708/10000: episode: 28, duration: 0.083s, episode steps:  12, steps per second: 145, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.295116, mae: 3.267162, mean_q: 6.394154, mean_tau: 0.930552\n",
            "  755/10000: episode: 29, duration: 0.315s, episode steps:  47, steps per second: 149, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.308656, mae: 3.458793, mean_q: 6.727160, mean_tau: 0.927631\n",
            "  794/10000: episode: 30, duration: 0.275s, episode steps:  39, steps per second: 142, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 0.195713, mae: 3.675361, mean_q: 7.223770, mean_tau: 0.923374\n",
            "  858/10000: episode: 31, duration: 0.454s, episode steps:  64, steps per second: 141, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.248195, mae: 3.846773, mean_q: 7.616866, mean_tau: 0.918276\n",
            "  922/10000: episode: 32, duration: 0.442s, episode steps:  64, steps per second: 145, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.300557, mae: 4.134573, mean_q: 8.156261, mean_tau: 0.911940\n",
            "  982/10000: episode: 33, duration: 0.423s, episode steps:  60, steps per second: 142, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.345196, mae: 4.395840, mean_q: 8.684896, mean_tau: 0.905802\n",
            " 1018/10000: episode: 34, duration: 0.239s, episode steps:  36, steps per second: 151, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.356026, mae: 4.594453, mean_q: 9.074756, mean_tau: 0.901049\n",
            " 1050/10000: episode: 35, duration: 0.216s, episode steps:  32, steps per second: 148, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.656 [0.000, 1.000],  loss: 0.332789, mae: 4.738885, mean_q: 9.402183, mean_tau: 0.897683\n",
            " 1096/10000: episode: 36, duration: 0.301s, episode steps:  46, steps per second: 153, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.587 [0.000, 1.000],  loss: 0.330476, mae: 4.876135, mean_q: 9.789614, mean_tau: 0.893823\n",
            " 1151/10000: episode: 37, duration: 0.381s, episode steps:  55, steps per second: 144, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.422418, mae: 5.052620, mean_q: 10.060487, mean_tau: 0.888823\n",
            " 1216/10000: episode: 38, duration: 0.431s, episode steps:  65, steps per second: 151, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.482576, mae: 5.362485, mean_q: 10.703651, mean_tau: 0.882883\n",
            " 1305/10000: episode: 39, duration: 0.626s, episode steps:  89, steps per second: 142, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.338510, mae: 5.755807, mean_q: 11.633226, mean_tau: 0.875260\n",
            " 1428/10000: episode: 40, duration: 0.789s, episode steps: 123, steps per second: 156, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.425407, mae: 6.238004, mean_q: 12.598148, mean_tau: 0.864766\n",
            " 1536/10000: episode: 41, duration: 0.703s, episode steps: 108, steps per second: 154, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.539461, mae: 6.713018, mean_q: 13.601806, mean_tau: 0.853331\n",
            " 1686/10000: episode: 42, duration: 0.950s, episode steps: 150, steps per second: 158, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 0.608090, mae: 7.363369, mean_q: 14.898833, mean_tau: 0.840561\n",
            " 1810/10000: episode: 43, duration: 0.808s, episode steps: 124, steps per second: 154, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 0.710386, mae: 7.971093, mean_q: 16.169761, mean_tau: 0.826998\n",
            " 1929/10000: episode: 44, duration: 1.260s, episode steps: 119, steps per second:  94, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.723718, mae: 8.552434, mean_q: 17.349773, mean_tau: 0.814969\n",
            " 2054/10000: episode: 45, duration: 0.857s, episode steps: 125, steps per second: 146, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.882163, mae: 9.070928, mean_q: 18.452890, mean_tau: 0.802891\n",
            " 2186/10000: episode: 46, duration: 1.266s, episode steps: 132, steps per second: 104, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.867407, mae: 9.695443, mean_q: 19.731273, mean_tau: 0.790169\n",
            " 2308/10000: episode: 47, duration: 1.128s, episode steps: 122, steps per second: 108, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.012358, mae: 10.316533, mean_q: 20.961505, mean_tau: 0.777596\n",
            " 2450/10000: episode: 48, duration: 1.058s, episode steps: 142, steps per second: 134, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.848740, mae: 10.824755, mean_q: 21.970248, mean_tau: 0.764529\n",
            " 2555/10000: episode: 49, duration: 0.747s, episode steps: 105, steps per second: 141, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.147249, mae: 11.441736, mean_q: 23.187259, mean_tau: 0.752302\n",
            " 2676/10000: episode: 50, duration: 0.827s, episode steps: 121, steps per second: 146, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.491732, mae: 11.926573, mean_q: 24.073080, mean_tau: 0.741115\n",
            " 2817/10000: episode: 51, duration: 0.921s, episode steps: 141, steps per second: 153, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.638471, mae: 12.467762, mean_q: 25.129578, mean_tau: 0.728146\n",
            " 2921/10000: episode: 52, duration: 0.676s, episode steps: 104, steps per second: 154, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.563354, mae: 12.902472, mean_q: 26.011886, mean_tau: 0.716019\n",
            " 3069/10000: episode: 53, duration: 1.320s, episode steps: 148, steps per second: 112, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.301866, mae: 13.383995, mean_q: 27.100379, mean_tau: 0.703545\n",
            " 3177/10000: episode: 54, duration: 1.142s, episode steps: 108, steps per second:  95, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.663845, mae: 13.877051, mean_q: 27.995870, mean_tau: 0.690872\n",
            " 3326/10000: episode: 55, duration: 1.179s, episode steps: 149, steps per second: 126, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.444977, mae: 14.370010, mean_q: 29.036797, mean_tau: 0.678151\n",
            " 3459/10000: episode: 56, duration: 0.913s, episode steps: 133, steps per second: 146, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.263519, mae: 14.750442, mean_q: 29.789120, mean_tau: 0.664192\n",
            " 3596/10000: episode: 57, duration: 0.931s, episode steps: 137, steps per second: 147, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.793831, mae: 15.324943, mean_q: 30.958257, mean_tau: 0.650827\n",
            " 3731/10000: episode: 58, duration: 1.049s, episode steps: 135, steps per second: 129, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.095031, mae: 15.758708, mean_q: 31.777717, mean_tau: 0.637363\n",
            " 3896/10000: episode: 59, duration: 2.019s, episode steps: 165, steps per second:  82, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.838672, mae: 16.298354, mean_q: 32.983331, mean_tau: 0.622513\n",
            " 4015/10000: episode: 60, duration: 1.572s, episode steps: 119, steps per second:  76, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.265061, mae: 16.783034, mean_q: 33.919518, mean_tau: 0.608455\n",
            " 4171/10000: episode: 61, duration: 2.204s, episode steps: 156, steps per second:  71, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.132481, mae: 17.320835, mean_q: 34.896231, mean_tau: 0.594843\n",
            " 4357/10000: episode: 62, duration: 2.309s, episode steps: 186, steps per second:  81, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.876760, mae: 17.873203, mean_q: 36.149503, mean_tau: 0.577913\n",
            " 4475/10000: episode: 63, duration: 1.082s, episode steps: 118, steps per second: 109, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.234224, mae: 18.312554, mean_q: 36.979793, mean_tau: 0.562866\n",
            " 4608/10000: episode: 64, duration: 1.487s, episode steps: 133, steps per second:  89, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.155190, mae: 18.473646, mean_q: 37.414367, mean_tau: 0.550441\n",
            " 4750/10000: episode: 65, duration: 3.322s, episode steps: 142, steps per second:  43, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.249521, mae: 18.974863, mean_q: 38.427335, mean_tau: 0.536829\n",
            " 4879/10000: episode: 66, duration: 2.101s, episode steps: 129, steps per second:  61, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.935355, mae: 19.348256, mean_q: 39.348255, mean_tau: 0.523414\n",
            " 5000/10000: episode: 67, duration: 0.848s, episode steps: 121, steps per second: 143, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.107803, mae: 19.838435, mean_q: 40.141060, mean_tau: 0.511039\n",
            " 5167/10000: episode: 68, duration: 1.146s, episode steps: 167, steps per second: 146, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.561523, mae: 20.135030, mean_q: 40.680838, mean_tau: 0.496783\n",
            " 5300/10000: episode: 69, duration: 0.917s, episode steps: 133, steps per second: 145, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 2.053547, mae: 20.461611, mean_q: 41.537305, mean_tau: 0.481933\n",
            " 5461/10000: episode: 70, duration: 1.817s, episode steps: 161, steps per second:  89, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.937829, mae: 20.962838, mean_q: 42.358338, mean_tau: 0.467380\n",
            " 5593/10000: episode: 71, duration: 1.311s, episode steps: 132, steps per second: 101, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 2.314656, mae: 21.177823, mean_q: 42.827031, mean_tau: 0.452877\n",
            " 5764/10000: episode: 72, duration: 1.349s, episode steps: 171, steps per second: 127, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.417675, mae: 21.606401, mean_q: 43.706512, mean_tau: 0.437878\n",
            " 5894/10000: episode: 73, duration: 1.168s, episode steps: 130, steps per second: 111, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 2.275588, mae: 21.940934, mean_q: 44.329335, mean_tau: 0.422979\n",
            " 6007/10000: episode: 74, duration: 1.378s, episode steps: 113, steps per second:  82, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 2.703721, mae: 22.228556, mean_q: 45.024596, mean_tau: 0.410950\n",
            " 6207/10000: episode: 75, duration: 3.099s, episode steps: 200, steps per second:  65, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.476378, mae: 22.752106, mean_q: 46.074483, mean_tau: 0.395457\n",
            " 6325/10000: episode: 76, duration: 0.819s, episode steps: 118, steps per second: 144, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 3.020187, mae: 23.022171, mean_q: 46.552556, mean_tau: 0.379716\n",
            " 6465/10000: episode: 77, duration: 1.003s, episode steps: 140, steps per second: 140, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.726507, mae: 23.125361, mean_q: 46.762458, mean_tau: 0.366945\n",
            " 6611/10000: episode: 78, duration: 1.007s, episode steps: 146, steps per second: 145, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.787814, mae: 23.542648, mean_q: 47.605173, mean_tau: 0.352788\n",
            " 6746/10000: episode: 79, duration: 0.987s, episode steps: 135, steps per second: 137, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.968667, mae: 23.818888, mean_q: 48.155786, mean_tau: 0.338878\n",
            " 6911/10000: episode: 80, duration: 1.134s, episode steps: 165, steps per second: 145, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.902492, mae: 24.086073, mean_q: 48.702811, mean_tau: 0.324028\n",
            " 7062/10000: episode: 81, duration: 1.040s, episode steps: 151, steps per second: 145, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.838499, mae: 24.617530, mean_q: 49.715183, mean_tau: 0.308386\n",
            " 7210/10000: episode: 82, duration: 1.026s, episode steps: 148, steps per second: 144, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.999038, mae: 24.575075, mean_q: 49.628314, mean_tau: 0.293585\n",
            " 7373/10000: episode: 83, duration: 1.133s, episode steps: 163, steps per second: 144, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.597121, mae: 25.183140, mean_q: 50.896161, mean_tau: 0.278191\n",
            " 7573/10000: episode: 84, duration: 1.469s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.615874, mae: 25.306894, mean_q: 50.994261, mean_tau: 0.260223\n",
            " 7693/10000: episode: 85, duration: 1.139s, episode steps: 120, steps per second: 105, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.041114, mae: 25.622070, mean_q: 51.761820, mean_tau: 0.244383\n",
            " 7893/10000: episode: 86, duration: 1.827s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.152802, mae: 25.942501, mean_q: 52.216227, mean_tau: 0.228543\n",
            " 8014/10000: episode: 87, duration: 0.794s, episode steps: 121, steps per second: 152, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 3.088402, mae: 26.412470, mean_q: 53.221371, mean_tau: 0.212653\n",
            " 8177/10000: episode: 88, duration: 1.115s, episode steps: 163, steps per second: 146, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.722551, mae: 26.789406, mean_q: 54.034502, mean_tau: 0.198595\n",
            " 8351/10000: episode: 89, duration: 1.196s, episode steps: 174, steps per second: 145, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.199254, mae: 27.176103, mean_q: 54.879798, mean_tau: 0.181914\n",
            " 8509/10000: episode: 90, duration: 1.051s, episode steps: 158, steps per second: 150, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.630266, mae: 27.368907, mean_q: 55.217784, mean_tau: 0.165480\n",
            " 8641/10000: episode: 91, duration: 0.881s, episode steps: 132, steps per second: 150, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 4.877837, mae: 27.545890, mean_q: 55.718056, mean_tau: 0.151125\n",
            " 8783/10000: episode: 92, duration: 0.957s, episode steps: 142, steps per second: 148, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.809545, mae: 27.626112, mean_q: 55.748331, mean_tau: 0.137562\n",
            " 8890/10000: episode: 93, duration: 0.699s, episode steps: 107, steps per second: 153, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 3.689296, mae: 27.852044, mean_q: 56.243456, mean_tau: 0.125236\n",
            " 9030/10000: episode: 94, duration: 0.915s, episode steps: 140, steps per second: 153, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 5.164577, mae: 28.151236, mean_q: 56.666667, mean_tau: 0.113010\n",
            " 9139/10000: episode: 95, duration: 0.712s, episode steps: 109, steps per second: 153, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.018809, mae: 28.188130, mean_q: 56.824603, mean_tau: 0.100684\n",
            " 9278/10000: episode: 96, duration: 0.905s, episode steps: 139, steps per second: 154, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.605075, mae: 28.338689, mean_q: 57.056646, mean_tau: 0.088408\n",
            " 9296/10000: episode: 97, duration: 0.126s, episode steps:  18, steps per second: 143, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.285580, mae: 28.854707, mean_q: 58.111225, mean_tau: 0.080637\n",
            " 9327/10000: episode: 98, duration: 0.212s, episode steps:  31, steps per second: 146, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 4.479605, mae: 28.117253, mean_q: 56.866197, mean_tau: 0.078211\n",
            " 9357/10000: episode: 99, duration: 0.221s, episode steps:  30, steps per second: 136, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 10.182529, mae: 28.199357, mean_q: 56.350989, mean_tau: 0.075192\n",
            " 9418/10000: episode: 100, duration: 0.534s, episode steps:  61, steps per second: 114, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.557 [0.000, 1.000],  loss: 5.198874, mae: 28.731943, mean_q: 57.608809, mean_tau: 0.070687\n",
            " 9430/10000: episode: 101, duration: 0.126s, episode steps:  12, steps per second:  95, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.719800, mae: 27.864126, mean_q: 56.240028, mean_tau: 0.067074\n",
            " 9456/10000: episode: 102, duration: 0.265s, episode steps:  26, steps per second:  98, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 12.297758, mae: 29.522870, mean_q: 59.301289, mean_tau: 0.065193\n",
            " 9470/10000: episode: 103, duration: 0.139s, episode steps:  14, steps per second: 101, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 5.097137, mae: 29.178318, mean_q: 58.538850, mean_tau: 0.063213\n",
            " 9508/10000: episode: 104, duration: 0.379s, episode steps:  38, steps per second: 100, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.208943, mae: 28.915104, mean_q: 58.404307, mean_tau: 0.060639\n",
            " 9535/10000: episode: 105, duration: 0.274s, episode steps:  27, steps per second:  99, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 3.172074, mae: 28.442228, mean_q: 57.560634, mean_tau: 0.057421\n",
            " 9567/10000: episode: 106, duration: 0.347s, episode steps:  32, steps per second:  92, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.741190, mae: 28.790189, mean_q: 58.121396, mean_tau: 0.054501\n",
            " 9600/10000: episode: 107, duration: 0.343s, episode steps:  33, steps per second:  96, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.927670, mae: 29.277032, mean_q: 58.861276, mean_tau: 0.051283\n",
            " 9627/10000: episode: 108, duration: 0.285s, episode steps:  27, steps per second:  95, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.051117, mae: 29.110980, mean_q: 58.744649, mean_tau: 0.048313\n",
            " 9642/10000: episode: 109, duration: 0.178s, episode steps:  15, steps per second:  84, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 7.725388, mae: 28.841432, mean_q: 57.775116, mean_tau: 0.046234\n",
            " 9698/10000: episode: 110, duration: 0.448s, episode steps:  56, steps per second: 125, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.483446, mae: 29.409140, mean_q: 58.797777, mean_tau: 0.042720\n",
            " 9709/10000: episode: 111, duration: 0.082s, episode steps:  11, steps per second: 133, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.709893, mae: 28.905057, mean_q: 58.062746, mean_tau: 0.039403\n",
            " 9762/10000: episode: 112, duration: 0.389s, episode steps:  53, steps per second: 136, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 3.631464, mae: 29.135800, mean_q: 58.833358, mean_tau: 0.036235\n",
            " 9772/10000: episode: 113, duration: 0.075s, episode steps:  10, steps per second: 133, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 10.277477, mae: 30.537321, mean_q: 61.107244, mean_tau: 0.033117\n",
            " 9787/10000: episode: 114, duration: 0.118s, episode steps:  15, steps per second: 128, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4.383114, mae: 29.612603, mean_q: 59.552585, mean_tau: 0.031879\n",
            " 9813/10000: episode: 115, duration: 0.193s, episode steps:  26, steps per second: 134, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5.407787, mae: 30.153273, mean_q: 60.684485, mean_tau: 0.029850\n",
            " 9824/10000: episode: 116, duration: 0.082s, episode steps:  11, steps per second: 133, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 3.367105, mae: 29.923523, mean_q: 60.337883, mean_tau: 0.028018\n",
            " 9836/10000: episode: 117, duration: 0.087s, episode steps:  12, steps per second: 137, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 9.623147, mae: 29.955336, mean_q: 60.538522, mean_tau: 0.026880\n",
            " 9888/10000: episode: 118, duration: 0.372s, episode steps:  52, steps per second: 140, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5.110596, mae: 29.643300, mean_q: 59.827612, mean_tau: 0.023712\n",
            " 9902/10000: episode: 119, duration: 0.099s, episode steps:  14, steps per second: 141, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 6.992676, mae: 29.389905, mean_q: 59.195701, mean_tau: 0.020445\n",
            " 9926/10000: episode: 120, duration: 0.187s, episode steps:  24, steps per second: 128, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.944027, mae: 29.976465, mean_q: 60.193662, mean_tau: 0.018564\n",
            " 9946/10000: episode: 121, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 4.502785, mae: 30.174926, mean_q: 60.748506, mean_tau: 0.016386\n",
            " 9966/10000: episode: 122, duration: 0.153s, episode steps:  20, steps per second: 131, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 7.542357, mae: 29.659159, mean_q: 60.056450, mean_tau: 0.014406\n",
            " 9987/10000: episode: 123, duration: 0.153s, episode steps:  21, steps per second: 137, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 4.602786, mae: 30.011974, mean_q: 60.851723, mean_tau: 0.012376\n",
            "done, took 87.073 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 137.000, steps: 137\n",
            "Episode 2: reward: 146.000, steps: 146\n",
            "Episode 3: reward: 145.000, steps: 145\n",
            "Episode 4: reward: 132.000, steps: 132\n",
            "Episode 5: reward: 142.000, steps: 142\n",
            "Episode 6: reward: 138.000, steps: 138\n",
            "Episode 7: reward: 177.000, steps: 177\n",
            "Episode 8: reward: 142.000, steps: 142\n",
            "Episode 9: reward: 124.000, steps: 124\n",
            "Episode 10: reward: 133.000, steps: 133\n",
            "Episode 11: reward: 138.000, steps: 138\n",
            "Episode 12: reward: 131.000, steps: 131\n",
            "Episode 13: reward: 128.000, steps: 128\n",
            "Episode 14: reward: 140.000, steps: 140\n",
            "Episode 15: reward: 139.000, steps: 139\n",
            "Episode 16: reward: 159.000, steps: 159\n",
            "Episode 17: reward: 148.000, steps: 148\n",
            "Episode 18: reward: 119.000, steps: 119\n",
            "Episode 19: reward: 137.000, steps: 137\n",
            "Episode 20: reward: 126.000, steps: 126\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f404d75b610>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "id": "dVgg_-CEaAW4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "aad2e91e-f35b-4d7a-ae55-7d0c9e3f7505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f404d5aafd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnvbSGJAUqpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}